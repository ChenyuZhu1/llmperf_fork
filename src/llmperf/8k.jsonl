{"input": "Why is it important for the sides of the fuselage to be sloped (tumbled home)?", "context": "Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go \"perfectly.\" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane.\nThis is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes.\nWhile building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying \"banana\" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built.\nFirst understand that the plans show the finished form of the plane. They show the \"projected\" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are \"foreshortened\" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to \"develop\" the \"true\" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat.\nSecond, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel \"undevelopable\" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition \"developable\". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a \"compounded\" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain.\nInitially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home), the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock.\nThis method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later.\nLayout to ensure a fair and true fuselage starts by drawing a reference line (baseline) on the building surface. Refer to figures 2 & 3 and use a wire guide to draw a very straight baseline. About 500 lbs. Of tension should be adequate. One could use a chalk line, but we're talking airplanes here, not house framing.\nThe main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Although the baseline will still be the reference, the top and bottom longerons will be laid separately.\nLayout differences don't end there. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. Another major difference is that the bottom & side panels are applied after forming the fuselage box section. This is mainly to obtain the ability to \"fair\" the side and bottom surfaces and insure a straight and true shape.\nRefer to figure 1 for the layout of the new developed side panel. The firewall (station a) is layed out perpendicular to the baseline. Longitudinal (station) measurements are given along the length of the baseline from the firewall. Vertical dimensions are given to reference the angle and breadths of the station at the baseline.\nNotice that the top longeron is bowed outward and that the stations are spaced slightly greater than called out in the plans. When the panels are formed into the box frame section ,they will work into the dimensions specified in the plans.\nStrike a centerline, longer than is needed on the building surface using a wire guide. Draw off the firewall line perpendicular to the centerline at one end.\nUsing the distances listed in the balloons, mark them off on the centerline. Distances are measured to the nearest sixteenth of an inch. Take time to mark them off carefully. Don't mark off the distances in a cumulative fashion. Use the firewall as a common reference.\nUsing the angles listed at each station, mark off a station line longer than is needed. The angles are measured to the nearest hundredth of a degree. Take time to mark them off carefully.\nAt each station, start by marking off each short (bottom longeron) line distance from the centerline. Use your set of trammels or beam compass for doing this. Mark the intersection of the short line with the station line.\nAt each station, mark off each long (top longeron) line distance from the intersection of the short line distance and the station line. Again the trammels or beam compass is best for completing this step. Mark the intersection of the long line distance with the station line.\nUsing the longeron as a batten, trace out the inside and outside curves of the longeron. After the batten is secure, in between each station, fasten a keeper block inside and outside to preserve the shape of the longeron taking care to avoid potential future interference with the diagonal members to be installed later. The fairing blocks can be removed or left in place if they won't interfere with building. The vertical station members and their diagonals can now be measured and positioned. Remember to refer to the plans for the material thickness direction.\nAfter vertical and diagonal members are cut and fitted, take time to draw their outlines on the building surface to cut down on time and confusion when laying out the opposite side.\nFinishing the side panel is accomplished in a manner similar to that called for in the handbook with the exception that the side and bottom skin panels will be attached later.\nThe next article in the series will discuss jigging and building techniques to ensure alignment and straightness of the flat built side panels. Also covered will be building a \"strongback\" jig to assure alignment of the side panels when they are formed into their final shape.\nPart 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the side and bottom ply skins.\nU.S. Mail: Densmore Associates, inc.\nANSI \"D\" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand.\n\"Scarfing\" is the practice of splicing plywood so that short pieces of plywood can be used to span long distances. On the KR, it is required on both the fuselage skins and spar webs. The angle of the splice should be 10 to 12 degrees to maintain strength across the joint. Also, joints should coincide with structural members, such as spar webs or fuselage truss members.\nThis scarfer is made by mating a regular plunge router (this one costs about $50) to a table saw. Obviously, you really only need a table saw to cut the chamfer, but it does make a nice heavy table for scarfing. You could just as easily use a large work table as the base.First, set the table saw for a 5.5 degree cut (for a 1:12 joint, or 6.5 degree cut for a 10:1 joint), and run a 1 x 6 through on edge to chamfer a corner on the board. Then drill the board for three router mounting holes (two are countersunk) and connect the assembly to the table saw with two 1/4 inch bolts. Use a long (2-3 inch) straight cutting bit to do the cutting. Adjust the bit so it doesn't interfere with your table top, and go to town. Keep pressure on the plywood to ensure contact with the table while you're scarfing. Make sure you feed your material from the same end as you would if you were sawing, or the router will take your plywood away from you and put a big dent in your garage door.\nIn the late 60's Ken Rand and Stuart Robinson were working as flight system engineers for Douglas Avionics. Ken was working as an electrical engineer, having previously worked for Sperry as an autopilots project engineer, while Stu's degree was in aeronautical engineering from Northrop University. They were two of the guys at the end of the DC-8,9, and 10 assembly lines responsible for correcting some of the nits and picks in various systems before delivery to the customer.\nThey both wanted to build a fast, inexpensive airplane which was also economical to maintain. Several designs were considered, and plans were bought first for the Jeanie's Teenie and then the Taylor Monoplane. The Monoplane was more to their liking, but would require some modification to fit their needs. A cooperative redesign effort ensued, with virtually no dimensions left untouched. Only the basic fuselage structure, airfoil, and powerplant were retained. The tail shape was Stu's, and came directly from the big DC-8s parked on the ramp outside his office window. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport.\nKen was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Ken's wife Jeanette became owner of RR overnight, and stepped up to keep the plans and parts coming. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79.\nTo date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5 KR2Ss now in the air. Much of the development work done on KR's is now done by the builders themselves. KR builders tend to be innovative, which leads to some interesting modifications. Some of the mods that work eventually creep into the plans. The KR2S is a case in point. Many builders who'd heard of the pitch sensitivity and tight cabin of the KR2 began to build an enlarged version, with the length determined by the most commonly available longeron material. The result is a KR2 that is stretched 2\" between firewall and main spar, and 14\" behind the main spar. Higher gross weights dictated more wing area, with the new standard becoming the Diehl wing skin. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts.\nMike Stearns addresses the KR Forum crowd.\nThis year's KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Mike Stearns spoke on several topics, including the many sources for KR and homebuilding information available on the Internet. He also mentioned KRNet, the list server devoted entirely to KR aircraft, as well as several notable World Wide Web home pages. He also brought a sample of the new Rand Robinson wing skins with him, and discussed their high temperature core prepreg construction. His KR2S will receive the first set, which is currently being installed at Hinson Composites.\nSteve Trentman spoke on his turbine installation. It uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus. sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. More on this exciting development in next month's issue of KROnline.\nLes Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. After researching the KR series, and reading Neil Bingham's \"A Critical Analysis of the KR2\" (Jan 88 Sport Aviation), Les decided to build his as a single seater, stretched 24\" in the tail, while maintaining a stock width firewall. His fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. It is skinned with 1/8\" birch plywood. Spars are covered with plywoood on both fore and aft sides, ala KR2S. Diehl wing skins provide the lift. Horizontal stabilizer and elevator were stretched 7\" longer on each side, while the vertical stabilizer and rudder were stretched 8\" taller. . The fuselage to cowling junction was made more graceful by adding 1.5 inches to the height of the firewall end of the fuselage sides.\nLes's canopy is a Dragonfly, using a four linkage system to swing forward when opening. The canopy frame fits snugly into a recess in the foward deck, providing an excellent wind and water seal. The fiberglass work is exemplary.\nSeating is luxurious for one.\nThe cowling is also a work of art, and uses NACA ducts for efficiency. Female molds were made for all the fiberglass parts on Les's plane, so he could proabably be persuaded to make more, if demand dictates. Les also machines a multitude of KR aluminum and steel parts which he now offers for sale.\nThe firewall was reinforced with aluminum brackets and angles bolted between the longerons in anticipation of the 200 lb Subaru EA-81 engine installation. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy.\nOriginally built as a taildragger, the fixed gear is made from 4130 steel tubing. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6\" Cleveland calipers for braking. An early taxi test accident damaged the main gear, and prompted Les to change to tricycle gear. Again, he designed his own fiberglass main gear, and uses a Diehl nose wheel fork with a 4130 strut and 6\" wheel up front.\nEarly tests revealed cooling problems, which prompted a radiator move from the firewall to a lower cowling location.\nThe first flight was almost a disaster, as test pilot Randy Smith lost power right after takeoff. He managed a 180 with a safe downwind landing with only minor nosewheel pant damage. The culprit proved to be a spark plug with too much reach, which was quickly remedied. Subsequent flights have shown water temp to be about 210 degrees, oil temp is 220-230, and airspeed is about 180 mph.\nShopping for the Partially Built KR.\nThis story starts about twenty years ago when I first started looking at the KR-2 as the plane I'd like to build. The only problem at that time was a lack of money, lack of knowledge, and a lack of job stability. I liked the design, except for the low ground clearance of the retractable gear and that a KR was going to be a tight fit for me to fly.\nOver the past twenty years I've owned a number of planes, but still always wanted to build my own. I needed one that would fit me, my budget requirements, and have the speed and performance that I wanted. When \"KITPLANES\" published the article featuring Roy Marsh's new KR-2S, it was the first I had heard of any major modifications or improvements to the same old KR design. I believe that article and Roy Marsh's workmanship have probably been the greatest boon to Rand Robinson (RR) in the last twenty years. It certainly caught my eye! Here was the same design I had decided I wanted to build twenty years ago, with all of the improvements I wanted. It was sitting on fixed gear with some reasonable ground clearance. It had the capability to be built large enough to accommodate me. It has enough prefab parts available that it didn't have to be 100% scratch built if I decided to hurry the project along. And it had the speed I wanted. I knew that Roy's published speeds were probably not realistic expectations for the average KR, but after knocking around for the last three years in my Champ, anything over 90 mph seems pretty fast to me.\nAfter purchasing the info kit and the sales video from Rand Robinson, the next step after deciding for sure to build this plane was to order the KR-2 plans and the KR-2S addendum. I finally got my plans and was putting together my first order to start the plane, when my partner in the Champ pointed out that there was a partially completed KR-2S for sale in Trade-a-plane. My initial answer was \"No, I don't even want to look at it. I want to build my own from scratch.\" My partner insisted that for the advertised price and the fact that it wasn't too far away, I ought to at least give the guy a call and investigate it. \"No, I don't think I want to buy someone else's problems,\" I persisted. That night I went home and crunched up some numbers on the calculator and finally came to the conclusion that for the sake of my budget for the next several years, I really should give this guy a call.\nThree days later, I flew to his place about 400 miles away to take a look at his project. At this point I should probably mention that I consider myself to be fairly knowledgeable about airplane construction, although the vast majority of my experience is with tube and fabric. The rest of this article deals with what I looked for and more importantly what I missed and have had to repair in the last year since I purchased the project.\nWhen we went to the seller's house, I found that the left wing was built using the Dan Diehl wing skins and the right wing skins were leaning against the wall inside the house. Also the canopy was in the house with the canopy covered with paper and tape. I wanted to inspect the fuselage first, so off we went to the shop.\nThere I found a fuselage sitting on it's gear painted in primer gray. The first step was to inspect the quality of workmanship of what could be seen as it sat. The interior of the fuselage looked as if it had been built with a great deal of care. The fit and finish of all of the interior wood was very nice. Even the gussets looked like they had been painstakingly perfectly fitted. The glass work on the turtle back also looked very precise and clean. It was evenly faired into the vertical and horizontal stabs. The tail also appeared to be well built with the exception of a depression directly over the front and rear spars in the horizontal stabs. He explained that when he moved recently, that he had shot the plane with gray primer to protect it from the weather since he wouldn't have ready access to a shop to put it in right away. It ended up sitting out in the hot south Texas summer sun for a few weeks before he got a shop rented to work in. That caused the glass (or possibly the foam inside the horizontal stab) to swell, except that it held onto the spar, so it was slightly ballooned in front of and behind the spars. His recommendation was to fill it back smooth with micro.\nI also found a small linear crack in the lower left wing spar cap on the left wing stub. It appeared to be from over tightening the rear spar wing attach fitting bolts. His explanation was that the crack wasn't important because the rear spars only job is to keep the wings from folding back. I also noticed that the holes for attaching the outer wing to the wing stub were badly rounded out on the rear spar. He explained that the Diehl wing skins require the rear spar to be swept slightly more forward than the stock wings. This won't allow you to use the rear spar attach fittings from RR and that I would need to fabricate a new set of rear spar attach fittings.\nI also found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. I couldn't check for function since the right bellcrank and sheeve wasn't installed, the left wing also wasn't installed, and the right wing didn't exist yet.\nNext we pulled the inspection panels off of the fuselage and tail and looked at everything I could see with a good flashlight. I didn't find anything else that might be questionable about the fuselage except for a cracked elevator trim tab that was damaged when it fell off it's hanging place on the wall.\nNext we spent some time going over his builders log and builders photo album. I still hadn't seen anything that would dissuade me from buying this project.\nAt this point it was starting to get late and my ride down needed to get airborne for the flight home. I needed to make a decision about whether I wanted this project or not, but I hadn't inspected the wings and canopy yet. I took a cursory look at the left wing and saw lots on micro built up on it and some bubbles in the leading edge, but nothing that looked seriously wrong to my amateur eye. The right wing was only a set of spars in the shop and the Diehl wing skins in the house, so there wasn't much to look at there. The canopy was wrapped in paper and tape, so there wasn't much to look at there either. I decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the project. For the advertised price, I could build a new set of wings and still be way ahead financially. We negotiated a final price, shook hands, took my ride to the airport, and started off in search of a U-haul to haul the project home.\nNow, at this point, some of you are thinking about what I surely must have forgotten to inspect and why didn't I take a local A & P or EAA member along for the ride. First of all, I don't know any mechanics locally that have any experience with glass and our EAA chapter of which I am VP is woefully lacking in fiberglass knowledge. Secondly, as you will see, I missed plenty. Some by ignorance, some by just not looking close enough.\nNow for a list of the problems that I found over the last year and a few of the fixes that I came up with.\nI found that the lower set of rear spar attach fittings on the left rear spar were installed backwards with the longer spaced hole towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Also in the same area he had drilled through the rear spar with a hole saw to create a place for the aileron cable to pass through and managed to cut out the second from the outside vertical brace in the spar. Then he chose to install the aileron bellcranks in front of the rear spar, and cut another hole through the rear spar for the aileron push rod. He also managed to cut out the outside vertical brace in the spar. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in, cut the whole rear spar carrythrough out of the fuselage including ruining the left lower wing skin, or do something else creative to reinforce the spar cap and install a custom built set of attach fittings.\nI also found that after I built and installed the right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. The cable that crosses between the two bellcranks had a sharp uphill from the sheeve to the bellcrank in the last 12 inches on either side. This combined with the radius that the bellcranks turn caused the cross cable to pull up tight when the ailerons were pushed to either end of their travel, but allowed the cables to go very slack when the ailerons were centered. Also the Aileron pushrods needed to pass directly through the lower set of rear wing attach fittings to attach to the aileron. This whole rear spar and aileron bellcrank setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it.\nI decided that I had to remove the rear fittings from the left wing to be replaced with the new set that my neighborhood machinist was cutting out for me. When I put the wing on the work bench to start removing the rear fittings, I thought I had better take a closer look at the bubbles in the leading edge. I found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leading edge. What I found was that the leading edge had been floxed together and glassed over, but the mold release had never been scrubbed off the leading edge of the wing. It peeled apart for rebuild quite easily.\nWhen I got back to removing the rear spar attach fittings, I noticed that the woodwork inside the wing looked awfully dull. The reason was that the wing had been closed up without varnishing any of the woodwork. This was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer.\nI also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. Retapping the fitting the right direction seemed to be a good fix for that problem.\nWhen I finally got around to attaching the wing to the fuselage, I found that the front spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts.\nOn the fuselage I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. I must complement the manufacturers for taking care of that problem. One call to the Cleveland factory and they shipped me a new set of wheels and brakes even though the receipt for this set was over four years old and in the original builders name. Their only concern was that this set had never been placed in service yet.\nI chose to sand the load of micro off the left wing to see what it was covering. When I got down to the glass, I found that there was no glass for the aft inch and a half of the underside of the wing in front of the aileron hinge. With the Diehl wing skins, you build the wings, then cut the ailerons out of trailing edge of the wing. He had mismeasured and cut too much material off the bottom side of the trailing edge in front of the aileron. It was filled by floxing a piece of spruce into the gap to fill the space between the back edge of the fiberglass and the aileron mount. I chose to wrap the trailing edge of that wing, and the other wing to match with a couple of lay-ups of glass.\nWhen I sanded the primer off the aforementioned damaged trim tab, I found that the hinge was floxed to the leading edge of the foam insides of the tab, but not the glass. I also chose to wrap the front of the trim tab with a lay-up of glass.\nI decided to pull the paper off the canopy and take a look at it before I'm ready to bolt it on and fly. The original builder had blown his own canopy and after some of the previous problems, I was beginning to have some concerns about not having looked it over closely enough. The canopy turned out to have been blow a little too large. It ended up with a little larger bubble for headroom, which I didn't object to. However, it had more headroom on the right side than the left. Yes, it was just a little bit lopsided. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm.. My fear was that this is just thin enough that it may decide to lay on my head or in my lap when flying on a warm day. It will have to be replaced.\nI'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and I'm sure that there could have been many other problems that didn't but could have existed on this project. This is also not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. This article is intended to feature some of the problems that you may run into in buying someone else's project.\nThe final question is, knowing what I have found over the past year, would I have still purchased this project. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. There are a few things that I would have done differently, but nothing that I can't live with. Although I won't be able to say that I built it all from scratch, I have built and rebuild enough of the plane that I should have no problem qualifying under the 51% rule.\nYou can send comments directly to the author via e-mail at \"jscott@LANL.GOV\".\nHere is an brief explanation of how I built my turtledecks. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo.\nAfter the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade.\nAfter the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out.\nAt this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the \"backbone\" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees.", "answers": ["The sides of the fuselage are sloped to create a conical section when the fuselage is formed."], "length": 6250, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "41e27260a12cc40a778f8c1ba8bf643ee655871a458e5e1c"}
{"input": "What is the main methodology used in the research?", "context": "Paper Info\n\nTitle: On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement Learning\nPublish Date: Unkown\nAuthor List: Seth Karten, Siva Kailas, Huao Li, Katia Sycara\n\nFigure\n\nFigure1.By using contrastive learning, our method seeks similar representations between the state-message pair and future states while creating dissimilar representations with random states.Thus satisfying the utility objective of the information bottleneck.The depicted agents are blind and cannot see other cars.\nFigure 2.An example of two possible classes, person and horse, from a single observation in the Pascal VOC game.\nFigure 3. Blind Traffic Junction Left: Our method uses compositional complexity and contrastive utility to outperform other baselines in terms of performance and sample complexity.The legend provides the mean ± variance of the best performance.Right: Top: success, contrastive, and complexity losses for our method.Right, Bottom: success, autoencoder loss for ae-comm with supervised pretraining.\nFigure 4. Pascal VOC Game Representing compositional concepts from raw pixel data in images to communicate multiple concepts within a single image.Our method significantly outperforms ae-comm and no-comm due to our framework being able to learn composable, independent concepts.\nFigure 5. Blind Traffic Junction Social shadowing enables significantly lower sample complexity when compared to traditional online MARL.\nBeta ablation: Messages are naturally sparse in bits due to the complexity loss.Redundancy measures the capacity for a bijection between the size of the set of unique tokens and the enumerated observations and intents.Min redundancy is 1.0 (a bijection).Lower is better.\n\nabstract\n\nExplicit communication among humans is key to coordinating and learning. Social learning, which uses cues from experts, can greatly benefit from the usage of explicit communication to align heterogeneous policies, reduce sample complexity, and solve partially observable tasks. Emergent communication, a type of explicit communication, studies the creation of an artificial language to encode a high task-utility message directly from data.\nHowever, in most cases, emergent communication sends insufficiently compressed messages with little or null information, which also may not be understandable to a third-party listener. This paper proposes an unsupervised method based on the information bottleneck to capture both referential complexity and task-specific utility to adequately explore sparse social communication scenarios in multi-agent reinforcement learning (MARL).\nWe show that our model is able to i) develop a natural-language-inspired lexicon of messages that is independently composed of a set of emergent concepts, which span the observations and intents with minimal bits, ii) develop communication to align the action policies of heterogeneous agents with dissimilar feature models, and iii) learn a communication policy from watching an expert's action policy, which we term 'social shadowing'.\n\nINTRODUCTION\n\nSocial learning agents analyze cues from direct observation of other agents (novice or expert) in the same environment to learn an action policy from others. However, observing expert actions may not be sufficient to coordinate with other agents. Rather, by learning to communicate, agents can better model the intent of other agents, leading to better coordination.\nIn humans, explicit communication for coordination assumes a common communication substrate to convey abstract concepts and beliefs directly , which may not be available for new partners. To align complex beliefs, heterogeneous agents must learn a message policy that translates from one theory of mind to another to synchronize coordination.\nEspecially when there is complex information to process and share, new agent partners need to learn to communicate to work with other agents. Emergent communication studies the creation of artificial language. Often phrased as a Lewis game, speakers and listeners learn a set of tokens to communicate complex observations .\nHowever, in multi-agent reinforcement learning (MARL), agents suffer from partial observability and non-stationarity (due to unaligned value functions) , which aims to be solved with decentralized learning through communication. In the MARL setup, agents, as speakers and listeners, learn a set of tokens to communicate observations, intentions, coordination, or other experiences which help facilitate solving tasks .\nAgents learn to communicate effectively through a backpropagation signal from their task performance . This has been found useful for applications in human-agent teaming , multirobot navigation , and coordination in complex games such as StarCraft II . Communication quality has been shown to have a strong relationship with task performance , leading to a multitude of work attempting to increase the representational capacity by decreasing the convergence rates .\nYet these methods still create degenerate communication protocols , which are uninterpretable due to joined concepts or null (lack of) information, which causes performance degradation. In this work, we investigate the challenges of learning a arXiv:2302.14276v1 LG] 28 Feb 2023 messaging lexicon to prepare emergent communication for social learning (EC4SL) scenarios.\nWe study the following hypotheses: H1) EC4SL will learn faster through structured concepts in messages leading to higher-quality solutions, H2) EC4SL aligns the policies of expert heterogeneous agents, and H3) EC4SL enables social shadowing, where an agent learns a communication policy while only observing an expert agent's action policy.\nBy learning a communication policy, the agent is encouraged to develop a more structured understanding of intent, leading to better coordination. The setting is very realistic among humans and many computer vision and RL frameworks may develop rich feature spaces for a specific solo task, but have not yet interacted with other agents, which may lead to failure without alignment.\nWe enable a compositional emergent communication paradigm, which exhibits clustering and informativeness properties. We show theoretically and through empirical results that compositional language enables independence properties among tokens with respect to referential information. Additionally, when combined with contrastive learning, our method outperforms competing methods that only ground communication on referential information.\nWe show that contrastive learning is an optimal critic for communication, reducing sample complexity for the unsupervised emergent communication objective. In addition to the more human-like format, compositional communication is able to create variable-length messages, meaning that we are not limited to sending insufficiently compressed messages with little information, increasing the quality of each communication.\nIn order to test our hypotheses, we show the utility of our method in multi-agent settings with a focus on teams of agents, high-dimensional pixel data, and expansions to heterogeneous teams of agents of varying skill levels. Social learning requires agents to explore to observe and learn from expert cues.\nWe interpolate between this form of social learning and imitation learning, which learns action policies directly from examples. We introduce a 'social shadowing' learning approach where we use first-person observations, rather than third-person observations, to encourage the novice to learn latently or conceptually how to communicate and develop an understanding of intent for better coordination.\nThe social shadowing episodes are alternated with traditional MARL during training. Contrastive learning, which works best with positive examples, is apt for social shadowing. Originally derived to enable lower complexity emergent lexicons, we find that the contrastive learning objective is apt for agents to develop internal models and relationships of the task through social shadowing.\nThe idea is to enable a shared emergent communication substrate (with minimal bandwidth) to enable future coordi-nation with novel partners. Our contributions are deriving an optimal critic for a communication policy and showing that the information bottleneck helps extend communication to social learning scenarios.\nIn real-world tasks such as autonomous driving or robotics, humans do not necessarily learn from scratch. Rather they explore with conceptually guided information from expert mentors. In particular, having structured emergent messages reduces sample complexity, and contrastive learning can help novice agents learn from experts.\nEmergent communication can also align heterogeneous agents, a social task that has not been previously studied.\n\nMulti-Agent Signaling\n\nImplicit communication conveys information to other agents that is not intentionally communicated . Implicit signaling conveys information to other agents based on one's observable physical position . Implicit signaling may be a form of implicit communication such as through social cues or explicit communication such as encoded into the MDP through \"cheap talk\" .\nUnlike implicit signaling, explicit signaling is a form of positive signaling that seeks to directly influence the behavior of other agents in the hopes that the new information will lead to active listening. Multi-agent emergent communication is a type of explicit signaling which deliberately shares information.\nSymbolic communication, a subset of explicit communication, seeks to send a subset of pre-defined messages. However, these symbols must be defined by an expert and do not scale to particularly complex observations and a large number of agents. Emergent communication aims to directly influence other agents with a learned subset of information, which allows for scalability and interpretability by new agents.\n\nEmergent Communication\n\nSeveral methodologies currently exist to increase the informativeness of emergent communication. With discrete and clustered continuous communication, the number of observed distinct communication tokens is far below the number permissible . As an attempt to increase the emergent \"vocabulary\" and decrease the data required to converge to an informative communication \"language\", work has added a bias loss to emit distinct tokens in different situations .\nMore recent work has found that the sample efficiency can be further improved by grounding communication in observation space with a supervised reconstruction loss . Information-maximizing autoencoders aim to maximize the state reconstruction accuracy for each agent. How-ever, grounding communication in observations has been found to easily satisfy these input-based objectives while still requiring a myriad more samples to explore to find a task-specific communication space .\nThus, it is necessary to use task-specific information to communicate informatively. This will enable learned compression for task completion rather than pure compression for input recovery. Other work aims to use the information bottleneck to decrease the entropy of messages . In our work, we use contrastive learning to increase representation similarity with future goals, which we show optimally optimizes the Q-function for messages.\n\nNatural Language Inspiration\n\nThe properties of the tokens in emergent communication directly affect their informative ability. As a baseline, continuous communication tokens can represent maximum information but lack human-interpretable properties. Discrete 1-hot (binary vector) tokens allow for a finite vocabulary, but each token contains the same magnitude of information, with equal orthogonal distance to each other token.\nSimilar to word embeddings in natural language, discrete prototypes are an effort to cluster similar information together from continuous vectors . Building on the continuous word embedding properties, VQ-VIB , an information-theoretic observation grounding based on VQ-VAE properties , uses variational properties to provide word embedding properties for continuous emergent tokens.\nLike discrete prototypes, they exhibit a clustering property based on similar information but are more informative. However, each of these message types determines a single token for communication. Tokens are stringed together to create emergent \"sentences\".\n\nPreliminaries\n\nWe formulate our setup as a decentralized, partially observable Markov Decision Process with communication (Dec-POMDP-Comm). Formally, our problem is defined by the tuple, S, A, M, T , R, O, Ω, γ . We define S as the set of states, A i , i ∈ [1, N ] as the set of actions, which includes task-specific actions, and M i as the set of communications for N agents.\nT is the transition between states due to the multi-agent joint action space T : S × A 1 , ..., A N → S. Ω defines the set of observations in our partially observable setting. Partial observability requires communication to complete the tasks successfully. O i : M 1 , ..., M N × Ŝ → Ω maps the communications and local state, Ŝ, to a distribution of observations for each agent.\nR defines the reward function and γ defines the discount factor.\n\nArchitecture\n\nThe policy network is defined by three stages: Observation Encoding, Communication, and Action Decoding. The best observation encoding and action decoding architecture is task-dependent, i.e., using multi-layer perceptrons (MLPs), CNNs , GRUs , or transformer layers are best suited to different inputs.\nThe encoder transforms observation and any sequence or memory information into an encoding H. The on-policy reinforcement learning training uses RE-INFORCE or a decentralized version of MAPPO as specified by our experiments. Our work focuses on the communication stage, which can be divided into three substages: message encoding, message passing (often considered sparse communication), and message decoding.\nWe use the message passing from . For message decoding, we build on a multiheaded attention framework, which allows an agent to learn which messages are most important . Our compositional communication framework defines the message encoding, as described in section 4.\n\nObjective\n\nMutual information, denoted as I(X; Y ), looks to measure the relationship between random variables, which is often measured through Kullback-Leibler divergence , I(X; Y ) = D KL (p(x, y)||p(x) ⊗ p(y)). The message encoding substage can be defined as an information bottleneck problem, which defines a tradeoff between the complexity of information (compression, I(X, X)) and the preserved relevant information (utility, I( X, Y )).\nThe deep variational information bottleneck defines a trade-off between preserving useful information and compression . We assume that our observation and memory/sequence encoder provides an optimal representation H i suitable for sharing relevant observation and intent/coordination information. We hope to recover a representation Y i , which contains the sufficient desired outputs.\nIn our scenario, the information bottleneck is a trade-off between the complexity of information I(H i ; M i ) (representing the encoded information exactly) and representing the relevant information I(M j =i ; Y i ), which is signaled from our contrastive objective. In our setup, the relevant information flows from other agents through communication, signaling a combination of the information bottleneck and a Lewis game.\nWe additionally promote complexity through our compositional independence objective, This is formulated by the following Lagrangian, where the bounds on mutual information Î are defined in equations 1, 2, and 10. Overall, our objective is,\n\nComplexity through Compositional Communication\n\nWe aim to satisfy the complexity objective, I(H i , M i ), through compositional communication. In order to induce complexity in our communication, we want the messages to be as non-random as possible. That is, informative with respect to the input hidden state h. In addition, we want each token within the message to share as little information as possible with the preceding tokens.\nThus, each additional token adds only informative content. Each token has a fixed length in bits W . The total sequence is limited by a fixed limit, L l W l ≤ S, of S bits and a total of L tokens. We use a variational message generation setup, which maps the encoded hidden state h to a message m; that is, we are modeling the posterior, π i m (m l |h).\nWe limit the vocabulary size to K tokens, e j ∈ R D , j ∈ [1, K] ⊂ N, where each token has dimensionality D and l ∈ [1, L] ⊂ N. Each token m l is sampled from a categorical posterior distribution, 0 otherwise such that the message m l is mapped to the nearest neighbor e j . A set of these tokens makes a message m.\nTo satisfy the complexity objective, we want to use m i to well-represent h i and consist of independently informative m i l .\n\nIndependent Information\n\nWe derive an upper bound for the interaction information between all tokens. Proposition 4.1. For the interaction information between all tokens, the following upper bound holds: The proof is in Appendix A.1. Since we want the mutual information to be minimized in our objective, we minimize,\n\nInput-Oriented Information\n\nIn order to induce complexity in the compositional messages, we additionally want to minimize the mutual information I(H; M ) between the composed message m and the encoded information h. We derive an upper bound on the mutual information that we use as a Lagrangian term to minimize. Proposition 4.2. For the mutual information between the composed message and encoded information, the following upper bound holds:\nThe proof is in Appendix A.1. Thus, we have our Lagrangian term, Conditioning on the input or observation data is a decentralized training objective.\n\nSequence Length\n\nCompositional communication necessitates an adaptive limit on the total length of the sequence. Corollary 4.3. Repeat tokens, w, are redundant and can be removed. Suppose one predicts two arbitrary tokens, w k and w l . Given equation 1, it follows that there is low or near-zero mutual information between w k and w l .\nA trivial issue is that the message generator will predict every available token as to follow the unique token objective. Since the tokens are imbued with input-oriented information (equation 2), the predicted tokens will be based on relevant referential details. Thus, it follows that tokens containing irrelevant information will not be chosen.\nA nice optimization objective that follows from corollary 4.3 is that one can use self-supervised learning with an end-ofsequence (EOS) token to limit the variable total length of compositional message sequences. (3) Algorithm 1 Compositional Message Gen.(h t ) m i ∼ N ( ĥ; µ, σ) 9: end for 10: return m\n\nMessage Generation Architecture\n\nNow, we can define the pipeline for message generation. The idea is to create an architecture that can generate features to enable independent message tokens. We expand each compressed token into the space of the hidden state h (1-layer linear expansion) since each token has a natural embedding in R |h| .\nThen, we perform attention using a softmin to help minimize similarity with previous tokens and sample the new token from a variational distribution. See algorithm 1 for complete details. During execution, we can generate messages directly due to equation 1, resolving any computation time lost from sequential compositional message generation.\n\nUtility through Contrastive Learning\n\nFirst, note that our Markov Network is as follows: H j → M j → Y i ← H i . Continue to denote i as the agent identification and j as the agent ID such that j = i. We aim to satisfy the utility objective of the information bottleneck, I(M j ; Y i ), through contrastive learning as shown in figure 1. Proposition 5.1.\nUtility mutual information is lower bounded by the contrastive NCE-binary objective, The proof is in Appendix A.1. This result shows a need for gradient information to flow backward across agents along communication edge connections.\n\nExperiments and Results\n\nWe condition on inputs, especially rich information (such as pixel data), and task-specific information. When evaluating an artificial language in MARL, we are interested in referential tasks, in which communication is required to complete the task. With regard to intent-grounded communication, we study ordinal tasks, which require coordination information between agents to complete successfully.\nThus, we consider tasks with a team of agents to foster messaging that communicates coordination information that also includes their observations. To test H1, structuring emergent messages enables lower complexity, we test our methodology and analyze the input-oriented information and utility capabilities.\nNext, we analyze the ability of heterogeneous agents to understand differing communication policies (H2)). Finally, we consider the effect of social shadowing (H3), in which agents solely learn a communication policy from an expert agent's action policy. We additionally analyze the role of offline reinforcement learning for emergent communication in combination with online reinforcement learning to further learn emergent communication alongside an action policy.\nWe evaluate each scenario over 10 seeds.\n\nEnvironments\n\nBlind Traffic Junction We consider a benchmark that requires both referential and ordinal capabilities within a team of agents. The blind traffic junction environment requires multiple agents to navigate a junction without any observation of other agents. Rather, they only observe their own state location.\nTen agents must coordinate to traverse through the lanes without colliding into agents within their lane or in the junction. Our training uses REINFORCE . Pascal VOC Game We further evaluate the complexity of compositional communication with a Pascal VOC . This is a two-agent referential game similar to the Cifar game but requires the prediction of multiple classes.\nDuring each episode, each agent observes a random image from the Pascal VOC dataset containing exactly two unique labels. Each agent must encode information given only the raw pixels from the original image such that the other agent can recognize the two class labels in the original image. An agent receives a reward of 0.25 per correctly chosen class label and will receive a total reward of 1 if both agents guess all labels correctly.\nSee figure 2. Our training uses heterogeneous agents trained with PPO (modified from MAPPO repository). For simplicity of setup, we consider images with exactly two unique labels from a closed subset of size five labels of the original set of labels from the Pascal VOC data. Furthermore, these images must be of size 375 × 500 pixels.\nThus, the resultant dataset comprised 534 unique images from the Pascal VOC dataset.\n\nBaselines\n\nTo evaluate our methodology, we compare our method to the following baselines: (1) no-comm, where agents do not communicate; (2) rl-comm, which uses a baseline communication method learned solely through policy loss ; (3) ae-comm, which uses an autoencoder to ground communication in input observations ; (4) VQ-VIB, which uses a variational autoencoder to ground discrete communication in input observations and a mutual information objective to ensure low entropy communication .\nWe provide an ablation of the loss parameter β in table 1 in the blind traffic junction scenario. When β = 0, we use our compositional message paradigm without our derived loss terms. We find that higher complexity and independence losses increase sample complexity. When β = 1, the model was unable to converge.\nHowever, when there is no regularization loss, the model performs worse (with no guarantees about referential representation). We attribute this to the fact that our independence criteria learns a stronger causal relationship. There are fewer spurious features that may cause an agent to take an incorrect action.\nIn order to understand the effect of the independent concept representation, we analyze the emergent language's capacity for redundancy. A message token m l is redundant if there exists another token m k that represents the same information. With our methodology, the emergent 'language' converges to the exact number of observations and intents required to solve the task.\nWith a soft discrete threshold, the independent information loss naturally converges to a discrete number of tokens in the vocabulary. Our β ablation in table 1 yields a bijection between each token in the vocabulary and the possible emergent concepts, i.e., the enumerated observations and intents. Thus for β = 0.1, there is no redundancy.\nSparse Communication In corollary 4.3, we assume that there is no mutual information between tokens. In practice, the loss may only be near-zero. Our empirical results yield independence loss around 1e − 4. In table 1, the size of the messages is automatically compressed to the smallest size to represent the information.\nDespite a trivially small amount of mutual information between tokens, our compositional method is able to reduce the message size in bits by 2.3x using our derived regularization, for a total of an 8x reduction in message size over non-compositional methods such as ae-comm. Since the base unit for the token is a 32-bit float, we note that each token in the message may be further compressed.\nWe observe that each token uses three significant digits, which may further compress tokens to 10 bits each for a total message length of 20 bits.\n\nCommunication Utility Results\n\nDue to coordination in MARL, grounding communication in referential features is not enough. Finding the communication utility requires grounding messages in ordinal information. Overall, figure shows that our compositional, contrastive method outperforms all methods focused on solely input-oriented communication grounding.\nIn the blind traffic junction, our method yields a higher average task success rate and is able to achieve it with a lower sample complexity. Training with the contrastive update tends to spike to high success but not converge, often many episodes before convergence, which leaves area for training improvement.\nThat is, the contrastive update begins to find aligned latent spaces early in training, but it cannot adapt the methodology quickly enough to converge. The exploratory randomness of most of the early online data prevents exploitation of the high utility f + examples. This leaves further room for improvement for an adaptive contrastive loss term.\nRegularization loss convergence After convergence to high task performance, the autoencoder loss increases in order to represent the coordination information. This follows directly from the information bottleneck, where there exists a tradeoff between utility and complexity. However, communication, especially referential communication, should have an overlap between utility and complexity.\nThus, we should seek to make the complexity loss more convex. Our compositional communication complexity loss does not converge before task performance convergence. While the complexity loss tends to spike in the exploratory phase, the normalized value is very small. Interestingly, the method eventually converges as the complexity loss converges below a normal- ized 0.3.\nAdditionally, the contrastive loss tends to decrease monotonically and converges after the task performance converges, showing a very smooth decrease. The contrastive f − loss decreases during training, which may account for success spikes prior to convergence. The method is able to converge after only a moderate decrease in the f + loss.\nThis implies empirical evidence that the contrastive loss is an optimal critic for messaging. See figure 3.\n\nHeterogeneous Alignment Through Communication\n\nIn order to test the heterogeneous alignment ability of our methodology to learn higher-order concepts from highdimensional data, we analyze the performance on the Pascal VOC game. We compare our methodology against ae-comm to show that concepts should consist of independent information directly from task signal rather than compression to reconstruct inputs.\nThat is, we show an empirical result on pixel data to verify the premise of the information bottleneck. Our methodology significantly outperforms the observation-grounded ae-comm baseline, as demonstrated by figure 4. The ae-comm methodology, despite using autoencoders to learn observation-grounded communication, performs only slightly better than no-comm.\nOn the other hand, our methodology is able to outperform both baselines significantly. It is important to note that based on figure 4, our methodology is able to guess more than two of the four labels correctly across the two agents involved, while the baseline methodologies struggle to guess exactly two of thew four labels consistently.\nThis can be attributed to our framework being able to learn compositional concepts that are much more easily discriminated due to mutual independence.\n\nSocial Shadowing\n\nCritics of emergent communication may point to the increased sample complexity due to the dual communication and action policy learning. In the social shadowing scenario, heterogeneous agents can learn to generate a communication policy without learning the action policy of the watched expert agents. To enable social shadowing, the agent will alternate between a batch of traditional MARL (no expert) and (1st-person) shadowing an expert agent performing the task in its trajectory.\nThe agent only uses the contrastive objective to update its communication policy during shadowing. In figure , the agent that performs social shadowing is able to learn the action policy with almost half the sample complexity required by the online reinforcement learning agent. Our results show that the structured latent space of the emergent communication learns socially benevolent coordination.\nThis tests our hypothesis that by learning communication to understand the actions of other agents, one can enable lower sample complexity coordination. Thus, it mitigates the issues of solely observing actions.\n\nDiscussion\n\nBy using our framework to better understand the intent of others, agents can learn to communicate to align policies and coordinate. Any referential-based setup can be performed with a supervised loss, as indicated by the instant satisfaction of referential objectives. Even in the Pascal VOC game, which appears to be a purely referential objective, our results show that intelligent compression is not the only objective of referential communication.\nThe emergent communication paradigm must enable an easy-to-discriminate space for the game. In multi-agent settings, the harder challenge is to enable coordination through communication. Using contrastive communication as an optimal critic aims to satisfy this, and has shown solid improvements. Since contrastive learning benefits from good examples, this method is even more powerful when there is access to examples from expert agents.\nIn this setting, the communication may be bootstrapped, since our optimal critic has examples with strong signals from the 'social shadowing' episodes. Additionally, we show that the minimization of our independence objective enables tokens that contain minimal overlapping information with other tokens.\nPreventing trivial communication paradigms enables higher performance. Each of these objectives is complementary, so they are not trivially minimized during training, which is a substantial advantage over comparative baselines. Unlike prior work, this enables the benefits of training with reinforcement learning in multi-agent settings.\nIn addition to lower sample complexity, the mutual information regularization yields additional benefits, such as small messages, which enables the compression aspect of sparse communication. From a qualitative point of view, the independent information also yields discrete emergent concepts, which can be further made human-interpretable by a post-hoc analysis .\nThis is a step towards white-box machine learning in multi-agent settings. The interpretability of this learned white-box method could be useful in human-agent teaming as indicated by prior work . The work here will enable further results in decision-making from high-dimensional data with emergent concepts.\nThe social scenarios described are a step towards enabling a zero-shot communication policy. This work will serve as future inspiration for using emergent communication to enable ad-hoc teaming with both agents and humans.\n\nAppendix\n\nA.1. Proofs Proposition 4.1 For the interaction information between all tokens, the following upper bound holds: Proof. Starting with the independent information objective, we want to minimize the interaction information, which defines the conditional mutual information between each token and, Let π i m (m l |h) be a variational approximation of p(m l |h), which is defined by our message encoder network.\nGiven that each token should provide unique information, we assume independence between m l . Thus, it follows that our compositional message is a vector, m = [m 1 , . . . , m L ], and is jointly Gaussian. Moreover, we can define q( m|h) as a variational approximation to p(m|h) = p(m 1 ; . . . , m L |h).\nWe can model q with a network layer and define its loss as || m − m|| 2 . Thus, transforming equation 4 into variational form, we have, it follows that q( m|h) log q( m|h)d m ≥ q( m|h) log Thus, we can bound our interaction information, Proposition 4.2 For the mutual information between the composed message and encoded information, the following upper bound holds:\nProof. By definition of mutual information between the composed messages M and the encoded observations H, we have, Substituting q( m|h) for p( m|h), the same KL Divergence identity, and defining a Gaussian approximation z( m) of the marginal distribution p( m), it follows that, In expectation of equation 1, we have,\nThis implies that, for m = [m 1 , . . . , m L ], there is probabilistic independence between m j , m k , j = k. Thus, expanding, it follows that, where z(m l ) is a standard Gaussian. Proposition 5.1. Utility mutual information is lower bounded by the contrastive NCE-binary objective, Proof. We suppress the reliance on h since this is directly passed through.\nBy definition of mutual information, we have, Our network model learns π R + (y|m) from rolled-out trajectories, R + , using our policy. The prior of our network state, π R − (y), can be modeled from rolling out a random trajectory, R−. Unfortunately, it is intractable to model π R + (y|m) and π R − (y) directly during iterative learning, but we can sample y + ∼ π R + (y|m) and y − ∼ π R − (y) directly from our network during training.\nIt has been shown that log p(y|m) provides a bound on mutual information , with the expectation over l p(m l , y l ). However, we need a tractable understanding of the information Y . In the information bottleneck, Y represents the desired outcome. In our setup, y is coordination information that helps create the desired output, such as any action a − .\nThis implies, y =⇒ a − . Since the transition is known, it follows that a − =⇒ s − f , a random future state. Thus, we have, π This is similar to the proof for lemma A.5, but requires assumptions on messages m from the emergent language. We note that when m is random, the case defaults to lemma A.5. Thus, we assume we have at least input-oriented information in m given sufficiently satisfying equation 2. Given a sufficient emergent language, it follows that y =⇒ a + , where a + is an intention action based on m.\nSimilarly, since the transition is known, a + =⇒ s + f , a desired goal state along the trajectory. Thus, we have, π R + (y|m) = p(s = s + f |y, m). Recall the following (as shown in ), which we have adapted to our communication objective, Proposition A.3 (rewards → probabilities). The Q-function for the goal-conditioned reward function r g (s t , m t ) = (1 − γ)p(s = s g |y t ) is equivalent to the probability of state s g under the discounted state occupancy measure:\nand Lemma A.4. The critic function that optimizes equation 8 is a Q-function for the goal-conditioned reward function up to a multiplicative constant 1 The critic function f (s, m, s f ) = y enc(s f ) represents the similarity between the encoding y = enc(s, m) and the encoding of the future rollout s f .\nGiven lemmas A.5 A.6 A.8 and proposition A.7, it follows that equation 8 is the NCE-binary (InfoMAX ) objective, Î(M j , Y i ) = log σ(f (s, m, s + f )) + log 1 − σ(f (s, m, s − f )) which lower bounds the mutual information, I(M j , Y i ) ≥ Î(M j , Y i ). The critic function is unbounded, so we constrain it to [0, 1] with the sigmoid function, σ( * ).\nWe suppress the reliance on h since this is directly passed through. By definition of mutual information, we have, Our network model learns π R + (y|m) from rolled-out trajectories, R + , using our policy. The prior of our network state, π R − (y), can be modeled from rolling out a random trajectory, R−.\nUnfortunately, it is intractable to model π R + (y|m) and π R − (y) directly during iterative learning, but we can sample y + ∼ π R + (y|m) and y − ∼ π R − (y) directly from our network during training. It has been shown that log p(y|m) provides a bound on mutual information , with the expectation over l p(m l , y l ).\nHowever, we need a tractable understanding of the information Y . Lemma A.5. π R − (y) = p(s = s − f |y). In the information bottleneck, Y represents the desired outcome. In our setup, y is coordination information that helps create the desired output, such as any action a − . This implies, y =⇒ a − . Since the transition is known, it follows that a − =⇒ s − f , a random future state.\nThus, we have, π R − (y) = p(s = s − f |y). Lemma A.6. π R + (y|m) = p(s = s + f |y, m). This is similar to the proof for lemma A.5, but requires assumptions on messages m from the emergent language. We note that when m is random, the case defaults to lemma A.5. Thus, we assume we have at least input-oriented information in m given sufficiently satisfying equation 2. Given a sufficient emergent language, it follows that y =⇒ a + , where a + is an intention action based on m.\nSimilarly, since the transition is known, a + =⇒ s + f , a desired goal state along the trajectory. Thus, we have, π R + (y|m) = p(s = s + f |y, m). Recall the following (as shown in ), which we have adapted to our communication objective, Proposition A.7 (rewards → probabilities). The Q-function for the goal-conditioned reward function r g (s t , m t ) = (1 − γ)p(s = s g |y t ) is equivalent to the probability of state s g under the discounted state occupancy measure:\nand Lemma A.8. The critic function that optimizes equation 8 is a Q-function for the goal-conditioned reward function up to a multiplicative constant 1 p(s f ) : exp(f * (s, m, s f ) = 1 p(s f ) Q π s f (s, m). The critic function f (s, m, s f ) = y enc(s f ) represents the similarity between the encoding y = enc(s, m) and the encoding of the future rollout s f .\nGiven lemmas A.5 A.6 A.8 and proposition A.7, it follows that equation 8 is the NCE-binary (InfoMAX ) objective, which lower bounds the mutual information, I(M j , Y i ) ≥ Î(M j , Y i ). The critic function is unbounded, so we constrain it to [0, 1] with the sigmoid function, σ( * ).", "answers": ["An unsupervised method based on the information bottleneck and contrastive learning."], "length": 6235, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "120cb783c796fbedbc76f04cf9be3318e54a63cd642c4401"}
{"input": "What did the decision to base the water rates on usage reflect?", "context": "Time to clean house in Paso Robles Home\nFront Page » Time to clean house in Paso Robles\nSeptember 5, 2010 Opinion By JIM REED\nI’d like to give you an update on the issue of our civil servants cramming hundreds of millions of dollars in spending down our throats after the people of Paso Robles voted down the water rate increase last November. The rate increase is being hung up in the courts by the City Attorney. What was supposed to be a quick issue to get in front of a judge, has been drug out as long as possible by the City Attorney.\nEven if the courts throw out the current rate increase, I expect that our civil servants will just change a couple of words in the rate increase notice and force the same old plan on us again.\nThere is a real problem with the people we have hired to work for us in Paso Robles. It seems that decisions are made based on some agenda, even if it is contrary to citizens’ wishes.\nCity Councilmen Ed Steinbeck, Nick Gilman and Mayor Duane Picanco, on August 19th, voted unanimously to hire the same law firm employed by the City of Bell. You may have heard the recent news story about the City of Bell’s corrupt city representatives.\nThis law firm allowed the elected officials and City employees to pillage the General Fund for their own benefit, contrary to the rights and interests of the citizens. We are already paying several City employees $12,000 per month with equally ridiculous benefits and pensions. What does this say about our elected representatives?\nI believe most residents are like me. We elect people we believe have our best interest in mind. Over the last few years I have seen that nothing is farther from the truth. The people we have elected have lost track of the fact that “the City” exists to protect and deliver services to the citizens. To them it is some all-important ideal they strive to cultivate and improve according to their agenda. They have forgotten that they are elected to represent the citizens.\nWe have an election coming up in November. We have the opportunity to elect some responsible, principled people to represent us. If we elect more people from within this system, we will get more of the same type of government. We need to look at where the new candidates stand. Will they lawfully represent the citizens of the city? Or, are they happy with the way things are being run?\nWe have stood together in the past and have made real significant changes in important matters that are going to affect our lives for years to come. There are several thousand citizens that made their voice heard on the water issue, more than enough votes to make a change in our city government.\nPlease come out and vote for a democratic representative governing body for Paso Robles instead of the tyrannical leadership that exists now.\nJim Reed is a longtime resident of Paso Robles.\nSubjects: Opinion Paso Robles Paso Robles City Council Vote\tRelated:\n<- Previous Next ->\tEndless Summer Nights at Edna Valley, event photos Trial postponed for Paso Robles woman accused of forgery The comments below represent the opinion of the writer and do not represent the views or policies of CalCoastNews.com. (moderator@calcoastnews.com Comment Guidelines )\n2 whatisup says:\t09/13/2010 at 9:27 pm\npasoobserver – Here is something to observe and get you going in the right direction:\nCalifornia Government Code Section 65584\n(a) (1) For the fourth and subsequent revisions of the\nhousing element pursuant to Section 65588, the department shall\ndetermine the existing and projected need for housing for each region\npursuant to this article. For purposes of subdivision (a) of Section\n65583, the share of a city or county of the regional housing need\nshall include that share of the housing need of persons at all income\nlevels within the area significantly affected by the general plan of\n(2) While it is the intent of the Legislature that cities,\ncounties, and cities and counties should undertake all necessary\nactions to encourage, promote, and facilitate the development of\nhousing to accommodate the entire regional housing need, it is\nrecognized, however, that future housing production may not equal the\nregional housing need established for planning purposes.\n(b) The department, in consultation with each council of\ngovernments, shall determine each region’s existing and projected\nhousing need pursuant to Section 65584.01 at least two years prior to\nthe scheduled revision required pursuant to Section 65588. The\nappropriate council of governments, or for cities and counties\nwithout a council of governments, the department, shall adopt a final\nregional housing need plan that allocates a share of the regional\nhousing need to each city, county, or city and county at least one\nyear prior to the scheduled revision for the region required by\nSection 65588. The allocation plan prepared by a council of\ngovernments shall be prepared pursuant to Sections 65584.04 and\n65584.05 with the advice of the department.\n(c) Notwithstanding any other provision of law, the due dates for\nthe determinations of the department or for the council of\ngovernments, respectively, regarding the regional housing need may be\nextended by the department by not more than 60 days if the extension\nwill enable access to more recent critical population or housing\ndata from a pending or recent release of the United States Census\nBureau or the Department of Finance. If the due date for the\ndetermination of the department or the council of governments is\nextended for this reason, the department shall extend the\ncorresponding housing element revision deadline pursuant to Section\n65588 by not more than 60 days.\n(d) The regional housing needs allocation plan shall be consistent\nwith all of the following objectives:\n(1) Increasing the housing supply and the mix of housing types,\ntenure, and affordability in all cities and counties within the\nregion in an equitable manner, which shall result in each\njurisdiction receiving an allocation of units for low- and very low\n(2) Promoting infill development and socioeconomic equity, the\nprotection of environmental and agricultural resources, and the\nencouragement of efficient development patterns.\n(3) Promoting an improved intraregional relationship between jobs\n(4) Allocating a lower proportion of housing need to an income\ncategory when a jurisdiction already has a disproportionately high\nshare of households in that income category, as compared to the\ncountywide distribution of households in that category from the most\nrecent decennial United States census.\n(e) For purposes of this section, “household income levels” are as\ndetermined by the department as of the most recent decennial census\npursuant to the following code sections:\n(1) Very low incomes as defined by Section 50105 of the Health and\n(2) Lower incomes, as defined by Section 50079.5 of the Health and\n(3) Moderate incomes, as defined by Section 50093 of the Health\nand Safety Code.\n(4) Above moderate incomes are those exceeding the moderate-income\nlevel of Section 50093 of the Health and Safety Code.\n(f) Notwithstanding any other provision of law, determinations\nmade by the department, a council of governments, or a city or county\npursuant to this section or Section 65584.01, 65584.02, 65584.03,\n65584.04, 65584.05, 65584.06, 65584.07, or 65584.08 are exempt from\nthe California Environmental Quality Act (Division 13 (commencing\nwith Section 21000) of the Public Resources Code).\npasoobserver says:\t09/13/2010 at 6:52 pm\nTo whatisup —- First of all, I reviewed AB 602 Assembly Bill. Thanks. I am sorry to inform you but AB 602 is not the LAW as you so stated in your blog. I contacted the Deputy Chief Council’s office in Sacramento handling AB 602 to confirm your misstatement of facts. You know,in the English language, It shouldn’t be so difficult to answer some simple questions with a “YES” or “NO” answer. Yet, you are reluctant to do so, but you go on and on with a thesis along with some rhetoric. I never talked about a court suit over the “water issue”, I asked YOU, not about waiting for a court decision. Maybe, you did with some other people. Also, I was not ranting about the wineries usage of water. My response to you on your vague question about “there are people not paying their fair share for their use of water”. I related, are you talking about the wineries? I am well aware that most of the wineries are outside the city limits using the same aquifer. You took my question out of context., nice try! You are just being a popinjay and rhetorical. Also, you didn’t answer another question about “what is the unit cost of water” in Templeton? as compared to Paso Robles.\nwhatisup says:\t09/13/2010 at 8:54 pm\nI am on a well. I am sure you are capable of doing your own homework. I also am quite sure if you really contacted the Deputy Chief Counsel’s Office you have been set straight. What I gave you is a proposed small adjustment in the wide range of laws that make up the California Housing element. I assumed you could stumble onto the facts based on what I gave you. By the way, I believe you can review the Paso Robles Housing element plan on the City’s website or at the Library. The California Housing Element Laws that all cities and counties have to follow have been in place for almost 25 years. I realize you don’t actually have a clue how to look the laws up. Either educate yourself or keep making a fool of yourself, your choice. A simple Google search of California Housing Element Laws will get you going. Good Luck!\nTO WHATISUP — I WOULD LIKE TO KNOW WHAT LAW YOU ARE REFERRING TO THAT SAYS “WE” THE PEOPLE HAVE TO SUBSIDIZE NEW DEVELOPMENT? AGAIN, FOR THE THIRD TIME, YOU FAILED TO ANSWER MY QUESTIONS POSED TO YOU IN MY PRIOR RESPONSES TO YOU ON SEPT.10TH &11TH. IS THERE A REASON WHY YOU DON’T WANT TO ANSWER THEM? YOU DO WHAT OUR ELECTED OFFICIALS DO SO WELL, AND THAT IS “IN ONE EAR AND OUT OF THE OTHER EAR” IT SEEMS TO ME THAT YOU ARE EITHER EMPLOYED BY THE CITY OR YOU HAVE OTHER DEALING WITH THE CITY, SO BE IT. IT APPEARS TO ME THAT YOU THINK THE CITY DOES EVERYTHING RIGHT. APPARENTLY, YOU PRESENT YOURSELF AS BEING VERY BIAS ON CITY DECISIONS. IT LIKE THEY CAN’T DO ANYTHING WRONG ACCORDING TO YOUR LOGIC. THEY KNOW WHAT IS BEST FOR THE CITIZENS OF PASO,THAT IS A GOOD EXAMPLE OF ARROGANCE ALONG WITH NARCISSISM.\nWHAT PEOPLE ARE YOU TALKING ABOUT THAT DOESN’T PAY THEIR FAIR SHARE OF WATER? ARE YOU REFERRING TO THE WINERIES USING THE SAME AQUIFER?\nI BELIEVE YOU RELATED THAT YOU RESIDE IN TEMPLETON, BUT YOU OWN PROPERTY IN PASO. BY THE WAY, WHAT IS THE COST PER UNIT OF WATER USAGE IN TEMPLETON COMPARED TO PASO? OF COURSE, TEMPLETON IS IN AN UNINCORPORATED AREA (COUNTY JURISDICTION).\nWELL, I GAVE YOU SOME SUGGESTIONS ON HOW TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT. ALSO, REMEMBER IT’S THE CITIZENS’ MONEY THAT IS BEING SPENT. WHAT IS MOST IMPORTANT OF ALL, IS LET THE CITIZENS OF PASO DECIDE WITH THEIR VOTE ON HOW TO FINANCE THIS HUGE CAPITAL IMPROVEMENT PROJECT EXPENDITURE. JUST BE IN COMPLIANCE WITH STATE PROPOSITION 218 AND STOP CIRCUMVENTING THE LAW.\nWOULD YOU OBJECT TO HAVING TO FINANCE SOME NEW BONDS ON YOUR PROPERTY TAX BILL AS A ” SPECIAL TAX” OR AN ASSESSMENT TAX” TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT? A PERCENTAGE OF PASO CITIZENS FINANCE LOCAL SCHOOL BONDS ON THEIR PROPERTY TAX BILL AND DON’T HAVE ANY KIDS GOING TO SCHOOL. HOW ABOUT THAT COMPARISON FOR YOU TO THINK ABOUT? WHAT SAY YOU?\nI say less CapsLock, please.\nwhatisup says:\t09/12/2010 at 11:41 pm\nI have answered your questions. I have been quite detailed in my answers and I am sorry if you can’t deal with the detail. I guess it is your inconvenient truth. You do seem to like to deflect and go around in circles. Another example, now you are ranting about the wineries using the same aquaifier as the City. Let me be clear for you, I don’t like the amount of water the wineries are using. However, the wineries are in the County, not in the City and the City can’t do anything about it. They wineries are allowed to take the water they are taking even if it drops the City’s water levels in their wells. You need to complain to Sacramento. It sounds like you just don’t want to pay anything for the infrastructure because you really just don’t want it built.\nSeveral of your observations of my opinions are bizarre considering I have stated several times I believe the Courts need to decide if Paso Robles has, or has not followed the rules as to funding the infrastucture. Obviously, as I have stated before, if the City loses the lawsuit the infrastructure will have to be paid out of the City’s General Fund until a new method of payment is voted on by the Citizens of Paso Robles. Pretty clear.\nYour idea of charging based on a special assesment rather than the amount of water a property uses means that people who use little water, but live on a more expensive property will pay more than their share, based on their water usage. In addition, how do you deal with a rental unit where the renter is supposed to pay the water bill? Your idea is inherantly unfair, but my guess is it will favor you, so you don’t care if it is unfair and other people would pay part of your share. You also have decided that since I have alternative ideas to yours I must work for, or have business with the City of Paso Robles, another attempt to deflect from the issue. However, once again, I have never worked for the City or have ever done business with the City and don’t expect to ever do business with the City. I do own property in the City which is why I pay attention. Finally, it turns out there needs to be a fix to the housing element laws, the existance of which you are questioning. As I understand it the fix to the housing elemnt laws is because of some lawsuit. This should give you all the information you need to educate yourself on the California Housing Element laws that every city and county in California has to follow:\nBILL ANALYSIS ————————————————————\n|SENATE RULES COMMITTEE | AB 602|\n|Office of Senate Floor Analyses | |\n|1020 N Street, Suite 524 | |\n|(916) 651-1520 Fax: (916) | |\n|327-4478 | |\n———————————————————— THIRD READING\nBill No: AB 602\nAuthor: Feuer (D), et al\nAmended: 8/20/10 in Senate\nSENATE TRANSPORTATION & HOUSING COMM : 6-3, 6/29/10\nAYES: Lowenthal, DeSaulnier, Kehoe, Pavley, Simitian, Wolk\nNOES: Huff, Ashburn, Harman\nASSEMBLY FLOOR : Not relevant\nSUBJECT : Statute of limitations on housing element\nSOURCE : California Rural Legal Assistance Foundation\nHousing California DIGEST : This bill states the intent of the Legislature\nin enacting this bill to modify the courts opinion in Urban\nHabitat Program v. City of Pleasanton (2008) 164\nCal.App.4th 1561, with respect to the interpretation of\nSection 65009 of the Government Code, and revises and\nclarifies statute of limitations and remedies for specified\nhousing related challenges.\nSenate Floor Amendments of 8/20/10 revise the statute of\nlimitations and remedies for specified housing-related\nANALYSIS : The Planning and Zoning Law requires cities\nand counties to prepare and adopt a general plan, including\na housing element, to guide the future growth of a\ncommunity. Following a staggered statutory schedule,\ncities and counties located within the territory of a\nmetropolitan planning organization (MPO) must revise their\nhousing elements every eight years, and cities and counties\nin rural non-MPO regions must revise their housing elements\nevery five years. These five- and eight-year periods are\nknown as the housing element planning period.\nBefore each revision, each community is assigned its fair\nshare of housing for each income category through the\nregional housing needs assessment (RHNA) process. A\nhousing element must identify and analyze existing and\nprojected housing needs, identify adequate sites with\nappropriate zoning to meet its share of the RHNA, and\nensure that regulatory systems provide opportunities for,\nand do not unduly constrain, housing development. The\nreviews both draft and adopted housing elements to\ndetermine whether or not they are in substantial compliance\nwith the law. The Planning and Zoning Law and the Subdivision Map Act\nalso includes a number of sections governing zoning and\nentitlements specifically related to housing, including:\n? The Housing Accountability Act, which requires a city or\ncounty to make one or more specified findings in order to\ndisapprove a particular housing development.\n? A provision requiring cities and counties, when adopting\nan ordinance which limits the number of housing units\nwhich may be constructed on an annual basis, to make\nfindings as to the public health, safety, and welfare\nbenefits that justify reducing the housing opportunities\nof the region. ? Density bonus law, which requires cities and counties to\ngrant a developer a density bonus, incentives, and\nconcessions when the developer proposes to include\nspecified percentages of affordable housing within a\ndevelopment. ? The Least Cost Zoning Law, which requires cities and AB 602\ncounties to designate and zone sufficient vacant land for\nresidential use with appropriate standards to meet\nhousing needs for all income categories and to contribute\nto producing housing at the lowest possible cost.\n? A requirement that, when determining whether to approve a\ntentative subdivision map, a city or county shall apply\nonly those ordinances, policies, and standards in effect\nas of the date the developer’s application is deemed\nPrior to a recent court decision, it was understood that\ncurrent law allowed a party to challenge the adequacy of a\ncity’s or county’s housing element at any time during a\nplanning period, provided that the challenger brought the\naction “in support of or to encourage or facilitate the\ndevelopment of housing that would increase the community’s\nsupply of [affordable] housing.” The challenging party was\nrequired first to serve the city or county with a notice\nidentifying the deficiencies in the housing element. After\n60 days or the date on which the city or county took final\naction in response to the notice, whichever occurred first,\nthe challenging party had one year to file the action in\ncourt. This process and statute of limitations also\napplied to actions brought pursuant to the housing-related\nstatutes listed above. In 2006 Urban Habitat Program brought suit to challenge the\nCity of Pleasanton’s housing policies, including the city’s\nannual cap on housing permits and the city’s cap on the\naggregate number of permissible housing units, both of\nwhich Urban Habitat claimed were insufficient to allow the\ncity to meet its RHNA obligation. In 2008, the First\nDistrict California Court of Appeals issued an unpublished\ndecision in the case of Urban Habitat Program v. City of\nPleasanton allowing the case to proceed with respect to\nsome causes of action, but ruling that the challenge to the\nhousing element itself was time-barred. The court stated:\nAlthough the statute does not specify the time within\nwhich [a deficiency] notice must be given, it is our\nconclusion that the statute must be interpreted as\ncontaining a time limit within which this requirement\nmust be met? In sum, a party bringing a challenge AB 602\ngoverned by section 65009, subdivision (d), has 90\ndays from the date a legislative action is taken or\napproval is given to notify the local land use\nauthority of any claimed deficiencies in such an\naction or approval. Its claim then accrues 60 days\nafter it gives this notice.\nIn other words, instead of being able to initiate a\nchallenge to a deficient housing element at any time during\nthe planning period, housing advocates and other interested\nparties may now only initiate such a challenge by\nsubmitting a deficiency notice within 90 days of the\nhousing element’s adoption.\n1.Removes from the current list of city or county actions\nwhich may be challenged pursuant to Government Code 65009\nnotice and accrual provisions those actions related to\nthe Housing Accountability Act, the Subdivision Map Act,\nand the application of a Density Bonus ordinance to a\nparticular project, all of which are project-specific\nactions. The bill maintains the ability to use these\nnotice and accrual provisions to challenge the adequacy\nof a city’s or county’s density bonus ordinance\n2.Extends lengthening the time in which a deficiency notice\nmay be served to cover all remaining city or county\nactions described in this section of law, as opposed to\njust housing element challenges. In other words, the\namendments apply the longer timeframe to serve the\ndeficiency notice to actions relating to the Least Cost\nZoning Law, annual limits on housing permits, and the\nadequacy of a density bonus ordinance, in addition to\nhousing element law. 3.Provides that an entity challenging such an action in\nsupport of affordable housing may serve the deficiency\nnotice up to five years after the city’s or county’s\naction. After 60 days or the date on which the city or\ncounty takes final action in response to the notice,\nwhichever occurs first, the challenging party has one\nyear to file an action in court, except that the lawsuit AB 602\nmay not be filed more than five years after the city’s or\ncounty’s action. In other words, the entity must file\nthe lawsuit within one year of the expiration of the\ndeficiency notice or within five years of the city’s or\ncounty’s action, whichever occurs first.\n4.Provides that a housing element from a prior planning\nperiod may not be challenged if the city or county has\nadopted a revised housing element for the new planning\nGovernment Code 65755 . Current law requires a court, if it\nfinds any portion of a general plan, including a housing\nelement, out of compliance with the law, to include within\nits order or judgment one or more of the following remedies\nfor any or all types of developments or any or all\ngeographic segments of the city or county until the city or\ncounty has complied with the law:\n? Suspend the authority of the city or county to\nissue building permits.\ngrant zoning changes and/or variances.\ngrant subdivision map approvals.\n? Mandate the approval of building permits for\nresidential housing that meet specified criteria.\n? Mandate the approval of final subdivision maps for\nhousing projects that meet specified criteria.\n? Mandate the approval of tentative subdivision maps\nfor residential housing projects that meet specified\nThis bill clarifies that in any action or proceeding\nbrought pursuant to the notice and accrual provisions of\nGovernment Code Section 65009 described above, neither the\ncourt remedies described above nor any injunction against\nthe development of a housing project shall abrogate,\nimpair, or otherwise interfere with the full exercise of\nthe rights and protections granted to an applicant for a\ntentative map or a vesting tentative map under specified\nprovisions of the Subdivision Map Act or to a developer\nunder a specified provision relating to development AB 602\nUnder current law, HCD operates a number of grant programs\nto which cities and counties may apply. In many cases, the\nlaw requires a city or county to have an HCD-approved\nhousing element in order to be eligible for funding. This bill provides that if a third-party challenges the\nadequacy of a housing element in court and the court finds\nthat the housing element substantially complies with all of\nthe requirements of housing element law, the element shall\nbe deemed to be in compliance for purposes of state housing\nThe statutory language interpreted by the court and at\nissue in this bill was added to statute by AB 998 (Waters),\nChapter 1138, Statutes of 1983, a bill sponsored by the\nLeague of California Cities and the California Building\nIndustry Association. AB 998 created a short statute of\nlimitations period for land use decisions generally but\nprovided a specific exception to protect the ability to\nchallenge deficient housing elements. The Senate Housing\nand Land Use Committee and the Senate Third Reading\nanalysis of the bill stated that the bill:\nSpecifies that for challenges in support of low- and\nmoderate-income housing requirements, the petitioner\nshall notice local government 60 days prior to filing\naction. The [one-year] statute of limitations then\nbegins on the first day the legislative body fails to\nIn the intervening 25 years prior to the Urban Habitat\nruling, housing advocates filed and successfully settled at\nleast ten cases in which the 60-day deficiency notice was\nsent more than 90 days after adoption of the city’s or\ncounty’s housing element. In none of these cases was the\ntimeliness on the advocates’ suit contested. Likewise, six\nbills amended other portions of this statute during those\nintervening years, and there was never any controversy\nsurrounding the lack of a deadline for housing advocates to\nserve a deficiency notice nor any attempt to change the AB 602\nstatute in this regard. Current level of housing element compliance . According to\nHCD’s website as of June 7, 2010, only 46 percent of cities\nand counties have adopted an HCD-approved housing element\nfor the current planning period that began in 2005 for the\nSan Diego region, 2008 for the Southern California, Fresno,\nKern, and Sacramento regions, and the summer of 2009 for\nthe remaining areas of the state. Unlocking the private market . The purpose of housing\nelement law is to create opportunities for the private\nhousing market to function. Builders cannot build without\naccess to appropriately zoned land, and current land use\nplans in many cities and counties in California fail to\nprovide sufficient opportunities to accommodate projected\npopulation growth. The San Diego Association of\nGovernments’ Regional Comprehensive Plan describes this\ntypical California paradox in the following way:\nUnder current plans and policies, more than 90 percent\nof [the San Diego region’s] remaining vacant land\ndesignated for housing is planned for densities of\nless than one home per acre, and most is in the rural\nback country areas dependent upon scarce groundwater\nsupplies. And of the remaining vacant land planned for\nhousing in the 18 incorporated cities, only about\nseven percent is planned for multifamily housing. When\ntaken together, the current land use plans of the 19\nlocal jurisdictions do not accommodate the amount of\ngrowth anticipated in our region. SANDAG’s population\nforecast, which reflects the current adopted local\nland use plans in the region, projects that while\npopulation will increase by 37 percent by 2030,\nhousing will grow by just 30 percent. The forecast\nshows that if local plans are not changed, demand for\nhousing will continue to outpace the supply, just as\nHousing element law addresses this problem directly by\nrequiring cities and counties to zone land at appropriate\ndensities to accommodate the projected housing needs of all\nincome groups and to remove constraints that prevent such\nsites from being developed at the allowed densities. AB 602\nCities and counties, however, are not required to build\nhousing because that is the role of private developers.\nThe law holds cities and counties accountable only for that\nwhich they control: zoning and land use entitlements.\nWithout the ability to enforce housing element law, the\nmarket’s ability to meet housing demand may well remain\nlocked up.\nFISCAL EFFECT : Appropriation: No Fiscal Com.: No\nSUPPORT : (Verified 8/23/10)\nCalifornia Rural Legal Assistance Foundation (co-source)\nHousing California (co-source)\nAdvocates for Affordable Homes in Fremont\nCalifornia Coalition for Rural Housing\nCommunity Housing Improvement Program\nCommunity Housing Works\nEden Housing\nFair Housing of Marin\nGrassroots Leadership Network of Marin\nKennedy Commission\nPublic Advocates, Inc\nSan Diego Housing Federation\nSelf-Help Enterprises\nSierra Club of California\nAmerican Planning Association, California Chapter\nJA:nl 8/23/10 Senate Floor Analyses SUPPORT/OPPOSITION: SEE ABOVE\npasoobserver says:\t09/11/2010 at 11:17 pm\nTo whatisup — Thank you for your response to my comments. However, you failed to answer some of my questions that I mentioned to you. It’s almost like dealing with some City officials. They just let the public vent at their bimonthly council meetings. In my opinion, it’s difficult to deal with narcissism and arrogance. Over the years, there has been some very good input to our elected officials on how to proceed on the Nacimiento water pipeline,but it fell on deaf ears. You wanted me to answer some of your questions,but you did not answer some of my questions. Again, are you willing to subsidize new development?,Yes?or No?, are you willing to pay for a commodity that you are not receiving? Yes?or No? and another question for you. Are you willing to pay over 300% on your water bills within the five (5) year plan that the City has proposed? Also, the water rates will be subject to later increases too. By the way, I do concur with the city’s plan of “you pay for the amount of water units you use”. (748 gal=one unit). However, the higher water rates are not good for our senior citizens on fixed incomes and other struggling families in our community. My first suggestion years ago was desalination. The response was it was too expensive. Of course, now it is more expensive. I would suggest that our elected officials recall the existing bonds (The bonds can be recalled early). The City council can explain to the citizens in detail with financing of new bonds at a lower interest rate as of now for the sewer plant and Nacimiento water pipeline and present their new proposal in compliance with Proposition 218. Let the citizens of Paso VOTE on the financing bonds for their approval. Most of the citizens,that I had spoken to were not happy with the way our City Council handled the Nacimiento water pipeline project. The citizens of Paso didn’t give our City Council a “BLANK CHECK” for $176 million to spend without voter approval. I would suggest that it be a “special tax” or “an assessment” be levied on our property taxes. A percentage of those bonds can be deducted on Federal Income taxes. As it is now, a” fee” on a capital funding project is not deductible. Of course, there are homeowners would not go for this suggestion due to our poor economy. My analogy mentioned above would be, you would get something back on a “special tax” or an “assessment” verses nothing on a “fee”. What say you?\nwhatisup says:\t09/12/2010 at 9:02 am\nUnfortunately the law says we have to subsidize new development in California. I don’t like it, but it is the law. I know paying using the property taxes was bandied about. The argument against it was it would mean some would be paying for water they aren’t using and others could be big water users, but pay a small special assessment on their property taxes. I think the decision that was made to base it on usage was out of fairness. It seems to me if people are using water and not paying their share of the costs it is not fair. The Senior issue is very difficult. If someone is retired for twenty years is it realistic to think prices don’t go up during the 20 years of retirement. Think what prices were in 1990 compared to today. Should Seniors never have to pay for capital improvements? Paso Robles also had very low water rates. Rates that are no longer possible given the circumstances. Desalination will happen eventually. California is out of water. If you want to pay $1,000,000 a gallon there is no more allotable water of any consequence in California. The expense will be tremendous — still have to build a desalination plant, still have to build a pipeline. I don’t know if the plant has to be built along the ocean or if the salt water could be piped over to Paso Robles. If it has to be built along the ocean, Paso Robles doesn’t own land on the ocean and, in any case, the environmentalists will keep it in courts for years as they have done so for other proposed desalination plants in Southern California. Eventually necessity will force desalination past the environmentalists, but not yet.\npasojim says:\t09/13/2010 at 7:46 am\nWhatisup – On one of your previous post you made the comment you haven’t heard any of the legal suggestions for the water issue, But you obviously have. That is a good thing. So we can move the discussion ahead.\nOnce, again this was handled incorrectly by our city custodians from the beginning. And now here we are. The public is not supporting this very expensive, very limited benefit project. As you said, until a plan is developed that the public can support, things don’t look good.\nAll this discussion about the water issue has only reinforced my opinion the issue hasn’t been about water, only how the plan should be paid for. Or more specifically, to what extent do we allow our elected custodians and our un-elected GOD tzar decide which laws they will follow and which laws they will ignore. When the City GOD tzar tell citizens at a council meeting if we don’t agree with the City’s plan, then we should just sue him, and when the City Attorney explains to a citizen at a City Council meeting that she does have to respond to their questions because she does NOT work for them. When the project is voted down by the citizens and the council brings it right back up, it is clear that our elected representatives are not doing their job providing direction to their employees and listening to and representing the CITIZENS.\nThe subject of the original post was the need to elect different representation. I think with all the conversation made on this post, as well as the post on Cal Coast about the hiring of the new legal firm you were involved in, Supports my original opinion.", "answers": ["Fairness."], "length": 5701, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "8fbf0a6531d9250e6bcda0c7ba456441f6d4073bf08de826"}
{"input": "Can the denoiser be applied to circuits with non-Clifford noise?", "context": "Paper Info\n\nTitle: Compressed quantum error mitigation\nPublish Date: 10 May 2023\nAuthor List: Maurits Tepaske (from Physikalisches Institut, Universität Bonn), David Luitz (from Physikalisches Institut, Universität Bonn)\n\nFigure\n\nFIG.3.The out-of-time-ordered correlator C otoc i=L/2,j (t) as a function of the operator position j and time t, for the infinite temperature initial state, for a denoised second-order Trotter supercircuit with Trotter depth Mtrot = 32 and denoiser depth M = 2.We consider evolution times t = 0.5, 1, ..., 5, for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarizing noise with p = 0.01.\nFIG. 4. The complex eigenvalues λ of the noisy second-order Trotter supercircuit with Mtrot = 16 at time t = 1 (left), the corresponding optimized denoiser with M = 4 (center), and the denoised Trotter supercircuit (right).The Trotter circuit is for a L = 6 Heisenberg model with PBC, and all twoqubit channels are affected by depolarizing noise with p = 0.0046.The unit circle, on which unitary eigenvalues must lie, is shown in black, and the noiseless eigenvalues are shown as blue bars.It is evident that the denoiser recovers all the noiseless eigenvalues from the noisy circuit.\nFIG. 2. The complex eigenvalues λ of the noisy second-order Trotter supercircuit with Mtrot = 16 at time t = 1 (left), the corresponding optimized denoiser with M = 4 (center), and the denoised Trotter supercircuit (right).The Trotter circuit is for a L = 6 Heisenberg model with PBC, and all twoqubit channels are affected by depolarizing noise with p = 0.036.The unit circle, on which unitary eigenvalues must lie, is shown in black, and the noiseless eigenvalues are shown as blue bars.It is clear that the denoiser recovers with high accuracy the noiseless eigenvalues from the noisy circuit.\nFIG. 3. The half-chain channel entanglement entropy S at different two-qubit depolarizing noise strengths p, for a secondorder Trotter supercircuit with Mtrot = 16 and t = 2, for a M = 4 denoiser.The Trotter circuit is for a Heisenberg model with PBC of size L = 6.The different curves correspond to the different supercircuits, i.e. the noisy supercircuit, the denoiser, the corresponding denoised supercircuit, and the noiseless variant.\nFIG. 4. The out-of-time-ordered correlator C otoc i=L/2,j (t) as a function of the operator position j and stacked time t, for the infinite temperature initial state, for a denoised secondorder Trotter supercircuit with Trotter depth Mtrot = 32 and denoiser depth M = 2.It is optimized at t = 2 and stacked up to ten times.The calculations are for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarization with p = 0.01.The denoiser is affected by the same noise.\nFIG.6.The distribution of the ZZ angle α of M = 2 denoisers (top panels) and M = 8 denoisers (bottom panels), with the lightest color corresponding to the denoiser for the Trotter supercircuit with t = 0.5, and the darkest color with t = 5.As usual, we consider the Heisenberg model on a periodic chain, and second-order Trotter supercircuits with depths Mtrot = 8, 16, 32, 64, which together with the denoiser is affected by a two-qubit depolarizing noise with p = 0.01.The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively.\nFIG. 7. The sampling overhead γ of the optimized denoisers from Fig. 2 of the main text, with denoiser depths M = 1, 2, 4, 6, 8 and Trotter depths Mtrot = 8, 16, 32, 64 at times t = 0.5, 1, ..., 5, for the Heisenberg model on a chain with PBC affected by two-qubit depolarizing noise with p = 0.01.The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively.\nFIG.8.The domain wall magnetization Z dw after evolving a periodic density wall |dw |dw * with the denoised second-order Trotter supercircuits D C from Fig.2of the main text.These supercircuits have various Trotter depths Mtrot = 8, 16, 32, 64, denoiser depths M = 1, 2, 4, 6, 8, and evolution times t = 0.5, 1, ..., 5, for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarizing noise of strength p = 0.01.The denoiser is affected by the same noise.The non-denoised results are labelled with M = 0 and the noiseless results with p = 0.The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively.We see that the denoiser allows us to recover the noiseless behavior.\n\nabstract\n\nWe introduce a quantum error mitigation technique based on probabilistic error cancellation to eliminate errors which have accumulated during the application of a quantum circuit. Our approach is based on applying an optimal \"denoiser\" after the action of a noisy circuit and can be performed with an arbitrary number of extra gates.\nThe denoiser is given by an ensemble of circuits distributed with a quasiprobability distribution. For a simple noise model, we show that efficient, local denoisers can be found, and we demonstrate their effectiveness for the digital quantum simulation of the time evolution of simple spin chains. Introduction.\n-Quantum information processing has been theoretically shown to hold great promises, and quantum algorithms were developed which can in principle achieve an exponential speed-up over their classical counterparts, both for general purpose computing and quantum simulation . However, present day quantum computing prototypes still suffer from significant noise processes which hinder the execution of many potentially groundbreaking quantum algorithms .\nNontrivial quantum algorithms typically require large sequences of quantum gates, each of which introduces dissipation and hence an overall loss of coherence, eventually rendering the results useless. Until quantum error correction becomes practical, quantum error mitigation seems to be more feasible to increase the accuracy of expectation values.\nHere the goal is to induce the (partial) cancellation of errors that stem from noisy quantum gates by extending the circuit corresponding to the desired algorithm with an ensemble of gates , sampled from a quasiprobability distribution. The traditional way to accomplish this is with the gatewise method from , where noise is mitigated by inverting the noise channel of each gate separately, i.e. the cancellation of errors is performed for each gate on its own.\nHere the local noise channel is approximated in a way such that it can be easily inverted analytically, e.g. using Pauli twirling . Gates are then sampled from the inverted noise channel by interpreting it as a quasiprobability distribution. Because in this gate-wise approach every noisy gate has to be modified separately, the sign problem is exponentially large in the number of gates, limiting the practicality of the mitigation.\nThe success of the gate-wise approach resulted in a large body of work concerning these methods , including extensions for simultaneous mitigation of multiple gates by Pauli-twirling entire layers or variationally constructing a mitigating matrix product operator . In principle, errors during the execution of a circuit can propagate and accumulate.\nThese propagated errors * david.luitz@uni-bonn.de ≈ C\n\nC\n\nFIG. 1. An example of the quantum error mitigation procedure used in this work for the time evolution of the wave function of a spin chain. The ideal second-order Trotter supercircuit C of depth Mtrot = 1 (light blue) is approximated by applying a denoiser D of depth M = 1 (red) to the noisy Trotter supercircuit C (dark blue).\nBecause the denoiser is applied after fully executing the noisy Trotter supercircuit, it represents an approximate inverse of the global noise channel with a precision tunable by the depth of the denoiser. can potentially blow up and lead to large errors for the circuit as a whole . Here we introduce a mitigation technique that takes into account the propagation of errors, can be performed with a tunable number of extra gates, and works for non-Clifford local noise channels since the inversion of the accumulated global noise channel is implicit.\nWe first execute the targeted noisy circuit completely, letting the noise propagate and accumulate, and only afterwards we apply an extra random circuit sampled from a quasiprobability distribution. We call the corresponding ensemble of random circuits a denoiser, and we construct it such that upon averaging the accumulated errors cancel.\nEssentially, the denoiser inverts a global noise channel. Since we will construct it as a local brickwall circuit, following the classical preprocessing approach from , we call this compressed quantum error mitigation. Method. -Due to the inevitable coupling of a quantum processor to its environment, every qubit operation is affected by noise.\nTherefore, the simplest technique to minimize the impact of the resulting noise is to minimize the number of operations when performing a quantum algorithm. In we showed that many-body time evolution operators can be efficiently compressed into brick-wall circuits with high fidelity per gate. In this Letter, we consider the noise explicitly by treating quantum operations as (generally non-unitary) quantum channels, corresponding to completely positive and trace preserving (CPTP) maps .\nFor example, instead of a noiseless two-qubit gate G, which acts on a quantum state |ρ in superoperator form as G|ρ = G⊗G * |ρ , we get the noisy channel G = N G, where the noise channel N implements the two-qubit noise . These channels are used to construct a \"supercircuit\" C = N G i=1 Gi , consisting of N G channels, which is affected by multi-qubit accumulated noise.\nThis supercircuit encodes an ensemble of circuits . For simplicity, we assume that the noisy channels Gi in each half brickwall layer are lattice inversion and translation invariant, such that we can construct a denoiser with these properties, limiting the number of variational parameters. The purpose of quantum error mitigation is to modify the ensemble of circuits described by C in a way that we can use it to obtain the noiseless expectation values.\nIn superoperator language, we do this by following the supercircuit C with a denoiser supercircuit D, such that D C is as close to the noiseless supercircuit C = C ⊗ C * as possible. Here C is the target unitary circuit. Because the noise channel N is non-unitary, hence making the supercircuit C non-unitary, we need to use a non-unitary denoiser to retrieve the unitary C.\nWe illustrate the mitigation procedure in Fig. , where a denoiser with one layer is used to mitigate errors for a second-order Trotter supercircuit with one layer. This circuit architecture is commonly used to simulate the time evolution of a quantum many-body system, until some time t, with controllable precision , and we will use it to benchmark the denoiser.\nIn practice, we cannot directly implement a supercircuit, and so we have to utilize its interpretation as an ensemble of circuits. Essentially, after executing a shot of the noisy circuit we sample the denoiser and apply it. The goal is to construct the denoiser in a way that averaging over many of its samples cancels the accumulated errors and gives us a good approximation of the noiseless expectation values.\nIt should be noted that our approach requires more gate applications on the quantum processor than with the gate-wise scheme, since there each sample from the mitigation quasiprobability distribution can be absorbed into the original circuit, whereas our approach increases the circuit depth. We take this into account by imposing the same noise on the denoiser.\nFurthermore, within our scheme, the dimensionality of the quasiprobabilistic mitigating ensemble can be controlled, in contrast to the gate-wise approach where it is equal to the gate count. To facilitate the stochastic interpretation we parameterize each two-qubit denoiser channel G i as a sum of CPTP maps, such that we can sample the terms in this sum and execute the sampled gate on the quantum processor.\nConcretely, we use a trace preserv-ing sum of a unitary and a non-unitary channel. For the unitary part we take a two-qubit unitary channel U( φ i ) = U ( φ i ) ⊗ U * ( φ i ), with U ( φ i ) a two-qubit unitary gate parameterized by φ i . For this we take the two-qubit ZZ rotation exp(−iα(σ z ⊗ σ z )) with angle α, which can be obtained from native gates on current hardware , and dress it with four general one-qubit unitaries, only two of which are independent if we want a circuit that is space inversion symmetric around every bond.\nThe resulting gate has 7 real parameters φ i . For the non-unitary part, which is essential because D has to cancel the non-unitary accumulated noise to obtain the noiseless unitary circuit, we use a general onequbit measurement followed by conditional preparation channel M( , with V a general one-qubit unitary and each κ i a 3-dimensional vector, resulting in a real 9-dimensional ζ i .\nThis yields the two-qubit correlated measurement M( With these parts we construct the parameterization with coefficients η i ∈ R that satisfy η 0 + η 1 = 1 because G i is trace preserving. Note that here the tensor product symbol corresponds to combining two one-qubit channels to make a two-qubit channel, whereas in most of the paper it is used to link the column and row indices of a density matrix.\nWe construct the denoiser from the noisy channels Gi = N G i . With this parameterization one denoiser channel has 17 independent real parameters, such that a denoiser of depth M , i.e. consisting of M brickwall layers, has 34M real parameters (we use one unique channel per half brickwall layer). For reference, a general channel has 544M parameters.\nTo determine the mitigated expectation values we use the full expression where |ρ 0 is the initial state and |1 is the vectorized identity operator on the full Hilbert space. To evaluate this on a quantum processor, we use the stochastic interpretation of (1) to resample . In particular, from each channel (1) we get a unitary with probability p 0 = |η 0 |/γ and a measurement followed by conditional preparation with probability p 1 = |η 1 |/γ.\nHere γ = |η 0 | + |η 1 | is the sampling overhead, which characterizes the magnitude of the sign problem from negative η i . For quasiprobability distributions, i.e. with γ > 1, every denoiser sample has an extra sign sgn(η) = N G g=1 sgn(η g ), 2. The normalized distance between the denoised Trotter supercircuit D C and the noiseless Trotter supercircuit C (top panels), at evolution times t = 0.5, 1, ..., 5, and the twopoint z-spin correlator C zz i=L/2,j=L/2 (t) of a spin on the middle site at times 0 and t (bottom panels), for the infinite temperature initial state.\nWe consider denoisers with depths M = 1, 2, 4, 6, 8 and second-order Trotter circuits with depths Mtrot = 16, 32, 64. In the top panels we use a Heisenberg chain with L = 8, and in the bottom panels with L = 14, both with periodic boundary conditions. All gates are affected by two-qubit depolarizing noise with p = 0.01.\nThe non-denoised results are labelled with M = 0, and the noiseless values with p = 0. where sgn(η g ) is the sign of the sampled coefficient of the gth channel. γ = 1 means that all signs are positive. Observables Ô p=0 for the noiseless circuit are then approximated by resampling the observables from the denoiser ensemble\nwhere γ = N G g=1 γ g is the overall sampling overhead, with γ g the overhead of the gth gate. Clearly, a large γ implies a large variance of Ô p=0 for a given number of samples, with accurate estimation requiring the cancellation of large signed terms. The number of samples required to resolve this cancellation of signs is bounded by Hoeffding's inequality, which states that a sufficient number of samples to estimate Ô p=0 with error δ at probability 1 − ω is bounded by (2γ 2 /δ 2 ) ln(2/ω) .\nSince γ scales exponentially in γ g , it is clear that a denoiser with large M and γ 1 will require many samples. We observed that decompositions with γ > 1 are crucial for an accurate denoiser. Restricting to γ = 1 leads to large infidelity and no improvement upon increasing the number of terms in or the depth M of the denoiser.\nSimply put, probabilistic error cancellation of gate noise introduces a sign problem and it is crucial to find optimal parameterizations (1) which minimize γ to make the approach scalable. This issue arises in all high performance error mitigation schemes , because the inverse of a physical noise channel is unphysical and cannot be represented as a positive sum over CPTP maps.\nThis is clearly visible in the spectra of the denoiser, which lies outside the unit circle (cf. Fig. ). This makes the tunability of the number of gates in each denoiser sample a crucial ingredient, which allows control over the sign problem, because we can freely choose the η i in . For the parametrization (1) of denoiser channels, we try to find a set of parameters for error mitigation by minimizing the normalized Frobenius distance between the noiseless and denoised supercircuits\nwhich bounds the distance of output density matrices and becomes zero for perfect denoising. We carry out the minimization of on a classical processor, using gradient descent with the differential programming algorithm from . Instead of explicitly calculating the accumulated global noise channel and subsequently inverting it, we approximate the noiseless supercircuit C with the denoised supercircuit D C, effectively yielding a circuit representation D of the inverse noise channel.\nResults. -To benchmark the denoiser we apply it to the second-order Trotter circuits of the spin-1/2 Heisenberg chain with periodic boundary conditions (PBC) where is the Pauli algebra acting on the local Hilbert space of site i. A second-order Trotter circuit for evolution time t with depth M trot consists of M trot − 1 half brickwall layers with time step t/M trot and two layers with half time step .\nWe consider circuits that are affected by uniform depolarizing noise with probability p for simplicity, but our approach can be used for any non-Clifford noise. The two-qubit noise channel is which acts on neighboring qubits i and i + 1 and is applied to each Trotter and denoiser gate, and p = 0.01 unless stated otherwise.\nWe study circuits with depths M trot = 16, 32, 64 for evolution times t = 0.5, 1, ..., 5, and denoisers D with depths M = 1, 2, 4, 6, 8. In the top panels of Fig. we show (4) for a chain of size L = 8 as a function of time t. Here it can be seen that even for M trot = 32 a denoiser with M = 1 already improves by roughly an order of magnitude at all considered t.\nDepending on M trot and t, further increasing M lowers , with the biggest improvements occurring for high precision Trotter circuits with large depth M trot = 64 and short time t = 0.5, where the Trotter gates are closer to the identity than in the other cases. At the other extreme, for M trot = 16 the improvements are relatively small upon increasing M > 2. In all cases the denoiser works better at early times than at late times, again indicating that it is easier to denoise Trotter gates that are relatively close to the identity.\nTo probe the accuracy of the denoiser on quantities that do not enter the optimization, as a first test we consider the two-point correlator between spins at different times where we have chosen the infinite temperature initial state, and C(t) is the Trotter supercircuit for time t. In the bottom panels of Fig. we show C zz i=L/2,j=L/2 (t) for the supercircuits from the upper panels, now for a L = 14 chain.\nHere we see that at M trot = 16 we can retrieve the noiseless values already with M = 1, but that increasing M trot makes this more difficult. At M trot = 64 we see larger deviations, and improvement upon increasing M is less stable, but nonetheless we are able to mitigate errors to a large extent. As a further test, we compute the out-of-time-ordered correlator (OTOC) ]\nIn Fig. we show the results for i = L/2, for a Trotter circuit with depth M trot = 32 and a denoiser with depth M = 2. Here we see that a denoiser with M M trot is able to recover the light-cone of correlations, which are otherwise buried by the noise. In the Supplementary Material we consider how the denoiser performs at different noise levels p, and how the denoised supercircuits perform under stacking.\nThere we also calculate domain wall magnetization dynamics, and show the distribution of the optimized denoiser parameters and the sampling overhead associated to the denoiser as a whole. In Fig. we show the eigenvalues of the noisy supercircuits for a noisy second-order Trotter supercircuit with M trot = 16 at t = 1 (left), the corresponding optimized denoiser with M = 4 (center), and the denoised supercircuit (right).\nThe eigenvalues λ of a unitary supercircuit lie on the unit circle, and in the presence of dissipation they are pushed to the center. We see that the spectrum of the denoiser lies outside the unit circle, making it an unphysical channel which cures the effect of the noise on the circuit, such that the spectrum of the denoised circuit is pushed back to the unit circle.\nThe noiseless eigenvalues are shown as blue bars, making it clear that the denoiser is able to recover the noiseless eigenvalues from the noisy circuit. In the Supplementary Material we show the spectra for a p = 0.036 denoiser, where we observe a clustering of eigenvalues reminiscent of Refs. . There we also investigate the channel entropy of the various supercircuits .\nConclusion. -We have introduced a probabilistic error cancellation scheme, where a classically determined denoiser mitigates the accumulated noise of a (generally non-Clifford) local noise channel. The required number of mitigation gates, i.e. the dimensionality of the corresponding quasiprobability distribution, is tunable and the parameterization of the corresponding channels provides control over the sign problem that is inherent to probabilistic error cancellation.\nWe have shown that a denoiser with one layer can already significantly mitigate errors for second-order Trotter circuits with up to 64 layers. This effectiveness of low-depth compressed circuits for denoising, in contrast with the noiseless time evolution operator compression from , can be understood from the non-unitarity of the denoiser channels.\nIn particu-lar, measurements can have non-local effects, since the measurement of a single qubit can reduce some highly entangled state (e.g. a GHZ state) to a product state, whereas in unitary circuits the spreading of correlations forms a light-cone. To optimize a denoiser with convenience at L > 8, the optimization can be formulated in terms of matrix product operators or channels , which is convenient because the circuit calculations leading to the normalized distance and its gradient are easily formulated in terms of tensor contractions and singular value decompositions .\nThis provides one route to a practical denoiser, which is relevant because the targeted noiseless circuit and the accompanying noisy variant in (4) need to be simulated classically, confining the optimization procedure to limited system sizes with an exact treatment or limited entanglement with tensor networks.\nNonetheless, we can use e.g. matrix product operators to calculate (4) for some relatively small t, such that the noiseless and denoised supercircuits in (4) have relatively small entanglement, and then stack the final denoised supercircuit on a quantum processor to generate classically intractable states.\nAnalogously, we can optimize the channels exactly at some classically tractable size and then execute them on a quantum processor with larger size. Both approaches are limited by the light-cone of many-body correlations, as visualized in Fig. , because finite-size effects appear when the light-cone width becomes comparable with system size.\n1. The normalized distance (left) and z spin correlator C zz i=L/2,j=L/2 (right), for a second-order Trotter supercircuit of depth Mtrot = 16 for time t = 1, affected by various twoqubit depolarizing errors p. We compare the values obtained with and without a denoiser, i.e. M > 0 and M = 0, to the noiseless values (p = 0).\nThe denoiser is affected by the same noise as the Trotter circuit. We consider denoisers with depths M = 1, 2, 4, 6, 8, and we use a L = 8 Heisenberg chain with PBC for the normalized distance, while for the correlator we use L = 14. * david.luitz@uni-bonn.de to observe that even for larger noise strength p, the local observable C zz improves significantly even with denoisers of depth M = 1.\nFor large noise strengths, we generally see that the optimization of the denoiser becomes difficult, leading to nonmonotonic behavior as a function of p, presumably because we do not find the global optimum of the denoiser. It is interesting to analyze the spectra of the supercircuits considered in this work.\nAs mentioned in the main text, the spectrum of the ideal, unitary supercircuit C lies on the unit circle. The comparison to this case is therefore instructive. In the main text, we showed an example of the spectra in Fig. for moderate noise strength. Here, we show additional data for stronger noise p = 0.036 in Fig. for a denoiser with M = 4 layers, optimized to mitigate errors for a second-order Trotter supercircuit with M trot = 16 layers at time t = 1.\nThe eigenvalues λ of the noisy supercircuit C are clustered close to zero, far away from the unit circle (except for λ = 1), showing that the circuit is strongly affected by the noise. To mitigate the impact of the noise, the denoiser consequently has to renormalize the spectrum strongly. If it accurately represents the inverse of the global noise channel, its spectrum has to lie far outside the unit circle, which is the case.\nInterestingly, we observe a clustering of eigenvalues which is reminiscent to the spectra found in . By comparison to these works, we suspect that this is due to the local nature of the denoiser, and warrants further investigation. The right panel of Fig. shows the result of the denoiser, pushing the eigenvalues back to the unit circle, nearly with the exact same distribution along the circle as the noiseless eigenvalues (blue bars).\nDue to the strong noise, this is not achieved perfectly, and it is clear that this cannot work in principle if the global noise channel has a zero eigenvalue. The complexity of an operator can be quantified by its operator entanglement entropy . Here we calculate the half-chain channel entanglement entropy S of the noiseless C, noisy C, denoiser D, and denoised D C supercircuits.\nWe define S as the entanglement entropy of the state that is related to a supercircuit C via the Choi-Jamio lkowski isomorphism, i.e. ψ C = χ C /N , where the process matrix χ ab,cd C = C ac,bd is simply a reshaped supercircuit and N ensures normalization. Then we have S = −Tr [ψ C ln ψ C ]. This entropy measure is a particular instance of the \"exchange entropy\", which characterizes the information exchange between a quantum system and its environment .\nIn Fig. we plot the various S for a second-order Trotter circuit with M trot = 16 at t = 2, for a denoiser with M = 4, both affected by two-qubit depolarizing noise with p ∈ [10 −3 , 10 −1 ]. The Trotter circuit is for a Heisenberg model with L = 6 and PBC. We see that at large p, the noise destroys entanglement in the noisy supercircuit, and that the denoiser S increases to correct for this, such that the denoised supercircuit recovers the noiseless S.\nHere we investigate how denoised supercircuits perform upon repeated application. We optimize the denoiser for a Trotter supercircuit for a fixed evolution time t. Then, to reach later times, we stack the denoised supercircuit n times to approximate the evolution up to time nt: In Fig. we stack a denoised t = 1 supercircuit up to n = 20 times and calculate the correlation function, defined in the main text, for the middle site.\nWe consider Trotter depths M trot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8, for a L = 14 Heisenberg chain with p = 0.01 depolarizing two-qubit noise. The noisy results correspond to M = 0 and the noiseless results to p = 0. In Fig. we calculate the OTOC, defined in the main text, with stacked time evolution for a denoised t = 2 supercircuit with M trot = 32 and M = 2, stacked up to ten times.\nWe see that the stacked supercircuit performs very well, and the additional precision obtained by using deep denoisers (M = 8) pays off for long evolution times, where we see convergence to the exact result (black dashed lines in Fig. ) as a function of M . FIG. . The two-point z-spin correlator C zz i=L/2,j=L/2 (t) of a spin on the middle site at times 0 and t, for the infinite temperature initial state, for denoised second-order Trotter supercircuits that are optimized at evolution time t = 1 and then stacked up to twenty times.\nWe use Trotter depths Mtrot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8. The calculations were performed for a periodic Heisenberg model with L = 14 and PBC, affected by two-qubit depolarizing noise with strength p = 0.01, which also affects the denoiser. The non-denoised results are labelled with M = 0, and the noiseless results with p = 0.\nThe panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively. The costliest and most noise-susceptible operation is the two-qubit ZZ rotation with angle α, which is the foundation of the unitary piece in our channel parameterization, defined in the main text.\nFor completeness, we here present the α angles of the optimized denoisers. The results are shown in Fig. , which contains histograms for the channel count N G versus α. The histograms are stacked, with the lightest color corresponding to the angles of the denoiser at t = 0.5 and the darkest at t = 5. The top four panels are for a denoiser with M = 2 and the bottom four with M = 8.\nWe consider M trot = 8, 16, 32, 64. We see that in both cases the distribution widens upon increasing M trot , indicating that the unitary channels start deviating more from the identity. Moreover, while the M = 2 denoisers in all cases except M trot = 64 have ZZ contributions close to the identity, this is clearly not the case for M = 8.\nFor simplicity, we did not focus on obtaining denoisers with the smallest sampling overhead γ, which is required to minimize the sign problem and hence ease the sampling of mitigated quantities. Instead, we let the optimization freely choose the η i in the denoiser parameterization, as defined in the main text.\nIn Fig. we show the sampling overhead of the denoisers from Fig. of the main text. We see that for M = 1 and M = 2 the sampling overhead is relatively small and uniform across the different t, whereas for M > 2 the optimization sometimes yields a denoiser with large γ and other times with small γ. This could be related to the difference in α distributions from Fig. .\nThe large fluctuations of γ appears to stem from the difficulty in finding optimal deep denoisers, and our optimization procedure likely only finds a local minimum in these cases. Here C(t) is the Trotter supercircuit for time t. In Fig. we show Z dw for the circuits from Fig.", "answers": ["Yes, the denoiser works for non-Clifford local noise channels."], "length": 5389, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "13b04172d2261a0c9545a0e3670c1c43105ccab41ce1a726"}
{"input": "How can you level up in the early levels?", "context": "What is this game all about? (short version) Do you like the board game RISK®? Then chances are you’ll like QONQR. Your job is to join a faction and help your faction (team) take over the world. QONQR is an artificial intelligence that appeared on the Internet. We don’t know where it came from, or its purpose, but we know it is powerful. The Legion want to destroy QONQR because they believe it will enslave and exterminate humanity. The Swarm believe QONQR will advance the human race, and we should protect it. The Faceless don’t care, and want to steal the technology for their own uses. Pick a side, recruit your friends, and help your faction capture the towns and cities in which you live, work, and play. What is this game all about? (long version) Right now an invisible war is raging all around you. At stake: the Past, Present, and Future. A rogue Artificial Intelligence has been detected infiltrating the world’s networked infrastructure. Initial hacking of the source code has revealed incredible new technology. It is not certain whether this AI seeks to enhance or destroy humanity. It is only certain that it is here, and that is has a name: QONQR.Those who first detected the presence of QONQR on the global networks have argued fiercely over its intentions. They have split into viciously rival Factions, each co-opting the known QONQR technologies for their own ends. Even now the Factions battle over the entire globe seeking to gather resources and prepare for the coming power struggle. Whether you accept it or not, the war is here. Your survival, prosperity, and even glory depend on the choices you make and the skill you demonstrate from this point forward. You will be asked to join one of three Factions. The choice should not be made lightly. Your Faction Alignment will define your place in the war over QONQR. THE LEGION unite under the shared goals of destroying QONQR and saving humanity by crushing the nascent AI before it can mature. They are led by AGENT SUNDAY, a former commander of the NSA's Turing Task Force which has been valiantly stamping out dangerous AIs for years. THE SWARM are convinced that QONQR promises an era of unprecedented technological advancement and human prosperity. Nanobot weaponry expert KIMYO NAGUMO leads this faction in the battle to defend QONQR and assemble its future tech, accelerating humanity's path into the future. THE FACELESS are a loosely organized faction of militant hackers who want QONQR's technology for their own ends, but want to prevent the unavoidable nightmare of human slavery they believe it portends. When they choose to communicate, they do so through an anonymous vigilante who goes by the name PROMETHEUS. . What do I do first? Create a base, then launch. Launch nanobots until your fingers hurt. How do I create a base? On iOS there is a Base icon in the menu bar. For Windows Phone, you will find a bases button on the home screen. These take you to the Base Screen where you can see how many bases you have available to you. Once you create a base, be sure harvest often by returning to the list of your bases. You can see how full each base is by checking the fill percentage icon. Bases stop collecting once they are full. What is the point of creating bases and harvesting resources? You need bases to earn money. Bases collect rare elements over time which you can then harvest for your faction in exchange for qredits. Qredits can be used to purchase ordinance (like nanomissiles) and upgrades which will help you capture and hold battle zones more easily. What do you mean, “launch nanobots?” Nanobots are the invisible soldiers generated by your device (which has been transformed into “Scope” by advanced QONQR technology). Nanobots fight for control of the battle zones around you.. From the home screen click “Current Zone”. Once you have selected your zone, you will be able to deploy nanobots there.. Initially, you are just a level 1 recruit. You are only going to get a small attack formation with a limited range.. Other solders have to practice with rifles before they get tanks; you are no different. Once you prove your mettle, you’ll get access to bigger weapons. Soon you’ll be lobbing missiles hundreds of miles. How do I capture a zone? If you play for the Legion and are launching nanobots into a zone controlled by the Swarm, you will capture the zone for the Legion as soon as you have destroyed enough of the enemy that your nonbots outnumber theirs. If you are the person who causes the zone to change control to the Legion, you will be listed as the Capturer of the Zone. The Person with the most nanobots in the zone is the zone leader. What is my current zone? How do you know? Your current zone is determined by your proximity to the nearest zone center. So, while you might be inside the governmental boundaries of a city, your scope (phone) could tell you your current zone is a different city, if that city’s center is closer. So I just keep deploying? Yes, in the early levels of the game, just keep deploying and harvesting your bases. You will earn XP (experience points) proving your loyalty to your Faction.\tYou will level up quickly and soon have access to many more options. So all I can do is just attack? You only get assault bots to start. They are the most basic type of nanobot formation. As you level up you will get many more options, including bots that are good at defense, energy attacks, long-range deployments, formations that will buff the shields for all your faction-mates in the zone, and many more. My faction already controls this zone should I still attack? Yes, assault bots won’t attack your friends. You will increase the bot counts for that zone, which will deter opponents. Attack bots can defend your zone; they just aren’t very good at it. As you level up you will unlock defensive formations that are better for deploying if your faction already holds the zone. I’ll never knock out my enemy at this rate! In the first few levels your impact might feel minimal, but every deployment helps you gain experience. It won’t take long to level up if you keep at it. If you are unlucky enough to be in a zone where someone has already built up huge defenses, you may be in for a long fight. But remember, your scope moves with you. Go explore the world and find softer targets. Once you level up, it won’t seem like a toy gun against a battleship. You’ll get your big weapons once you prove yourself. We have already seen operatives brag about taking down 1,000,000 nanobots in just a couple days. Nothing is impossible. How do I attack a different zone? As you level up you will unlock more and stronger formations. Those new formations will have range. While at the early levels you can only attack nearby zones, as you level up your attacks will go 10-20 miles (roughly 15-30 Km) and you will eventually gain access to nanomissiles that can go hundreds of miles. What should I buy in the depot first? The smallest thing to buy is a refresh. We give you some of these as you level up so you can try them out. Refreshes will refill (or partially fill depending on the size of your tank) your bot bar or your energy bar. But after that, it depends on your goals. There is much to choose from. Do you want to be able to deploy more nanobots on every launch? Do you want to boost your offensive or defensive bots? Do you want to be able to launch missiles into towns far away? All of these things are possible. Look through the depot and see what you like. Most of the time you will need to buy an upgrade before you can buy the ordinance. For example, missiles are fairly inexpensive, but you need to buy the MX Rack Missile Launcher before you can launch them. Buy the upgrade first. What is the difference between qredits and cubes in the depot? A qredit (aka: credit, which looks like the child of a Q and € ) is the type of currency you earn in the game by harvesting your bases. Cubes (aka: power cubes) are purchased with real money in the Bank section of the Depot. We want everyone to be able to do everything in the game for free, by earning qredits, but for those who want to move a bit faster, you can purchase cubes to speed things along. Purchasing cubes is how QONQR makes money. We very much appreciate your support. Every purchase you make helps us to keep making improvements in the game. Future enhancements will enable you to earn cubes in game. Why can’t I create another base? Additional bases become available as you level up. At the start, you will get a new base every 5 levels. If you don’t have any more bases available to build, you will need to level up. If you have a base available, but aren’t allowed to use it, it is because you already have a base in that zone. You can only have one base in a zone. Get yourself to another zone, then create your base there. My bases are collecting credits at different rates. Your bases collect resources faster if your faction controls the zone. Do your best to either put your bases in zones you can control by yourself, or find zones with strong players in the same faction and put your bases there to maximize your credit collection. The game says I’m in a town that doesn’t exist. QONQR tracks almost 3 million battle zones of varying strategic value in 250 countries. Sometimes those zones include locations that haven’t existed for over 100 years. That’s pretty cool if you ask us. If you find a zone that looks like a duplicate or is just plain wrong, however, let us know on the forums under “Zone Corrections”. How do I move my current position on the map? You might need to take a car, bus, plane or train depending on which zone you are trying to get to, but if you want to move on the map, you need to move in the real world. QONQR is a location-based game, which means you play where you are. However we don't want to make you move to play, we want you to plan when you move. QONQR goes with you as you move through the daily activities in your life. Where do I find the Strategy Guide? Here: http://community.qonqr.com/index.php?/topic/1191-the-official-qonqr-strategy-guide/ How do I win? That is for you to decide. There is still much to discover. We don’t even know if QONQR is good or evil. Why is it here? What is its purpose? Help your faction further its goals and unlock all the secrets of QONQR!\nCongratulations to the Swarm on their overwhelming victory in Atlantis in May 2015 -- taking and retaining all Atlantis zones from beginning to end is hard to argue with -- most convincing -- well done Swarm!\nRumor has it that the Duggers have 20 scopes. Some families really do have a wife and ten kids... Ha Anyways.... Multi scoping is generally not encouraged, usually if you mention that you do it on the forums or in groupme your not going to be a very liked person, even within your own faction. What tends to happen is if you start multiscoping then your enemies start doing it, then you get into a war where ever person has 6 phones and nobody is happy. I've seen or heard about situations like this in a lot of different locations.\nGreat job, faceless! It was actually an exciting Atlantis and I prefer it that way. Way to bring your A game.\nYou know what we need in this game? Nano-Nukes!!\nAttention (insert faction here), We, the (insert faction here) are tired of the way you constantly (circle one).. A. Cube rage us B. Bully us with numbers C. Talk mean to us It hurts our feelings because (circle one).. A. We don't cube B. We don't have as many allies C. We have no sense of humor and/or no backbone Please refrain from participating in the above selected actions for above selected reasons so as the game is enjoyable for (insert faction here). Regards, (insert name and faction here) There we go...this should streamline the entire complaint process of the forums. Copy and reuse as needed. You're welcome.\nSilver, you mentioned in a blog that there are levels beyond 150, is it safe to say there is no level limit anymore or is that for us to discover? Another question, I'm not sure if I'm the only one noticing this but it seems like bot decay doesn't work against Zone Assault bots. I've hit two players I know have been inactive for well over 6 months but my attack had the same effect it did yesterday before the update. However I did notice against Deflection bots I am getting 2x the kill power. Also in response to decontaminatoR. Paid gaming isn't just something for adults. When I was 13 the best options for handheld gaming was GameBoy or GameGear and the games at the time cost anywhere between $29-$39 dollars. I had a paper route to pay for my games. So, no offense, but you can afford $0.99 for a game. You don't even need a paper route, just check under your couch cushions and I'm sure you'll find a few quarters.\nI want to start by thanking Faceless. This round of Atlantis, Legion and Faceless were doubled in sized and probably spending by Swarm. I contacted some great players from the other side and put together a nonaggression pact, this pact was one of the most impressive agreements I've seen in the more than two years of playing. Hundreds of people worldwide stuck to this agreement and put past feelings behind us. It was awesome to see both sides stick so closely to each other in fighting against Swarm. I want to really thank everyone who showed honor by standing behind me and the faceless command when we suggested that, the people who really gave it a chance and then most importantly, to all the players who honored it. Faceless, thank you very much! We stood no chance of winning without you! The battle came down to literally one launch in one of three zones. I'd say that with that being said everyone fought incredibly hard, so I want to give Swarm the respect they deserve. You guys really show out and play to win. Good game, 2 against 1 is not easy, no matter how large your crew is. And Legion, we had many late, late nights, many very long days. You guys killed it this month! We didn't take home a trophy, but I would say we all have something to be proud of! The other leaders who helped me coordinate everything were awesome! So many great people kept everything moving forward 24 hours a day for the whole week. Thank you to everyone who gave it your all for the whole week even when we saw that Swarm had 4x our bots at the end of just one day. Many people would have given up, but we held in and **** near won! I've won Atlantis battles with Faceless and with Legion but I will say, nothing was as fun or incredible as this round! You guys are fantastic and I can't wait to do it all again next month! Hopefully we have more Legion and Faceless show up for the next round, but I know that even if we don't, we will figure out a way, just as warriors do. PEW! PEW! PEW!\nI must start with an announcement: Camels are not the only animal in the middle east. You have overused it already. It's saddens me that anyone would still find it funny. Get a bit of originality. For anyone not wanting to read all this drivel, skip to the end (hint: Bold stuff). At what point have I bullied anyone? When was the last aggressive or threatening message you or any of your members received from me? Never. Legion and Swarm (in the UK) have teamed up because Faceless are dominating in London. That makes sense. It's a three sided fight and if one force becomes too powerful, the other two can join forces to try and take them down. But: We are dominating in the London area while your alliance is attacking players outside of that area. Then you expect me to sit back and do nothing. You are specifically using me as an excuse for why you have teamed up but then you're attacking players outside of my reach, sometimes with Europe involved. If that is not reason enough to attack you then I'm not sure what is. You lot cube, multiscope and have multi faction accounts and still complain. I don't complain about anything you do. Play the way you want to, ill do the same. I am never rude to anyone, always polite no matter what the message is, never brag about what I do, can do or have done, never threaten anyone, try and keep in touch with the few swarm or legion who are civilised to me, listen to any message form any side and if I can help in any way, I try my best to. Formed agreements with enemy (that I didn't need to) that ended up slapping me in the face. I have even in the past taken the time to find out who some of the younger players are so I know to avoid them. When I'm in a different country I try and find who the bullies are. Those players who threaten and brag and laugh at others. Those are my targets (if they exist in those areas). I don't see how anyone can think I'm a bully. Is it your money? Its none of your business how I spend it. Either way your numbers are way off. 700 dollars a day? Did you just decide to blindly strike the number pad to come up with that figure? Best part of one of your posts is saying that the devs should worry about my welfare. What concerns you about me spending my money? Maybe I'll self harm due to overspending? I wont have enough to buy food because I bought too many cubes? I don't understand what they are supposed to worry about. I have issues? At what point did you deduce this oh mighty psychologist? Wait a minute are you my bank account manager? What do you know about how much I can or cant afford? Random guy spends money on something he enjoys. The end. Told you last time to give up on all the drama but I guess thank you for the concern. You play to make me spend more? And? What do you think that accomplishes? In fact how do you even make me spend? You don't even attack me. You sulk when I'm in the UK and when I fly out and you find out, you call in Europe to help you take an zone or two. As they say, whatever floats your boat. As for Atlantis: Please try again. I quit Atlantis when we were winning. I have on occasion involved myself after I was asked to help out but on the whole I give it a miss. The last time (months ago) was the last 10 minutes of Atlantis and Legion fought hard. We lost. I was not the only Faceless player to quit Atlantis. Quite a few of us thought it lasted too long and had too many zones to fight over. I hear the duration has been reduced. You can't honestly believe I changed my sleeping pattern for the game. I was in California for a month. I was jet lagged. My sleeping pattern was a bit off when I got back. You're not even accurate about when I deploy. Pay attention. You should know this by now: I play as and when I want, sometimes every 20 minutes and sometimes I do long stretches and sometimes I'm busy and don't deploy for hours. You wont always win, and nor will I. Try and get satisfaction form at least trying to win or putting up a good fight. About the limit on cubing. Please. I suggested that last year. Twice. Unfortunately multiscoping is allowed and so prevalent that it kind of ruins the idea. These are the facts: YOU and YOUR side threatened Faceless members who support London. One of the members threatened is actually London based. What do you expect him to do while his city is under attacked? A few of your members don't know when to keep quiet. They sent messages to us threatening specific players and telling them their zones will be dropped just because they have helped London. Basically if they help London then they get their zones wiped. And you have the cheek to call me a bully? We stood up to your members specifically because they were trying to bully. That is the reason we went strong months ago and took those big zones. How is this me bullying you? This is me answering your threats. I didn't attack those zones \"just because i can\", it's just because I should. Because of your threats. You can blame your members for the loss of those 2 or 3 big zones. These are the basics: You attack one of our zones. We look at the list of attackers and pick one of the players who deployed the most, find a zone of his or hers and attack it. We don't need to justify our attacks with \"because there are Faceless players within 30 miles\". What, every time we attack a zone we need to send some letter explaining why? You attack us or we attack you, for any reason. That's the game. When some of you were cheating and you could not win you complained, when you cube and lose you complain, when you invite all of Europe to attack and win you still complain. I just think you like to make a fuss. Last words (for now): You think I attack zones as a means of getting attention? I get enough of it form your threads. The only attention seeker here is you with your victim attitude and pity us posts.\nI've had some very angry emails today from a couple users who are upset their rival achieved the ability to switch factions freely having played for one of the factions for only 1 hour. I've been accused of doing favors, changing the rules, and various other backhanded deals. It appears it comes down reading the rules. You do not have to play for every faction for 60 days in order to earn free switching status. Here is a common scenario many players have used to achieve free switching status and avoiding playing for one faction they despise. 1. Start with Swarm 2. Switch to Legion (earn Spy) play for 60 days 3. Switch back to Swarm (earn Double Agent) play for 60 days 4. Switch to Faceless (earn Mercenary) immediately switch back to Swarm or Legion Below is the complete text on the switch nanobots screen. It is the same text that has been there from Day 1 with the exception of the level 100 rules that went into place earlier this year, where you could switch as much as you want before level 100 , but those switches don't count towards the medals. This text has been part of this description since Jan 15, 2013. \"Players that earn all three spy awards, may once again switch factions at any time as they could during the training levels 1 through 99.\" Prior to Jan 15, the text said this. \"Players that earn all three awards may be given the opportunity to switch factions more quickly in future updates (contact support for more information)\" I pulled that right out of source control, which includes the entire change history. Here is the complete text from this page. http://portal.qonqr....r/SwitchFaction WARNING: Defection has consequences! Self-destruct will be initiated on all your nanobots. Without the self-destruct, you would be required to battle against your former self to regain control of your zones. You will lose the capture and leadership of any zones you currently hold. Lifetime captures will be unaffected. If you are still completing the training levels and have not reached Level 100, you may switch as often as you like to find the faction that suits you best. Once you have reached Level 100 switching factions has rewards, but also has additional consequences beyond the self-destruct of all your nanobots. Defection will usually result in a demotion in rank. This is accomplished through awards with negative rank points. Those awards are: Spy - First switch to an opposing faction (-20 points) Double Agent - Return to a faction from which you had previously defected (-20 points) Mercenary - Become a member of all three factions (-20 points) Other Faction Change Details: You may not switch factions again until at least 60 days have passed since your last faction switch. Defection point penalties are applied only once per award Players that earn all three spy awards, may once again switch factions at any time as they could during the training levels 1 through 99. The decision to switch factions is one that must be made with strong determination. Nanobots cannot be reanimated once destroyed. You will retain your earned experience, level, formations, qredits, cubes, and upgrades. However, as far as your zones go, you will be starting over.\n...has got to be one of the funniest moments I've seen in qonqr yet lol.\nThe **** change operation was a success!\nAs a relatively new player for faceless in a region dominated by swarm i can understand the OP. However, judging from the numbers i see here on a regular basis i think you are asking a bit much. My idea would be the opposite approach. Why not add a weapon with extremely short range, let's say like 5km that acts like a bomb and make it much stronk? That would add some serious home advantage. Or alternativley make attack formations lose power over range (exclude nanos and plasma). Maybe something like that would allow newcomers to at least get a foothold in their homezones. It's just an idea, maybe i overlooked something?\nThis is by design. Some day it is possible (I said someday) we could offer skins for your scope. So we will need a uniform color scheme. You can tell the formation families based on the shape of the box. Trapezoid is attack, diamond is defense, and octagon is support. It will take some time to get comfortable with the change.\nGeophysical based game. Anyways, probably not a bad idea but, considering the issues with the three platforms and the development of blue for those platforms, I doubt the resources are available for development on a new platform. Seen the blog? Its Qonqr meets wheel of fortune!\nI am happy to announce that today I both completed the training and captured my first zone.\n^THIS so now that qonqr has been thoroughly funded, can we have blue now? Or is that not happening still lol.\nAtleast I am not legion and there for we can have this intelligent discussion rather than just compete over who has the best words XD ohhhhh someone bring the bill to legion cuz someone just served them extra double order of stir fried SNAPPPPPP If the devs wont make zone dueling for us I hope out there somewhere are those who would empty a zone and challenge one on one to a local battle. I'd like to see the transcript of deployments made / moves made as well that would be neat I think such events would be cool. I suppose if people give up on atlantis as it works now they can schedule their own tournements in empty atlantis zones.. have a team clear the zone.. put 1 vs 1 or teams vs teams.. like fisticuffs challenges.. find out what these warriors are really made of!\n@Qonqrd everyone you know must face palm every time you make a post. Its embarrassing. Mega cubers or whatever you want to call them are not great for the opposing team surrounding them but are great for the game itself (money) and for the team they are part of. Multiscopers are not not great for the opposing team surrounding them and bring nothing to the game but are great for the team they are part of. Both have a negative impact on enemy teams/players but only one benefits the game itself. Both can make people want to quit out of frustration. And that's not great for the game. @OP unlimited refresh is over powered. Its frustrating to fight against a ridiculous amount of refreshes. Unfortunately i dont see anything changing unless this game gets a lot more people playing. More people might mean more money for the company from various sources. More money from various players might mean they can limit the players who spend a ton and still generate a healthy income. The main issue i see with limiting refreshes is someone multiscoping and spending money. He now has two, three, four accounts to refresh with and gets the advantage. Its tricky.\ney dun new ho to yet it uff.\nYet you complain almost everyday here, on your website, Twitter, and YouTube channel that the game needs to change because cubing has such an impact.\nIt was fun. Swarm had me scared at first, but it turned into kind of a bullying match between us and legion. Last hour became p obvious which way it was gonna go. Legion rly stepped up their game in the end there, respect.\nWe are investigating this. Here is what we know: Several of the accounts used the same password. Most of the accounts belonged to people who knew each other personally. The accounts were all switched from the same IP Addresses. The person who logged in, got into each account on the first attempt, so they knew the password for each account. What you should know: QONQR never stores passwords, not even in the logs. Passwords are hashed (one way encrypted) and can never be decrypted When you authenticate to our servers, we hash the password you gave us and compare it to the encrypted password in the database to see if they match. Access to our database in the could is restricted tightly and we are confident no one breached the system. What you should do: Don't use the same password as other people you play with. Don't share your password with anyone.\nI heard all the French players fled to the UK after one German player accidentally shot a single missile into France.\nMost factions now use GroupMe or Line as their means of communication, the forums are too slow as a means of communication and insecure for specific faction conversations. Think of the forums are more of a gaming information resource rather than a means of communication. Contact the top players of your faction in the leader boards of your state and they will likely point you in the right direction to chatting with your local faction. The developers are also building some sort of new chat system into the game, we don't know much about it but apparently beta testing for the chat will be happening very soon (next couple of weeks) according to their timelines.\nA way to honor the dead? Nah, how bout a way to dishonor the dead.\nCould just build it up and retain their capture. Remember Bizzy, staq to the heavens.\nI just read this entire thread. I am now tuckered.\nDoes anyone know if Bot Booster has an effect on Seekers and how much dmg they do to attacking players? Also on the topic of seekers, does the amount of skeers in a battlefield have any effect on how much damage they do?\nInteractive map of real-time zone captures.\nYou know that is something I didn't factor in there. Time. The player who can consistently and constantly launch wins against the guy who casually picks up the phone on occasion or has to work away from a cell phone for 8 hours. Good point. And yes I can't argue skill doesn't factor in, it just seems like less of a factor than other games is all.\nI cant see why a closed forum, open only to registered Qonqr accounts, cant be used. **** spammers!\nGotta love synclock! I suspect linjin has a problem. Maybe the dvs should look into it.\nNo, Naamah...I clearly understood what you were trying to convey. I'll even go as far as to agree that what your facing now is, while fully allowed and deemed completely acceptable by the developers, unbalanced and wrong. However..the imbalance isn't in the game and isn't something that, from a business standpoint, is likely to be regulated. The game is fair..the advantages are provided to all players. It's the players themselves that throw the balance into chaos because, as you've said, not everyone can afford spending several thousand bucks a year in only one game. Truthfully, in my opinion, the moment you admitted to buying cubes yourself your complaint became silly..because I'm sure there's a player out there, who has bought NO cubes, who can make this same complaint about you that you are making about others here. I know these things because I've read them so...many...times in this forum. There was likely a time, ages ago when I was a young Massune, that I even posted a few myself. That was the purpose of my post..I was poking fun at addition of yet another cuber/bully/trash talk complaint on the forum. It wasn't directed at your personal plight so much as the idea that someone, yet again, finds it necessary to lobby for a spending cap on the only real way for this game to make money. As to your specific problem..you, like all those that have raised this topic before you, have few options to rectify the issue. Here are a few that seem to have worked for others..fight harder, recruit better, spend less time complaining and more time organizing, budget for more cubes or quit. I'd rather not see you opt for the latter..but to each their own.\nI agree, we should find a way to honor the dead, but I don't think keeping their towers infinitely is necessarily the solution. The game must go on. I'm pretty sure the point of bot decay was to clear the game of inactive player bots so that new players can have a chance to rise up, not to dishonor the bots of dead players.\nThe following are frequently asked questions about the new server update (so far) It still says Training Complete on my iPhone. -\tDownload the update from iTunes It still says Training Complete on my Android. -\tSorry, Android will not be updated again until the QONQR Blue beta is released My XP per launch keeps going down -\tThis is XP throttling and is intended to limit the ability for people to leve1 from 1 to 100 in a single day through heavy cubing. The XP throttle was introduced with the original version of QONQR in 2012,and the throttle formula is the same for levels above 100. The throttle resets at midnight UTC every day. How do I buy the Bot Regeneration Accelerator? -\tCurrently the Bot Regen Accelerator can only be purchased through http://portal.qonqr.com. Go to the Depot and review your scope upgrades. The new QONQR Blue clients will allow for this purchase to be made in the app using your mobile billing. I don’t have a PayPal account -\tFor users interested in purchasing the Regen upgrade, but who do not have a PayPal account, PayPal does give you the option to checkout using your billing information without creating an account. PayPal is not allowed in my country, or I don’t have a credit or debit card -\tPlease contact support@QONQR.com for alternate options Is Bot Regen Accelerator counted as part of the 100% scope upgrades? -\tYes, but there is a bug that does not increase scope upgrade percent when you purchase this upgrade, that will be fixed in the coming days. For all other questions, please read the 7 blog posts prior to 7/29/2015 for information on what was included in the update today.\nBye Fack, its been a pleasure being allied and against you.\nThe two big issues that are both killing the game slowly and keeping it from growing exponentially are cube injustice and new player ramp. The game obviously also needs to provide a consistent and growing revenue stream as well. I think Silver needs to rethink how revenue is generated if he is going to address cube injustice and new player ramp. For revenue generation I would suggest a model that doesn't give a significant combat advantages. Download and play for free from level 0-99 Pay small monthly fee to get full functionality or play for free at 50% of offensive/defensive funtionality Still buy cubes, but cubes are used for following: - Credit Boost: harvest more credits for a period of time - Range Extension: ability to use standard attack/defense formations at extended ranges - Base Share: get 100% credit attainment even in bases owned by another faction - Purchase additional ordinance - Zone Name Change - ability to customize zone names. \"Breggland\" - Faction Change with Bots - pay for the ability to keep up to 50% of your bots with faction change - Experience Boost: %increase in experienced gained while leveling - Other: anything that helps grow a player or provides enjoyment, but doesn't tip the battle capability of a scope. New Player Ramp/Integration into Game - Offer paid immediate ramp package: one price to become 100 with full upgrades - Like the changes in Blue - Create new zones in Metro areas that only 0-99 level can launch into, with statewide ranges Understand that catering to those who have money and like to use it for an advantage is a good business model and for those people it might be ok to offer very expensive options: - Shield generators: temporary energy shield that adds X% increase to defense or stops X% of damage - EMP's: turns Absorbs off for X minutes. Does not destroy, just turns off - Chain Lighting: Does damage across multiple players in a zone From a development standpoint I have no idea what is possible, easy or hard, but the general idea is to make the playing field more fair for the standard player while maintaining and growing a business revenue stream.\nYou need to come to the Northeast US. We handle our business like no other.", "answers": ["Keep deploying and harvesting your bases to earn experience points and level up quickly."], "length": 6594, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "3c8e9fef2eae8f49aae38b7314a3425b99c3ca8b051e8293"}
{"input": "Who was Ralph Rokebye's brother?", "context": "Rokebye, Ralph of Yorks, arm. Gloucester Hall, matric. 9 Nov., 1582, aged 15; student of Lincoln's Inn 1585. See Foster's Inns of Court Register.\nRokebye, Ralph of Herts (? Yorks), gent. Broadgates Hall, matric. entry 28 Feb., 1589-90, aged 14. See pedigree in Foster's Yorkshire Collection.\nRokeby, William (brother of Sir Richard, treasurer of Ireland, son of John, of Thundercliffe Grange, Yorks), fellow of King's Hall, Cambridge, D.Can.L.; rector of Sandal 1487, and of Halifax, Yorks, 1502, rector of Fakenham, Norfolk, 1496, chancellor of Ireland 1498, and 1515, bishop of Meath, and privy councillor 1507, archbishop of Dublin 1512, archdeacon of Surrey 1520, until his death 29 Nov., 1521. See Ath. ii. 717; Cotton, i. 25; & Lansdowne MS. 979, ff. 4, 6.\nRolfe, Augustine (Rolfus) M.A. from Queen's Coll., Cambridge, 1595; incorporated 10 July, 1599.\nRolf, Richard B.A. from Emanuel Coll., Cambridge, 1584-5 (incorporated 11 July, 1585); M.A. 1588. See Foster's Graduati Cantab.\nRolfe, William cler. fil. New Coll., matric. 10 March, 1656-7, B.A. 1660, fellow, M.A. 14 Jan., 1663-4; rector of Brampton 1668, and of Stoke Bruern, Northants, 1676, until his death, buried (at Stoke Bruern) 6 Sept., 1693. See Baker's Northants, i. 86.\nRolfe, William s. William, of Stoke-Bruern, Northants, cler. Brasenose Coll. 7 July, 1688, aged 16; student of Inner Temple 1692, buried in the Temple church 1 March, 1692-3. See Foster's Inns of Court Reg.\nRolle, Denis youngest son John, of Steventon, Devon, equitis. Exeter Coll., matric. 15 Feb., 1666-7, aged 17; brother of John same date.\nRolle, Denis s. D., of Heanton, Devon, arm. Exeter Coll. matric. 24 Oct., 1687, aged 17; B.A. 1691, M.A. 1694 (as Denys), rector of Merton, Devon, 1696. See Samuel 1687, & Foster's Index Ecclesiasticus.\nRolle, (Sir) Henry of Devon, arm. fil. Broadgates Hall, matric. 14 June, 1594, aged 18; student of Middle Temple 1597 (as son and heir of Henry, of Steventon, Devon, esq.), knighted 23 July, 1603, died in 1617. See Foster's Inns of Court Reg.\nRolle, Henry of Devon, arm. Exeter Coll., matric. 20 March, 1606-7, aged 17; bar.-at-law, Inner Temple, 1618, bencher 1633 (2s. Robert, of Heanton, Devon), M.P. Callington 1621-2, 1624-5, Truro 1625-1626, 1628-9, serjeant-at-law 1640, recorder of Dorchester 1636, a judge of king's bench 1645, chief justice of upper bench 1648-55, died 30 July, 1656, buried in Shapwick church, Somerset; brother of Samuel 1605. See Ath. iii. 416; & Foster's Judges and Barristers.\nRolle, Henry s. Alex., of Tavistock, Devon, gent. Christ Church, matric. 23 March, 1696-7, aged 17.\nRolle, John of Devon, arm. Exeter Coll., matric. 30 May, 1589, aged 15; B.A. 8 Feb., 1592-3, M.A. 25 May, 1596.\nRolle, John 1s. John, of Steventon, Devon, equitis. Exeter Coll., matric. 15 Feb., 1666-7, aged 18; of Bicton, Devon; died in his father's lifetime, buried at Bicton 22 April, 1689; brother of Denis 1667, and father of Denis 1698.\nRolle, Richard s. Richard, of Cookeburye, Devon, gent. New Inn Hall, matric. 26 Sept., 1634, aged 18; B.A. from Jesus Coll., Cambridge, 1638, incorporated from Gloucester Hall 17 Dec., 1639, M.A. 2 July, 1642, rector of Sheviocke, Cornwall, 1656; father of the next-named. See Foster's Index Eccl.\nRolle, Richard s. R., of Sheviock, Cornwall, cler. St. Alban Hall, matric. 3 July, 1674, aged 17; B.A. 1678.\nRolle, Robert (Rooles or Roales) fellow New Coll. 1551-60 from Mark Lane, city of London, B.A. 26 June, 1555, M.A. 26 July, 1560, B.D. 22 Jan., 1572-3, D.D. June, 1585, a teacher in Westminster school; perhaps canon of Combe (4) in Wells, 1574, and rector of Stoke Climsland, Devon, 1574. See O.H.S. i. 345; & Foster's Index Eccl.\nRolle, Samuel s. Denis, of Great Torrington, Devon, arm. Exeter Coll., matric. 16 July, 1687, aged 18, B.A. 1691; bar.-at-law Middle Temple 1697; M.P. Barnstaple 1705, died 1747; see Denis 1687. See Foster's Judges and Barristers.\nRolle, William B.C.L. 14 July, 1528; perhaps vicar of Yarncombe, Devon, 1536. See Foster's Index Ecclesiasticus.\nRolles, Gabriel (Rooles) B.A. from St. John's Coll., Cambridge, 1610-11, M.A. 1614; incorporated 13 July, 1619, rector of East Locking, Berks, 1620, as Rolle. See Foster's Graduati Cantab.\nRolles, Richard gent. Jesus Coll., matric. 1 March, 1632-3, B.A. next day, M.A. 15 Oct., 1635; perhaps created B.D. 20 Dec. 1642, \"ex regis gratia,\" rector of Wavendon, Bucks, and of Witham, Essex, 1646, by the Westminster assembly. See Add. MS. 15,670, p. 70.\nRolles, William s. Richard, of Lewknor, Oxon, gent. St. John's Coll., matrie. 12 March, 1637-8, aged 17, B.A. 9 Nov., 1641, M.A. 6 July, 1644; B.D. from Jesus Coll. 12 Sept., 1661, rector of Wheatfield, Oxon, 1660, and of Chalfont St. Giles, Bucks, 1662. See Foster's Index Eccl.\nRolles, William created M.A. from Exeter Coll. 14 April, 1648.\nRolleston, Simon created M.A. 31 Aug., 1636.\nRolleston, Thomas of Devon, gent. Wadham Coll., matric. 12 May, 1620, aged 16.\nRollinson, Francis 1584. See Rallinson.\nRollinson, William s. \"Jose,\" of London, gent. St. John's Coll., matric. 7 March, 1694-5, aged 15; perhaps brother of John Rawlinson, of New Coll. 1692. See page 1236.\nRolt, Edward youngest son of Tho., of London, equitis. Merton Coll., matric. 7 Nov., 1701, aged 15; of Sacomb, Herts, and Chippenham, Wilts, student of Lincoln's Inn, 1702, M.P. St. Mawes 1713, Grantham 1715-22, Chippenham 1722; died 22 Dec., 1722; his father knighted 1 Oct., 1682, and died 9 Sept., 1710. See Foster's Parliamentary Dictionary.\nRolte, George s. Thomas, of St. Margarets par. Darenth, Kent, pleb. St. Alban Hall, matric. 17 June, 1631, aged 18; B.A. 20 June, 1631, M.A. 29 April, 1634, incorporated at Cambridge 1639.\nRomane, Edmund pleb. Balliol Coll., matric. 20 Feb., 1627-8, aged 18; B.A. next day, M.A. 3 June, 1630.\nRomaine, Matthew pleb. Balliol Coll., matric. 10 June, 1630, B.A. same day, M.A. 14 May, 1633, vicar of Stoke Gaylard, Dorset, 1639; father of the next. See Foster's Index Eccl.\nRomayne, Thomas s. Matth., of Stoke Gaylard, Dorset, minister. Wadham Coll., matric. 17 July, 1669, aged 17; B.A. from Hart Hall 1673, \"the intruded\" rector of Stoke Gaylard 1675. See Foster's Index Eccl.\nRomayne, William (Ronayne) gent. Trinity Coll., matric. 31 July, 1671, aged 16.\nRome, Harcourt s. William, of London, p.p. Brasenose Coll., matric. 13 Dec., 1672, aged 17.\nRome, William s. G. (? \"Gul.\"), of Northampton (city), pleb. Brasenose Coll., matric. 11 Dec., 1684, aged 16.\nRomney, Joseph B.A. from Emanuel Coll., Cambridge, 1610-11, M.A. 1614; incorporated 8 July, 1614, student of Inner Temple 1610, as of London, gent. See Foster's Inns of Court Reg.\nRone, John s. Randolph, of Hanmer, Flints, pleb. Brasenose Coll., matric. 10 Oct., 1634, aged 18; D.D. Trinity Coll., Dublin, 25 Jan., 1666 (as Roane), vicar of Hanmer, Flints, 1644, ejected same year, dean of Clogher 1667, bishop of Killaloe 1675, until his death 5 Sept., 1692. See Cotton's Fasti Ecc. Hib. i. 467.\nRone, William of New Coll. 1661. See Roane.\nRoode, Edward (or Rode) B.A. 21 July, 1522, M.A. 26 Nov., 1534; perhaps canon of Southwell 1561-73.\nRoode, Edward cler. fil. Merton Coll., matric. 22 Nov., 1650; Eton postmaster 1649, fellow 1651, B.A. 2 March, 1651-2, M.A. 14 Dec., 1655; incorporated at Cambridge 1657, and LL.D. 1671; vicar of Gamlingay, co. Cambridge, rector of one moiety 1661, and of the other 1677; died at Cambridge 1689. See Burrows, 525; & O.H.S. iv. 292.\nRoode, Onesiphorus s. Edward, of Thame, Oxon, sacerd. New Inn Hall, matric. 27 Oct., 1637, aged 16, B.A. 1 July, 1641; incorporated at Cambridge 1645; chaplain to the house of lords after the expulsion of the bishops; minister of New chapel, Tuttle-Fields, Westminster, 1648, until ejected in 1660. See Calamy, i. 195.\nRood, Richard M.A. from Pembroke Coll. 5 Dec., 1634.\nRooke, John s. Tho., of Broadwell, co. Gloucester, pleb. Pembroke Coll., matric. 1 March, 1683-4, aged 17; brother of Thomas 1693.\nRooke, John s. Tho., of Whitchurch, Wilts, gent. Balliol Coll., matric. 14 Jan., 1713-14, aged 17.\nRooke, Nicholas s. Arthur, of Totnes, Devon, gent. Exeter Coll., matric. 10 March, 1670-1, aged 16; B.A. 1674, M.A. 1677, rector of Dartington, Devon, 1679. See Foster's Index Eccl.\nRooke, Robert \"ser.\" Oriel Coll., matric. 1 April, 1656, B.A. 1659.\nRooke, Robert s. R., p.p. St. Alban Hall, matric. 30 March, 1677, aged 17.\nRooke, Thomas pleb. Christ Church, matric. 3 May, 1659.\nRooke, William (Roock) of Dorset, pleb. Brasenose Coll., matric. entry under date 20 March, 1578-9, aged 19; B.A. from St. Alban Hall 30 Jan., 1582-3, M.A. 9 May, 1586.\nRooke, William of Dorset, gent. New Coll., matric. 12 July, 1605, aged 18; B.A. 21 Feb., 1608-9, chaplain, M.A. 16 Dec., 1611, rector of North Cheriton, Somerset, 1618. See Foster's Index Eccl.\nRooke, William s. J., of Workington, Cumberland, p.p. Queen's Coll., matric. 22 Oct., 1669, aged 17; B.A. 1674, M.A. 1677, B.D. 1690, vicar of Plumstead, Kent, 1691, and rector of Hadley, Hants, 1695. See Foster's Index Eccl.\nRookes, Christopher (Rokys or Rokkis) B.A. 8 July, 1522, M.A. 1 July, 1527, B.D. supd. Oct., 1540; principal of Magdalen Hall 1529-32, vicar of Stanstead Abbots, Herts, 1534. See Foster's Index Eccl.\nRookes, Jonas B.A. from Magdalen Hall 24 April, 1599, M.A. 11 Feb., 1601-2 (2s. William, of Roydes Hall); vicar of Penistone, Yorks, 1619, see Foster's Index Eccl.; styled fellow and bursar of University Coll. in Foster's Yorkshire Collection, possibly brother of the next-named.\nRookes, Robert of Yorks, pleb. Magdalen Hall, matric. 14 May, 1602, aged 19; possibly brother of the last-named.\nRo(o)kes, William demy Magdalen Coll. 1544, B.A. supd. 1551, fellow 1552-71, M.A. 27 April, 1556, B.Med. supd. 24 April, 1561. See Bloxam, iv. 99.\nRookes, William s. William, of Rhodes Hall, Yorks, gent. University Coll., matric. 30 June, 1665, aged 16; died at Oxford in 1667.\nRoope, Ambrose s. A., of Dartmouth Parva, Devon, arm. Exeter Coll., matric. 15 March, 1671-2, aged 16.\nRoope, George s. Ant., of Bradford, Wilts, gent. Hart Hall, matric. 10 Oct., 1702, aged 15.\nRoope, John s. Nicholas, of Dartmouth, Devon, gent. Exeter Coll., matric. 17 Nov., 1637, aged 15; student of Lincoln's Inn 1638. See Foster's Inns of Court Reg.\nRoope, Nicholas of Devon, gent. Broadgates Hall, matric. 6 Feb., 1606-7, aged 18; B.A. 6 Nov., 1610; probably father of the last-named.\nRooper, Thomas s. T., of London, gent. Trinity Coll., matric. 9 July, 1699, aged 16; B.A. 1703, M.A. 19 Feb., 1705-6, as Roper.\nRooper, William of St. Alban Hall 1667. See Roper.\nRoos, Brian D.Can.L. or doctor of decrees of the university of Valentia; incorporated 3 Feb., 1510-11; died 1529, buried in the church of Chelray. See Fasti, i. 31.\nRoot, Isaac pleb. St. John's Coll., matric. 2 July, 1658, admitted to Merchant Taylors' school 1649 (only son of Isaac, merchant taylor); born in Trinity parish 20 Aug., 1641. See Robinson, i. 193.\nRoots, Richard s. Tho., of Tunbridge, Kent, gent. St. John's Coll., matric. 26 Dec., 1689, aged 15; demy Magdalen Coll. 1690-1702, B.A. 1693, M.A. 1696, rector of Chilmarck, Wilts, 1702-27, canon of Sarum 1722, rector and vicar of Bishopstone, Wilts, 1728; brother of William 1699. See Rawl. iii. 447, and xix. 90; Bloxam, vi. 111; & Foster's Index Eccl.\nRoots, Thomas of Sussex, pleb. Magdalen Hall, matric. entry 17 Nov., 1581, aged 13; B.A. supd. 1 July, 1584, bar.-at-law, Lincoln's Inn, 1594. See Foster's Judges and Barristers.\nRootes, Thomas s. William, of Tunbridge, Kent, pleb. St. John's Coll., matric. 31 Jan., 1628-9, aged 23; B.A. 12 Feb., 1628-9, vicar of Long Stanton All Saints, co. Cambridge, 1630. See Add. MSS. 15,669-70; & Foster's Index Eccl.\nRootes, Thomas pleb. St. John's Coll., matric. 2 July, 1658; B.A. 1661, M.A. 1666; possibly father of Richard 1689, and William 1699.\nRoots, William s. Tho., of Tunbridge, Kent, gent. Christ Church, matric. 16 March, 1698-9, aged 18; B.A. 1704; clerk Magdalen Coll. 1705-11, M.A. 1707, rector of Little Berkhampstead, Herts, 1714; brother of Richard 1689. See Bloxam, ii. 85; & Foster's Index Eccl.\nRoper, Francis s. Robert, of Trimdon, co. Durham, gent. Corpus Christi Coll., matric. 16 Dec., 1661, aged 18; probably identical with Francis, son of Robert, of Kelloe, co. Durham, farmer, was admitted sizar of St. John's Coll., Cambridge, 21 Sept., 1658, aged 16; fellow, B.A. 1662-3, M.A. 1666, B.D. 1673, vicar of Waterbeach, co. Cambridge, 1678, canon of Ely 1686-90, rector of Northwold, Norfolk, 1687, died 13 April, 1719. See Mayor, 138; Surtees' Durham, i. 107; & Foster's Index Eccl.\nRoper, John (or Rooper) demy Magdalen Coll., from Berks, M.A. fellow, 1483, D.D. disp. 27 June, 1506, (first) Margaret professor of divinity, 1500, vice-chancellor of the university 1505, and 1511, principal of Salesurry and George Hall, rector of Witney, Oxon, 1493, vicar of St. Mary's church, Oxford, canon of Cardinal Coll. 1532; died May, 1534. See Ath. i. 76; & Landsowne MS. 979, f. 118.\nRoper, John B.A. disp. 4 July, 1512.\nRoper, Thomas of Trinity Coll. 1699. See Rooper.\nRoper, Philip of Kent, arm. Gloucester Hall, matric. 7 Sept., 1588, aged 15 (subscribes Rooper).\nRoper, William (subscribes Rooper) of co. Hereford, militis fil. St. Alban Hall, matric. entry dated 5 June, 1607, aged 13; probably of Malmains, Kent, 2nd son of Sir Christopher Roper, afterwards 2nd baron Teynham. See Foster's Peerage.\nRoscarrock, Henry of Cornwall, arm. Hart Hall, matric. entry under date 17 Dec., 1576, aged 21; probably son of Thomas, of Roscarrock, and brother of the next, and of Richard 1581.\nRoscarrock, John B.A. 11 Feb., 1576-7; perhaps from Exeter Coll. (and 1s. Thomas, of Roscarrock, Cornwall); died 24 Nov., 1608; brother of Henry and Richard. See O.H.S. xii. 65.\nRoscarrock, Nicolas (Roiscariot) B.A. supd. 3 May, 1568, student Inner Temple 1571, as of Roscarrock, Cornwall. See Foster's Inns of Court Reg.\nRoscarrock, Richard of Cornwall, arm. Broadgates Hall, matric. entry under date circa 1581, aged 19; student of Middle Temple 1583 (as 3s. Thomas, of Roscarrock, Cornwall, esq.), brother of Henry and John. See Foster's Inns of Court Reg.\nRosdell, Christopher of Yorks, pleb. St. Edmund Hall, matric. entry under date 22 Dec., 1576, aged 22, B.A. 4 July, 1576; rector of St. Bennet Sherehog, London, 1579, and vicar of Somerton, Somerset, 1582. See Foster's Index Eccl.\nRose, Christopher s. John, of Marlow, Bucks, gent. Christ Church, matric. 13 Feb., 1622-3, aged 21, B.A. same day; rector of Hutton, Essex, 1642. See Foster's Index Ecclesiasticus.\nRose, Christopher s. Giles, of Lynn Regis, Norfolk, gent. Lincoln Coll., matric. 8 July, 1670, aged 15; student of Gray's Inn, 1673. See Foster's Gray's Inn Register.\nRose, Gilbert Augustinian Canon, B.D. supd. 22 May, 1512, and supd. 12 Dec., 1519, for incorporation as D.D.\nRose, Henry \"ser.\" Lincoln Coll., matric. 22 July, 1658, B.A. 16 Jan., 1660-1, fellow 1662 from Pirton, Oxon, M.A. 1663 (incorporated at Cambridge 1688), B.D. 1672; minister of All Saints, Oxford, but running much into debt, and marrying beneath himself, left his fellowship and church about 1674, retired to London, and at length to Ireland. See Ath. iv. 561.\nRose, Hugh s. \"Dav. Ni.\" (Nigg 4to.), of Ross, Scotland, p.p. (subs. pleb.). Balliol Coll., matric. 3 April, 1707, aged 20; B.A. 1709.\nRose, John B.A. 8 June, 1519, fellow Merton Coll. 1523, M.A. 31 March, 1525; one of these names vicar of Shoreham, Kent, 1536. See Foster's Index Ecclesiasticus.\nRose, John of co. Leicester, pleb. Merton Coll., matric. 24 Nov., 1581, aged 21.\nRose, John s. Jeremy, of Swell, co. Gloucester, pleb. Corpus Christi Coll., matric. 12 Dec., 1623, aged 15; B.A. 4 July, 1626.\nRose, John s. Rich., of Halberton, Devon, gent. Exeter Coll., matric. 14 May, 1688, aged 17.\nRose, John s. J., of West Derby, co. Lancaster, pleb. University Coll., matric. 7 March, 1712-13, aged 18, B.A. 1716; rector of Bilborough, Notts, 1722. See Foster's Index Eccl.\nRose, Jonathan s. Th., of Mickleton, co. Gloucester, pleb. St. Alban Hall, matric. 16 May, 1677, aged 18; B.A. 9 Feb., 1680-1.\nRose, Joseph s. Thomas, of Sturminster Newton, Dorset, pleb. Oriel Coll., matric. 12 Dec., 1623, aged 19.\nRose, Richard B.A. from Exeter Coll. 14 June, 1621; perhaps student of Middle Temple 1622 (as son and heir of John, of Lyme, Dorset, gent.), and M.P. Lyme Regis April-May, 1640, 1640 (l.p.), till his death after 1648. See Foster's Inns of Court Reg. & Foster's Parliamentary Dictionary.\nRose, Richard arm. Exeter Coll., matric. 29 March, 1656; student of Lincoln's Inn 1659, as 4s. Richard, of Wootton Fitzwarren, Dorset, esq. See Foster's Inns of Court Reg.\nRose, Richard s. Richard, of Monks Kirby, co. Warwick, pleb. Magdalen Coll., matric. 3 May, 1672, aged 16 (as Rosse); chorister 1670-6. See Bloxam, i. 95.\nRose, Richard s. R(ichard), of Wyng, Bucks, gent. Trinity Coll., matric. 7 May, 1680, aged 16; bar.-at-law, Inner Temple, 1699. See Foster's Judges and Barristers.\nRose, Stephen of co. Gloucester, pleb. Corpus Christi Coll., matric. 21 Jan., 1619-20, aged 16; B.A. 13 Nov., 1621, M.A. 2 July, 1625, vicar of Aldermaston 1627, and rector of Barkham 1633, and of Arborfield, Berks, 1640, and perhaps of Hartley Mawditt, Hants, 1652. See Foster's Index Ecclesiasticus.\nRose, Stephen \"ser.\" Lincoln Coll., matric. 19 Nov., 1650.\nRose, Stephen \"servi. fil.\" Magdalen Coll., matric. 19 Nov., 1650 (subscribes \"serv.\").\nRose, Stephen \"ser.\" Magdalen Coll., subscribed 23 Nov., 1655; B.A. from Wadham Coll. 1659, vicar of Cold Overton, co. Leicester, 1662-3, and rector of Woolhampton, Berks, 1667-95, father of Temple. See Foster's Index Eccl.\nRose, Temple s. Step., of Woolhampton, Berks, cler. Trinity Coll., matric. 29 March, 1693, aged 17, B.A. 1696.\nRose, Thomas Minorite, B.D. 22 June, 1509.\nRose, Thomas of Herts, pleb. Magdalen Hall, matric. 10 Oct., 1589, aged 15.\nRose, Thomas s. Seth, of Telscombe, Sussex, sacerd. Oriel Coll., matric. 5 June, 1640, aged 18; his father rector of Telscombe 1604, etc. See Foster's Index Eccl.\nRose, Thomas s. Edw., ", "answers": ["Sir Richard."], "length": 2952, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "dee63735510f6958f4c5bf318421c9b9fac0bc8b3e341a4a"}
{"input": "What are the datasets used in this community for research?", "context": "\\section{Introduction}\nUnderwater robot picking is to use the robot to automatically capture sea creatures like holothurian, echinus, scallop, or starfish in an open-sea farm where underwater object detection is the key technology for locating creatures. Until now, the datasets used in this community are released by the Underwater Robot Professional Contest (URPC$\\protect\\footnote{Underwater Robot Professional Contest: {\\bf http://en.cnurpc.org}.}$) beginning from 2017, in which URPC2017 and URPC2018 are most often used for research. Unfortunately, as the information listed in Table \\ref{Info}, URPC series datasets do not provide the annotation file of the test set and cannot be downloaded after the contest. \nTherefore, researchers \\cite{2020arXiv200511552C,2019arXiv191103029L} first have to divide the training data into two subsets, including a new subset of training data and a new subset of testing data, and then train their proposed method and other \\emph{SOTA} methods. On the one hand, training other methods results in a significant increase in workload. On the other hand, different researchers divide different datasets in different ways, \n\\begin{table}[t]\n\\renewcommand\\tabcolsep{3.5pt}\n\\caption{Information about all the collected datasets. * denotes the test set's annotations are not available. \\emph{3} in Class means three types of creatures are labeled, \\emph{i.e.,} holothurian, echinus, and scallop. \\emph{4} means four types of creatures are labeled (starfish added). Retention represents the proportion of images that retain after similar images have been removed.}\n\\centering \n\\begin{tabular}{|l|c|c|c|c|c|}\n\\hline\nDataset&Train&Test&Class&Retention&Year \\\\ \n\\hline \nURPC2017&17,655&985*&3&15\\%&2017 \\\\\n\\hline\nURPC2018&2,901&800*&4&99\\%&2018 \\\\\n\\hline\nURPC2019&4,757&1,029*&4&86\\%&2019 \\\\\n\\hline\nURPC2020$_{ZJ}$&5,543&2,000*&4&82\\%&2020 \\\\\n\\hline\nURPC2020$_{DL}$&6,575&2,400*&4&80\\%&2020 \\\\\n\\hline\nUDD&1,827&400&3&84\\%&2020 \\\\\n\\hline \n\n\\end{tabular}\n\\label{Info}\n\\end{table}\n\\begin{figure*}[htbp]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{example.pdf}\n\\end{center}\n   \\caption{Examples in DUO, which show a variety of scenarios in underwater environments.}\n\\label{exam}\n\\end{figure*}\ncausing there is no unified benchmark to compare the performance of different algorithms.\nIn terms of the content of the dataset images, there are a large number of similar or duplicate images in the URPC datasets. URPC2017 only retains 15\\% images after removing similar images compared to other datasets. Thus the detector trained on URPC2017 is easy to overfit and cannot reflect the real performance.\nFor other URPC datasets, the latter also includes images from the former, \\emph{e.g.}, URPC2019 adds 2,000 new images compared to URPC2018; compared with URPC2019, URPC2020$_{ZJ}$ adds 800 new images. The URPC2020$_{DL}$ adds 1,000 new images compared to the URPC2020$_{ZJ}$. It is worth mentioning that the annotation of all datasets is incomplete; some datasets lack the starfish labels and it is easy to find error or missing labels. \\cite{DBLP:conf/iclr/ZhangBHRV17} pointed out that although the CNN model has a strong fitting ability for any dataset, the existence of dirty data will significantly weaken its robustness.\nTherefore, a reasonable dataset (containing a small number of similar images as well as an accurate annotation) and a corresponding recognized benchmark are urgently needed to promote community development.\n\n\nTo address these issues, we introduce a dataset called Detecting Underwater Objects (DUO) by collecting and re-annotating all the available underwater datasets. It contains 7,782 underwater images after deleting overly similar images and has a more accurate annotation with four types of classes (\\emph{i.e.,} holothurian, echinus, scallop, and starfish). \nBesides, based on the MMDetection$\\protect\\footnote{MMDetection is an open source object detection toolbox based on PyTorch. {\\bf https://github.com/open-mmlab/mmdetection}}$ \\cite{chen2019mmdetection} framework, we also provide a \\emph{SOTA} detector benchmark containing efficiency and accuracy indicators, providing a reference for both academic research and industrial applications. It is worth noting that JETSON AGX XAVIER$\\protect\\footnote{JETSON AGX XAVIER is an embedded development board produced by NVIDIA which could be deployed in an underwater robot. Please refer {\\bf https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit} for more information.}$ was used to assess all the detectors in the efficiency test in order to simulate robot-embedded environment. DUO will be released in https://github.com/chongweiliu soon.\n\nIn summary, the contributions of this paper can be listed as follows.\n\n  $\\bullet$ By collecting and re-annotating all relevant datasets, we introduce a dataset called DUO with more reasonable annotations as well as a variety of underwater scenes.\n\n  $\\bullet$ We provide a corresponding benchmark of \\emph{SOTA} detectors on DUO including efficiency and accuracy indicators which could be a reference for both academic research and industrial applications. \n\n\n\\pagestyle{empty}\n\\section{Background}\nIn the year of 2017, underwater object detection for open-sea farming is first proposed in the target recognition track of Underwater Robot Picking Contest 2017$\\protect\\footnote{From 2020, the name has been changed into Underwater Robot Professional Contest which is also short for URPC.}$ (URPC2017) which aims to promote the development of theory, technology, and industry of the underwater agile robot and fill the blank of the grabbing task of the underwater agile robot. The competition sets up a target recognition track, a fixed-point grasping track, and an autonomous grasping track. The target recognition track concentrates on finding the {\\bf high accuracy and efficiency} algorithm which could be used in an underwater robot for automatically grasping.\n\nThe datasets we used to generate the DUO are listed below. The detailed information has been shown in Table \\ref{Info}.\n\n  {\\bf URPC2017}: It contains 17,655 images for training and 985 images for testing and the resolution of all the images is 720$\\times$405. All the images are taken from 6 videos at an interval of 10 frames. However, all the videos were filmed in an artificial simulated environment and pictures from the same video look almost identical. \n  \n   {\\bf URPC2018}: It contains 2,901 images for training and 800 images for testing and the resolutions of the images are 586$\\times$480, 704$\\times$576, 720$\\times$405, and 1,920$\\times$1,080. The test set's annotations are not available. Besides, some images were also collected from an artificial underwater environment.\n  \n  {\\bf URPC2019}: It contains 4,757 images for training and 1029 images for testing and the highest resolution of the images is 3,840$\\times$2,160 captured by a GOPro camera. The test set's annotations are also not available and it contains images from the former contests.\n  \n  {\\bf URPC2020$_{ZJ}$}: From 2020, the URPC will be held twice a year. It was held first in Zhanjiang, China, in April and then in Dalian, China, in August. URPC2020$_{ZJ}$ means the dataset released in the first URPC2020 and URPC2020$_{DL}$ means the dataset released in the second URPC2020. This dataset contains 5,543 images for training and 2,000 images for testing and the highest resolution of the images is 3,840$\\times$2,160. The test set's annotations are also not available.\n  \n  {\\bf URPC2020$_{DL}$}: This dataset contains 6,575 images for training and 2,400 images for testing and the highest resolution of the images is 3,840$\\times$2,160. The test set's annotations are also not available.\n  \n  {\\bf UDD \\cite{2020arXiv200301446W}}: This dataset contains 1,827 images for training and 400 images for testing and the highest resolution of the images is 3,840$\\times$2,160. All the images are captured by a diver and a robot in a real open-sea farm.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{pie.pdf}\n\\end{center}\n   \\caption{The proportion distribution of the objects in DUO.}\n\\label{pie}\n\\end{figure}\n\n\n\n\\begin{figure*}\n  \\centering\n  \\subfigure[]{\\includegraphics[width=3.45in]{imagesize.pdf}}\n  \\subfigure[]{\\includegraphics[width=3.45in]{numInstance.pdf}}\n  \\caption{(a) The distribution of instance sizes for DUO; (b) The number of categories per image.}\n  \\label{sum}\n\\end{figure*}\n\\section{Proposed Dataset}\n\n\\subsection{Image Deduplicating}\nAs we explained in Section 1, there are a large number of similar or repeated images in the series of URPC datasets. Therefore, it is important to delete duplicate or overly similar images and keep a variety of underwater scenarios when we merge these datasets together. Here we employ the Perceptual Hash algorithm (PHash) to remove those images. PHash has the special property that the hash value is dependent on the image content, and it remains approximately the same if the content is not significantly modified. Thus we can easily distinguish different scenarios and delete duplicate images within one scenario. \n\nAfter deduplicating, we obtain 7,782 images (6,671 images for training; 1,111 for testing). The retention rate of the new dataset is 95\\%, which means that there are only a few similar images in the new dataset. Figure \\ref{exam} shows that our dataset also retains various underwater scenes.\n\n\\subsection{Image Re-annotation}\nDue to the small size of objects and the blur underwater environment, there are always missing or wrong labels in the existing annotation files. In addition, some test sets' annotation files are not available and some datasets do not have the starfish annotation. In order to address these issues, we follow the next process which combines a CNN model and manual annotation to re-annotate these images. Specifically, we first train a detector (\\emph{i.e.,} GFL \\cite{li2020generalized}) with the originally labeled images. After that, the trained detector predicts all the 7,782 images. We treat the prediction as the groundtruth and use it to train the GFL again. We get the final GFL prediction called {\\bf the coarse annotation}. Next, we use manual correction to get the final annotation called {\\bf the fine annotation}. Notably, we adopt the COCO \\cite{Belongie2014} annotation form as the final format.\n\\subsection{Dataset Statistics}\n{\\bf The proportion of classes}: The total number of objects is 74,515. Holothurian, echinus, scallop, and starfish are 7,887, 50,156, 1,924, and 14,548, respectively. Figure \\ref{pie} shows the proportion of each creatures where echinus accounts for 67.3\\% of the total. The whole data distribution shows an obvious long-tail distribution because the different economic benefits of different seafoods determine the different breed quantities.\n\n{\\bf The distribution of instance sizes}: Figure \\ref{sum}(a) shows an instance size distribution of DUO. \\emph{Percent of image size} represents the ratio of object area to image area, and \\emph{Percent of instance} represents the ratio of the corresponding number of objects to the total number of objects. Because of these small creatures and high-resolution images, the vast majority of objects occupy 0.3\\% to 1.5\\% of the image area.\n\n{\\bf The instance number per image}: Figure \\ref{sum}(b) illustrates the number of categories per image for DUO. \\emph{Number of instances} represents the number of objects one image has, and \\emph{ Percentage of images} represents the ratio of the corresponding number of images to the total number of images. Most images contain between 5 and 15 instances, with an average of 9.57 instances per image.\n\n{\\bf Summary}:\nIn general, smaller objects are harder to detect. For PASCAL VOC \\cite{Everingham2007The} or COCO \\cite{Belongie2014}, roughly 50\\% of all objects occupy no more than 10\\% of the image itself, and others evenly occupy from 10\\% to 100\\%. \nIn the aspect of instances number per image, COCO contains 7.7 instances per image and VOC contains 3. In comparison, DUO has 9.57 instances per image and most instances less than 1.5\\% of the image size.\nTherefore, DUO contains almost exclusively massive small instances and has the long-tail distribution at the same time, which means it is promising to design a detector to deal with massive small objects and stay high efficiency at the same time for underwater robot picking.\n\n\\section{Benchmark}\nBecause the aim of underwater object detection for robot picking is to find {\\bf the high accuracy and efficiency} algorithm, we consider both the accuracy and efficiency evaluations in the benchmark as shown in Table \\ref{ben}.\n\n\\subsection{Evaluation Metrics}\nHere we adopt the standard COCO metrics (mean average precision, \\emph{i.e.,} mAP) for the accuracy evaluation and also provide the mAP of each class due to the long-tail distribution.\n\n{\\bf AP} -- mAP at IoU=0.50:0.05:0.95.\n\n{\\bf AP$_{50}$} --  mAP at IoU=0.50.\n\n{\\bf AP$_{75}$} --  mAP at IoU=0.75. \n\n{\\bf AP$_{S}$} --   {\\bf AP} for small objects of area smaller than 32$^{2}$.\n\n{\\bf AP$_{M}$} --   {\\bf AP} for objects of area between 32$^{2}$ and 96$^{2}$.\n\n{\\bf AP$_{L}$} --   {\\bf AP} for large objects of area bigger than 96$^{2}$.\n\n{\\bf AP$_{Ho}$} --  {\\bf AP} in holothurian.\n\n{\\bf AP$_{Ec}$} --  {\\bf AP} in echinus.\n\n{\\bf AP$_{Sc}$} --  {\\bf AP} in scallop.\n\n{\\bf AP$_{St}$} --  {\\bf AP} in starfish.\n\n\nFor the efficiency evaluation, we provide three metrics:\n\n{\\bf Param.} --  The parameters of a detector.\n\n{\\bf FLOPs} --  Floating-point operations per second.\n\n{\\bf FPS} --  Frames per second.\n\nNotably, {\\bf FLOPs} is calculated under the 512$\\times$512 input image size and {\\bf FPS} is tested on a JETSON AGX XAVIER under MODE$\\_$30W$\\_$ALL. \n\n\\subsection{Standard Training Configuration}\nWe follow a widely used open-source toolbox, \\emph{i.e.,} MMDetection (V2.5.0) to produce up our benchmark. During the training, the standard configurations are as follows:\n\n  $\\bullet$ We initialize the backbone models (\\emph{e.g.,} ResNet50) with pre-trained parameters on ImageNet \\cite{Deng2009ImageNet}.\n\n  $\\bullet$ We resize each image into 512 $\\times$ 512 pixels both in training and testing. Each image is flipped horizontally with 0.5 probability during training.\n\n  $\\bullet$ We normalize RGB channels by subtracting 123.675, 116.28, 103.53 and dividing by 58.395, 57.12, 57.375, respectively.\n\n  $\\bullet$ SGD method is adopted to optimize the model. The initial learning rate is set to be 0.005 in a single GTX 1080Ti with batchsize 4 and is decreased by 0.1 at the 8th and 11th epoch, respectively. WarmUp \\cite{2019arXiv190307071L} is also employed in the first 500 iterations. Totally there are 12 training epochs.\n\n  $\\bullet$ Testing time augmentation (\\emph{i.e.,} flipping test or multi-scale testing) is not employed.\n\n\n\n\\subsection{Benchmark Analysis}\nTable \\ref{ben} shows the benchmark for the \\emph{SOTA} methods. Multi- and one- stage detectors with three kinds of backbones (\\emph{i.e.,} ResNet18, 50, 101) give a comprehensive assessment on DUO. We also deploy all the methods to AGX to assess efficiency.\n\nIn general, the multi-stage (Cascade R-CNN) detectors have high accuracy and low efficiency, while the one-stage (RetinaNet) detectors have low accuracy and high efficiency. However, due to recent studies \\cite{zhang2019bridging} on the allocation of more reasonable positive and negative samples in training, one-stage detectors (ATSS or GFL) can achieve both high accuracy and high efficiency.\n\n\\begin{table*}[htbp]\n\\renewcommand\\tabcolsep{3.0pt}\n\n\\begin{center}\n\\caption{Benchmark of \\emph{SOTA} detectors (single-model and single-scale results) on DUO. FPS is measured on the same machine with a JETSON AGX XAVIER under the same MMDetection framework, using a batch size of 1 whenever possible. R: ResNet.} \n\\label{ben}\n\\begin{tabular}{|l|l|c|c|c|ccc|ccc|cccc|}\n\\hline\nMethod&Backbone&Param.&FLOPs&FPS&AP&AP$_{50}$&AP$_{75}$&AP$_{S}$&AP$_{M}$&AP$_{L}$&AP$_{Ho}$&AP$_{Ec}$&AP$_{Sc}$&AP$_{St}$ \\\\ \n\\hline \n\\emph{multi-stage:} &&&&&&&&&&&&&& \\\\\n\n\\multirow{3}{*}{Faster R-CNN \\cite{Ren2015Faster}}\n&R-18&28.14M&49.75G&5.7&50.1&72.6&57.8&42.9&51.9&48.7&49.1&60.1&31.6&59.7\\\\\n&R-50&41.14M&63.26G&4.7&54.8&75.9&63.1&53.0&56.2&53.8&55.5&62.4&38.7&62.5\\\\\n&R-101&60.13M&82.74G&3.7&53.8&75.4&61.6&39.0&55.2&52.8&54.3&62.0&38.5&60.4\\\\\n\\hline\n\n\\multirow{3}{*}{Cascade R-CNN \\cite{Cai_2019}}\n&R-18&55.93M&77.54G&3.4&52.7&73.4&60.3&\\bf 49.0&54.7&50.9&51.4&62.3&34.9&62.3\\\\\n&R-50&68.94M&91.06G&3.0&55.6&75.5&63.8&44.9&57.4&54.4&56.8&63.6&38.7&63.5\\\\\n&R-101&87.93M&110.53G&2.6&56.0&76.1&63.6&51.2&57.5&54.7&56.2&63.9&41.3&62.6\\\\\n\\hline\n\n\\multirow{3}{*}{Grid R-CNN \\cite{lu2019grid}}\n&R-18&51.24M&163.15G&3.9&51.9&72.1&59.2&40.4&54.2&50.1&50.7&61.8&33.3&61.9\\\\\n&R-50&64.24M&176.67G&3.4&55.9&75.8&64.3&40.9&57.5&54.8&56.7&62.9&39.5&64.4\\\\\n&R-101&83.24M&196.14G&2.8&55.6&75.6&62.9&45.6&57.1&54.5&55.5&62.9&41.0&62.9\\\\\n\\hline\n\n\\multirow{3}{*}{RepPoints \\cite{yang2019reppoints}}\n&R-18&20.11M&\\bf 35.60G&5.6&51.7&76.9&57.8&43.8&54.0&49.7&50.8&63.3&33.6&59.2\\\\\n&R-50&36.60M&48.54G&4.8&56.0&80.2&63.1&40.8&58.5&53.7&56.7&65.7&39.3&62.3\\\\\n&R-101&55.60M&68.02G&3.8&55.4&79.0&62.6&42.2&57.3&53.9&56.0&65.8&39.0&60.9\\\\\n\\hline \n\\hline \n\\emph{one-stage:} &&&&&&&&&&&&&& \\\\\n\\multirow{3}{*}{RetinaNet \\cite{Lin2017Focal}}\n&R-18&19.68M&39.68G&7.1&44.7&66.3&50.7&29.3&47.6&42.5&46.9&54.2&23.9&53.8\\\\\n&R-50&36.17M&52.62G&5.9&49.3&70.3&55.4&36.5&51.9&47.6&54.4&56.6&27.8&58.3\\\\\n&R-101&55.16M&72.10G&4.5&50.4&71.7&57.3&34.6&52.8&49.0&54.6&57.0&33.7&56.3\\\\\n\\hline \n\n\\multirow{3}{*}{FreeAnchor \\cite{2019arXiv190902466Z}}\n&R-18&19.68M&39.68G&6.8&49.0&71.9&55.3&38.6&51.7&46.7&47.2&62.8&28.6&57.6\\\\\n&R-50&36.17M&52.62G&5.8&54.4&76.6&62.5&38.1&55.7&53.4&55.3&65.2&35.3&61.8\\\\\n&R-101&55.16M&72.10G&4.4&54.6&76.9&62.9&36.5&56.5&52.9&54.0&65.1&38.4&60.7\\\\\n\\hline \n\n\\multirow{3}{*}{FoveaBox \\cite{DBLP:journals/corr/abs-1904-03797}}\n&R-18&21.20M&44.75G&6.7&51.6&74.9&57.4&40.0&53.6&49.8&51.0&61.9&34.6&59.1\\\\\n&R-50&37.69M&57.69G&5.5&55.3&77.8&62.3&44.7&57.4&53.4&57.9&64.2&36.4&62.8\\\\\n&R-101&56.68M&77.16G&4.2&54.7&77.3&62.3&37.7&57.1&52.4&55.3&63.6&38.9&60.8\\\\\n\\hline \n\n\\multirow{3}{*}{PAA \\cite{2020arXiv200708103K}}\n&R-18&\\bf 18.94M&38.84G&3.0&52.6&75.3&58.8&41.3&55.1&50.2&49.9&64.6&35.6&60.5\\\\\n&R-50&31.89M&51.55G&2.9&56.8&79.0&63.8&38.9&58.9&54.9&56.5&66.9&39.9&64.0\\\\\n&R-101&50.89M&71.03G&2.4&56.5&78.5&63.7&40.9&58.7&54.5&55.8&66.5&42.0&61.6\\\\\n\\hline \n\n\\multirow{3}{*}{FSAF \\cite{zhu2019feature}}\n&R-18&19.53M&38.88G&\\bf 7.4&49.6&74.3&55.1&43.4&51.8&47.5&45.5&63.5&30.3&58.9\\\\\n&R-50&36.02M&51.82G&6.0&54.9&79.3&62.1&46.2&56.7&53.3&53.7&66.4&36.8&62.5\\\\\n&R-101&55.01M&55.01G&4.5&54.6&78.7&61.9&46.0&57.1&52.2&53.0&66.3&38.2&61.1\\\\\n\\hline \n\n\\multirow{3}{*}{FCOS \\cite{DBLP:journals/corr/abs-1904-01355}}\n&R-18&\\bf 18.94M&38.84G&6.5&48.4&72.8&53.7&30.7&50.9&46.3&46.5&61.5&29.1&56.6\\\\\n&R-50&31.84M&50.34G&5.4&53.0&77.1&59.9&39.7&55.6&50.5&52.3&64.5&35.2&60.0\\\\\n&R-101&50.78M&69.81G&4.2&53.2&77.3&60.1&43.4&55.4&51.2&51.7&64.1&38.5&58.5\\\\\n\\hline \n\n\\multirow{3}{*}{ATSS \\cite{zhang2019bridging}}\n&R-18&\\bf 18.94M&38.84G&6.0&54.0&76.5&60.9&44.1&56.6&51.4&52.6&65.5&35.8&61.9\\\\\n&R-50&31.89M&51.55G&5.2&58.2&\\bf 80.1&66.5&43.9&60.6&55.9&\\bf 58.6&67.6&41.8&64.6\\\\\n&R-101&50.89M&71.03G&3.8&57.6&79.4&65.3&46.5&60.3&55.0&57.7&67.2&42.6&62.9\\\\\n\\hline \n\n\\multirow{3}{*}{GFL \\cite{li2020generalized}}\n&R-18&19.09M&39.63G&6.3&54.4&75.5&61.9&35.0&57.1&51.8&51.8&66.9&36.5&62.5\\\\\n&R-50&32.04M&52.35G&5.5&\\bf 58.6&79.3&\\bf 66.7&46.5&\\bf 61.6&55.6&\\bf 58.6&\\bf 69.1&41.3&\\bf 65.3\\\\\n&R-101&51.03M&71.82G&4.1&58.3&79.3&65.5&45.1&60.5&\\bf 56.3&57.0&\\bf 69.1&\\bf 43.0&64.0\\\\\n\n\n\\hline \n\\end{tabular}\n\\end{center}\n\\end{table*}\nTherefore, in terms of accuracy, the accuracy difference between the multi- and the one- stage methods in AP is not obvious, and the AP$_{S}$ of different methods is always the lowest among the three size AP. For class AP, AP$_{Sc}$ lags significantly behind the other three classes because it has the smallest number of instances. In terms of efficiency, large parameters and FLOPs result in low FPS on AGX, with a maximum FPS of 7.4, which is hardly deployable on underwater robot. Finally, we also found that ResNet101 was not significantly improved over ResNet50, which means that a very deep network may not be useful for detecting small creatures in underwater scenarios. \n\nConsequently, the design of high accuracy and high efficiency detector is still the main direction in this field and there is still large space to improve the performance.\nIn order to achieve this goal, a shallow backbone with strong multi-scale feature fusion ability can be proposed to extract the discriminant features of small scale aquatic organisms; a specially designed training strategy may overcome the DUO's long-tail distribution, such as a more reasonable positive/negative label sampling mechanism or a class-balanced image allocation strategy within a training batch.\n\n\\section{Conclusion}\nIn this paper, we introduce a dataset (DUO) and a corresponding benchmark to fill in the gaps in the community. DUO contains a variety of underwater scenes and more reasonable annotations. Benchmark includes efficiency and accuracy indicators to conduct a comprehensive evaluation of the \\emph{SOTA} decoders. The two contributions could serve as a reference for academic research and industrial applications, as well as promote community development.\n\\bibliographystyle{IEEEbib}\n", "answers": ["URPC2017, URPC2018, URPC2019, URPC2020_ZJ and URPC2020_DL."], "length": 2616, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "b321c147c0e0a8a35ee8738ad89326ecad1509bf591b4233"}
{"input": "What is the attention module pretrained on?", "context": "Introduction\nSpeech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language BIBREF0; or in online courses, where audiences and speakers use different languages BIBREF1. To tackle this problem, existing approaches can be categorized into cascaded method BIBREF2, BIBREF3, where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method BIBREF4, BIBREF5, where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest. Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory.\nSince it is hard to obtain a large-scale ST dataset, multi-task learning BIBREF5, BIBREF6 and pre-training techniques BIBREF7 have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT. A common practice is to pre-train two encoder-decoder models for ASR and MT respectively, and then initialize the ST model with the encoder of the ASR model and the decoder of the MT model. Subsequently, the ST model is optimized with the multi-task learning by weighing the losses of ASR, MT, and ST. This approach, however, causes a huge gap between pre-training and fine-tuning, which are summarized into three folds:\nSubnet Waste: The ST system just reuses the ASR encoder and the MT decoder, while discards other pre-trained subnets, such as the MT encoder. Consequently, valuable semantic information captured by the MT encoder cannot be inherited by the final ST system.\nRole Mismatch: The speech encoder plays different roles in pre-training and fine-tuning. The encoder is a pure acoustic model in pre-training, while it has to extract semantic and linguistic features additionally in fine-tuning, which significantly increases the learning difficulty.\nNon-pre-trained Attention Module: Previous work BIBREF6 trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST does not benefit from the pre-training.\nTo address these issues, we propose a Tandem Connectionist Encoding Network (TCEN), which is able to reuse all subnets in pre-training, keep the roles of subnets consistent, and pre-train the attention module. Concretely, the TCEN consists of three components, a speech encoder, a text encoder, and a target text decoder. Different from the previous work that pre-trains an encoder-decoder based ASR model, we only pre-train an ASR encoder by optimizing the Connectionist Temporal Classification (CTC) BIBREF8 objective function. In this way, the additional decoder of ASR is not required while keeping the ability to read acoustic features into the source language space by the speech encoder. Besides, the text encoder and decoder can be pre-trained on a large MT dataset. After that, we employ common used multi-task learning method to jointly learn ASR, MT and ST tasks.\nCompared to prior works, the encoder of TCEN is a concatenation of an ASR encoder and an MT encoder and our model does not have an ASR decoder, so the subnet waste issue is solved. Furthermore, the two encoders work at tandem, disentangling acoustic feature extraction and linguistic feature extraction, ensuring the role consistency between pre-training and fine-tuning. Moreover, we reuse the pre-trained MT attention module in ST, so we can leverage the alignment information learned in pre-training.\nSince the text encoder consumes word embeddings of plausible texts in MT task but uses speech encoder outputs in ST task, another question is how one guarantees the speech encoder outputs are consistent with the word embeddings. We further modify our model to achieve semantic consistency and length consistency. Specifically, (1) the projection matrix at the CTC classification layer for ASR is shared with the word embedding matrix, ensuring that they are mapped to the same latent space, and (2) the length of the speech encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence. To bridge the length gap, source sentences in MT are lengthened by adding word repetitions and blank tokens to mimic the CTC output sequences.\nWe conduct comprehensive experiments on the IWSLT18 speech translation benchmark BIBREF1, demonstrating the effectiveness of each component. Our model is significantly better than previous methods by 3.6 and 2.2 BLEU scores for the subword-level decoding and character-level decoding strategies, respectively.\nOur contributions are three-folds: 1) we shed light on why previous ST models cannot sufficiently utilize the knowledge learned from the pre-training process; 2) we propose a new ST model, which alleviates shortcomings in existing methods; and 3) we empirically evaluate the proposed model on a large-scale public dataset.\nBackground ::: Problem Formulation\nEnd-to-end speech translation aims to translate a piece of audio into a target-language translation in one step. The raw speech signals are usually converted to sequences of acoustic features, e.g. Mel filterbank features. Here, we define the speech feature sequence as $\\mathbf {x} = (x_1, \\cdots , x_{T_x})$.The transcription and translation sequences are denoted as $\\mathbf {y^{s}} = (y_1^{s}, \\cdots , y_{T_s}^{s})$, and $\\mathbf {y^{t}} = (y_1^{t}, \\cdots , y_{T_t}^{t})$ repectively. Each symbol in $\\mathbf {y^{s}}$ or $\\mathbf {y^{t}}$ is an integer index of the symbol in a vocabulary $V_{src}$ or $V_{trg}$ respectively (e.g. $y^s_i=k, k\\in [0, |V_{src}|-1]$). In this work, we suppose that an ASR dataset, an MT dataset, and a ST dataset are available, denoted as $\\mathcal {A} = \\lbrace (\\mathbf {x_i}, \\mathbf {y^{s}_i})\\rbrace _{i=0}^I$, $\\mathcal {M} =\\lbrace (\\mathbf {y^{s}_j}, \\mathbf {y^{t}_j})\\rbrace _{j=0}^J$ and $ \\mathcal {S} =\\lbrace (\\mathbf {x_l}, \\mathbf {y^{t}_l})\\rbrace _{l=0}^L$ respectively. Given a new piece of audio $\\mathbf {x}$, our goal is to learn an end to end model to generate a translation sentence $\\mathbf {y^{t}}$ without generating an intermediate result $\\mathbf {y^{s}}$.\nBackground ::: Multi-Task Learning and Pre-training for ST\nTo leverage large scale ASR and MT data, multi-task learning and pre-training techniques are widely employed to improve the ST system. As shown in Figure FIGREF4, there are three popular multi-task strategies for ST, including 1) one-to-many setting, in which a speech encoder is shared between ASR and ST tasks; 2) many-to-one setting in which a decoder is shared between MT and ST tasks; and 3) many-to-many setting where both the encoder and decoder are shared.\nA many-to-many multi-task model contains two encoders as well as two decoders. It can be jointly trained on ASR, MT, and ST tasks. As the attention module is task-specific, three attentions are defined.\nUsually, the size of $\\mathcal {A}$ and $\\mathcal {M}$ is much larger than $\\mathcal {S}$. Therefore, the common training practice is to pre-train the model on ASR and MT tasks and then fine-tune it with a multi-task learning manner. However, as aforementioned, this method suffers from subnet waste, role mismatch and non-pre-trained attention issues, which severely limits the end-to-end ST performance.\nOur method\nIn this section, we first introduce the architecture of TCEN, which consists of two encoders connected in tandem, and one decoder with an attention module. Then we give the pre-training and fine-tuning strategy for TCEN. Finally, we propose our solutions for semantic and length inconsistency problems, which are caused by multi-task learning.\nOur method ::: TCEN Architecture\nFigure FIGREF5 sketches the overall architecture of TCEN, including a speech encoder $enc_s$, a text encoder $enc_t$ and a decoder $dec$ with an attention module $att$. During training, the $enc_s$ acts like an acoustic model which reads the input $\\mathbf {x}$ to word or subword representations $\\mathbf {h^s}$, then $enc_t$ learns high-level linguistic knowledge into hidden representations $\\mathbf {h^t}$. Finally, the $dec$ defines a distribution probability over target words. The advantage of our architecture is that two encoders disentangle acoustic feature extraction and linguistic feature extraction, making sure that valuable knowledge learned from ASR and MT tasks can be effectively leveraged for ST training. Besides, every module in pre-training can be utilized in fine-tuning, alleviating the subnet waste problem.\nFollow BIBREF9 inaguma2018speech, we use CNN-BiLSTM architecture to build our model. Specifically, the input features $\\mathbf {x}$ are organized as a sequence of feature vectors in length $T_x$. Then, $\\mathbf {x}$ is passed into a stack of two convolutional layers followed by max-pooling:\nwhere $\\mathbf {v}^{(l-1)}$ is feature maps in last layer and $\\mathbf {W}^{(l)}$ is the filter. The max-pooling layers downsample the sequence in length by a total factor of four. The down-sampled feature sequence is further fed into a stack of five bidirectional $d$-dimensional LSTM layers:\nwhere $[;]$ denotes the vector concatenation. The final output representation from the speech encoder is denoted as $\\mathbf {h^s}=(h^s_1, \\cdots , h^s_{\\frac{T_x}{4}})$, where $h_i^s \\in \\mathbb {R}^d$.\nThe text encoder $enc_t$ consists of two bidirectional LSTM layers. In ST task, $enc_t$ accepts speech encoder output $\\mathbf {h}^s$ as input. While in MT, $enc_t$ consumes the word embedding representation $\\mathbf {e^s}$ derived from $\\mathbf {y^s}$, where each element $e^s_i$ is computed by choosing the $y_i^s$-th vector from the source embedding matrix $W_{E^s}$. The goal of $enc_t$ is to extract high-level linguistic features like syntactic features or semantic features from lower level subword representations $\\mathbf {h}^s$ or $\\mathbf {e}^s$. Since $\\mathbf {h}^s$ and $\\mathbf {e}^s$ belong to different latent space and have different lengths, there remain semantic and length inconsistency problems. We will provide our solutions in Section SECREF21. The output sequence of $enc_t$ is denoted as $\\mathbf {h}^t$.\nThe decoder is defined as two unidirectional LSTM layers with an additive attention $att$. It predicts target sequence $\\mathbf {y^{t}}$ by estimating conditional probability $P(\\mathbf {y^{t}}|\\mathbf {x})$:\nHere, $z_k$ is the the hidden state of the deocder RNN at $k$ step and $c_k$ is a time-dependent context vector computed by the attention $att$.\nOur method ::: Training Procedure\nFollowing previous work, we split the training procedure to pre-training and fine-tuning stages. In pre-training stage, the speech encoder $enc_s$ is trained towards CTC objective using dataset $\\mathcal {A}$, while the text encoder $enc_t$ and the decoder $dec$ are trained on MT dataset $\\mathcal {M}$. In fine-tuning stage, we jointly train the model on ASR, MT, and ST tasks.\nOur method ::: Training Procedure ::: Pre-training\nTo sufficiently utilize the large dataset $\\mathcal {A}$ and $\\mathcal {M}$, the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.\nFor ASR task, in order to get rid of the requirement for decoder and enable the $enc_s$ to generate subword representation, we leverage connectionist temporal classification (CTC) BIBREF8 loss to train the speech encoder.\nGiven an input $\\mathbf {x}$, $enc_s$ emits a sequence of hidden vectors $\\mathbf {h^s}$, then a softmax classification layer predicts a CTC path $\\mathbf {\\pi }$, where $\\pi _t \\in V_{src} \\cup $ {`-'} is the observing label at particular RNN step $t$, and `-' is the blank token representing no observed labels:\nwhere $W_{ctc} \\in \\mathbb {R}^{d \\times (|V_{src}|+1)}$ is the weight matrix in the classification layer and $T$ is the total length of encoder RNN.\nA legal CTC path $\\mathbf {\\pi }$ is a variation of the source transcription $\\mathbf {y}^s$ by allowing occurrences of blank tokens and repetitions, as shown in Table TABREF14. For each transcription $\\mathbf {y}$, there exist many legal CTC paths in length $T$. The CTC objective trains the model to maximize the probability of observing the golden sequence $\\mathbf {y}^s$, which is calculated by summing the probabilities of all possible legal paths:\nwhere $\\Phi _T(y)$ is the set of all legal CTC paths for sequence $\\mathbf {y}$ with length $T$. The loss can be easily computed using forward-backward algorithm. More details about CTC are provided in supplementary material.\nFor MT task, we use the cross-entropy loss as the training objective. During training, $\\mathbf {y^s}$ is converted to embedding vectors $\\mathbf {e^s}$ through embedding layer $W_{E^s}$, then $enc_t$ consumes $\\mathbf {e^s}$ and pass the output $\\mathbf {h^t}$ to decoder. The objective function is defined as:\nOur method ::: Training Procedure ::: Fine-tune\nIn fine-tune stage, we jointly update the model on ASR, MT, and ST tasks. The training for ASR and MT follows the same process as it was in pre-training stage.\nFor ST task, the $enc_s$ reads the input $\\mathbf {x}$ and generates $\\mathbf {h^s}$, then $enc_t$ learns high-level linguistic knowledge into $\\mathbf {h^t}$. Finally, the $dec$ predicts the target sentence. The ST loss function is defined as:\nFollowing the update strategy proposed by BIBREF11 luong2015multi, we allocate a different training ratio $\\alpha _i$ for each task. When switching between tasks, we select randomly a new task $i$ with probability $\\frac{\\alpha _i}{\\sum _{j}\\alpha _{j}}$.\nOur method ::: Subnet-Consistency\nOur model keeps role consistency between pre-training and fine-tuning by connecting two encoders for ST task. However, this leads to some new problems: 1) The text encoder consumes $\\mathbf {e^s}$ during MT training, while it accepts $\\mathbf {h^s}$ during ST training. However, $\\mathbf {e^s}$ and $\\mathbf {h^s}$ may not follow the same distribution, resulting in the semantic inconsistency. 2) Besides, the length of $\\mathbf {h^s}$ is not the same order of magnitude with the length of $\\mathbf {e^s}$, resulting in the length inconsistency.\nIn response to the above two challenges, we propose two countermeasures: 1) We share weights between CTC classification layer and source-end word embedding layer during training of ASR and MT, encouraging $\\mathbf {e^s}$ and $\\mathbf {h^s}$ in the same space. 2)We feed the text encoder source sentences in the format of CTC path, which are generated from a seq2seq model, making it more robust toward long inputs.\nOur method ::: Subnet-Consistency ::: Semantic Consistency\nAs shown in Figure FIGREF5, during multi-task training, two different hidden features will be fed into the text encoder $enc_t$: the embedding representation $\\mathbf {e}^s$ in MT task, and the $enc_s$ output $\\mathbf {h^s}$ in ST task. Without any regularization, they may belong to different latent spaces. Due to the space gap, the $enc_t$ has to compromise between two tasks, limiting its performance on individual tasks.\nTo bridge the space gap, our idea is to pull $\\mathbf {h^s}$ into the latent space where $\\mathbf {e}^s$ belong. Specifically, we share the weight $W_{ctc}$ in CTC classification layer with the source embedding weights $W_{E^s}$, which means $W_{ctc} = W_{E^s}$. In this way, when predicting the CTC path $\\mathbf {\\pi }$, the probability of observing the particular label $w_i \\in V_{src}\\cup ${`-'} at time step $t$, $p(\\pi _t=w_i|\\mathbf {x})$, is computed by normalizing the product of hidden vector $h_t^s$ and the $i$-th vector in $W_{E^s}$:\nThe loss function closes the distance between $h^s_t$ and golden embedding vector, encouraging $\\mathbf {h}^s$ have the same distribution with $\\mathbf {e}^s$.\nOur method ::: Subnet-Consistency ::: Length Consistency\nAnother existing problem is length inconsistency. The length of the sequence $\\mathbf {h^s}$ is proportional to the length of the input frame $\\mathbf {x}$, which is much longer than the length of $\\mathbf {e^s}$. To solve this problem, we train an RNN-based seq2seq model to transform normal source sentences to noisy sentences in CTC path format, and replace standard MT with denoising MT for multi-tasking.\nSpecifically, we first train a CTC ASR model based on dataset $\\mathcal {A} = \\lbrace (\\mathbf {x}_i, \\mathbf {y}^s_i)\\rbrace _{i=0}^{I}$, and generate a CTC-path $\\mathbf {\\pi }_i$ for each audio $\\mathbf {x}_i$ by greedy decoding. Then we define an operation $S(\\cdot )$, which converts a CTC path $\\mathbf {\\pi }$ to a sequence of the unique tokens $\\mathbf {u}$ and a sequence of repetition times for each token $\\mathbf {l}$, denoted as $S(\\mathbf {\\pi }) = (\\mathbf {u}, \\mathbf {l})$. Notably, the operation is reversible, meaning that $S^{-1} (\\mathbf {u}, \\mathbf {l})=\\mathbf {\\pi }$. We use the example $\\mathbf {\\pi _1}$ in Table TABREF14 and show the corresponding $\\mathbf {u}$ and $\\mathbf {l}$ in Table TABREF24.\nThen we build a dataset $\\mathcal {P} = \\lbrace (\\mathbf {y^s}_i, \\mathbf {u}_i, \\mathbf {l}_i)\\rbrace _{i=0}^{I}$ by decoding all the audio pieces in $\\mathcal {A}$ and transform the resulting path by the operation $S(\\cdot )$. After that, we train a seq2seq model, as shown in Figure FIGREF25, which takes $ \\mathbf {y^s}_i$ as input and decodes $\\mathbf {u}_i, \\mathbf {l}_i$ as outputs. With the seq2seq model, a noisy MT dataset $\\mathcal {M}^{\\prime }=\\lbrace (\\mathbf {\\pi }_l, \\mathbf {y^t}_l)\\rbrace _{l=0}^{L}$ is obtained by converting every source sentence $\\mathbf {y^s}_i \\in \\mathcal {M}$ to $\\mathbf {\\pi _i}$, where $\\mathbf {\\pi }_i = S^{-1}(\\mathbf {u}_i, \\mathbf {l}_i)$. We did not use the standard seq2seq model which takes $\\mathbf {y^s}$ as input and generates $\\mathbf {\\pi }$ directly, since there are too many blank tokens `-' in $\\mathbf {\\pi }$ and the model tends to generate a long sequence with only blank tokens. During MT training, we randomly sample text pairs from $\\mathcal {M}^{\\prime }$ and $\\mathcal {M}$ according to a hyper-parameter $k$. After tuning on the validation set, about $30\\%$ pairs are sampled from $\\mathcal {M}^{\\prime }$. In this way, the $enc_t$ is more robust toward the longer inputs given by the $enc_s$.\nExperiments\nWe conduct experiments on the IWSLT18 speech translation task BIBREF1. Since IWSLT participators use different data pre-processing methods, we reproduce several competitive baselines based on the ESPnet BIBREF12 for a fair comparison.\nExperiments ::: Dataset ::: Speech translation data:\nThe organizer provides a speech translation corpus extracting from the TED talk (ST-TED), which consists of raw English wave files, English transcriptions, and aligned German translations. The corpus contains 272 hours of English speech with 171k segments. We split 2k segments from the corpus as dev set and tst2010, tst2013, tst2014, tst2015 are used as test sets.\nSpeech recognition data: Aside from ST-TED, TED-LIUM2 corpus BIBREF13 is provided as speech recognition data, which contains 207 hours of English speech and 93k transcript sentences.\nText translation data: We use transcription and translation pairs in the ST-TED corpus and WIT3 as in-domain MT data, which contains 130k and 200k sentence pairs respectively. WMT2018 is used as out-of-domain training data which consists of 41M sentence pairs.\nData preprocessing: For speech data, the utterances are segmented into multiple frames with a 25 ms window size and a 10 ms step size. Then we extract 80-channel log-Mel filter bank and 3-dimensional pitch features using Kaldi BIBREF14, resulting in 83-dimensional input features. We normalize them by the mean and the standard deviation on the whole training set. The utterances with more than 3000 frames are discarded. The transcripts in ST-TED are in true-case with punctuation while in TED-LIUM2, transcripts are in lower-case and unpunctuated. Thus, we lowercase all the sentences and remove the punctuation to keep consistent. To increase the amount of training data, we perform speed perturbation on the raw signals with speed factors 0.9 and 1.1. For the text translation data, sentences longer than 80 words or shorter than 10 words are removed. Besides, we discard pairs whose length ratio between source and target sentence is smaller than 0.5 or larger than 2.0. Word tokenization is performed using the Moses scripts and both English and German words are in lower-case.\nWe use two different sets of vocabulary for our experiments. For the subword experiments, both English and German vocabularies are generated using sentencepiece BIBREF15 with a fixed size of 5k tokens. BIBREF9 inaguma2018speech show that increasing the vocabulary size is not helpful for ST task. For the character experiments, both English and German sentences are represented in the character level.\nFor evaluation, we segment each audio with the LIUM SpkDiarization tool BIBREF16 and then perform MWER segmentation with RWTH toolkit BIBREF17. We use lowercase BLEU as evaluation metric.\nExperiments ::: Baseline Models and Implementation\nWe compare our method with following baselines.\nVanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation. Their model solves the subnet waste issue by concatenating an ST decoder to an ASR encoder-decoder model. Notably, their ST decoder can consume representations from the speech encoder as well as the ASR decoder. For a fair comparison, the speech encoder and the ASR decoder are initialized from the pre-trained ASR model. The Triangle model is fine-tuned under their multi-task manner.\nAll our baselines as well as TCEN are implemented based on ESPnet BIBREF12, the RNN size is set as $d=1024$ for all models. We use a dropout of 0.3 for embeddings and encoders, and train using Adadelta with initial learning rate of 1.0 for a maximum of 10 epochs.\nFor training of TCEN, we set $\\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.8$ in the pre-training stage, since the MT dataset is much larger than ASR dataset. For fine-tune, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$, same as the `many-to-many' baseline.\nFor testing, we select the model with the best accuracy on speech translation task on dev set. At inference time, we use a beam size of 10, and the beam scores include length normalization with a weight of 0.2.\nExperiments ::: Experimental Results\nTable TABREF29 shows the results on four test sets as well as the average performance. Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality. Besides, both pre-training and multi-task learning can improve translation quality, and the pre-training settings (2nd-4th rows) are more effective compared to multi-task settings (5th-8th rows). We observe a performance degradation in the `triangle+pretrain' baseline. Compared to our method, where the decoder receives higher-level syntactic and semantic linguistic knowledge extracted from text encoder, their ASR decoder can only provide lower word-level linguistic information. Besides, since their model lacks text encoder and the architecture of ST decoder is different from MT decoder, their model cannot utilize the large-scale MT data in all the training stages. Interestingly, we find that the char-level models outperform the subword-level models in all settings, especially in vanilla baseline. A similar phenomenon is observed by BIBREF6 berard2018end. A possible explanation is that learning the alignments between speech frames and subword units in another language is notoriously difficult. Our method can bring more gains in the subword setting since our model is good at learning the text-to-text alignment and the subword-level alignment is more helpful to the translation quality.\nExperiments ::: Discussion ::: Ablation Study\nTo better understand the contribution of each component, we perform an ablation study on subword-level experiments. The results are shown in Table TABREF37. In `-MT noise' setting, we do not add noise to source sentences for MT. In `-weight sharing' setting, we use different parameters in CTC classification layer and source embedding layer. These two experiments prove that both weight sharing and using noisy MT input benefit to the final translation quality. Performance degrades more in `-weight sharing', indicating the semantic consistency contributes more to our model. In the `-pretrain' experiment, we remove the pre-training stage and directly update the model on three tasks, leading to a dramatic decrease on BLEU score, indicating the pre-training is an indispensable step for end-to-end ST.\nExperiments ::: Discussion ::: Learning Curve\nIt is interesting to investigate why our method is superior to baselines. We find that TCEN achieves a higher final result owing to a better start-point in fine-tuning. Figure FIGREF39 provides learning curves of subword accuracy on validation set. The x-axis denotes the fine-tuning training steps. The vanilla model starts at a low accuracy, because its networks are not pre-trained on the ASR and MT data. The trends of our model and `many-to-many+pretrain' are similar, but our model outperforms it about five points in the whole fine-tuning process. It indicates that the gain comes from bridging the gap between pre-training and fine-tuning rather than a better fine-tuning process.\nExperiments ::: Discussion ::: Compared with a Cascaded System\nTable TABREF29 compares our model with end-to-end baselines. Here, we compare our model with cascaded systems. We build a cascaded system by combining the ASR model and MT model used in pre-training baseline. Word error rate (WER) of the ASR system and BLEU score of the MT system are reported in the supplementary material. In addition to a simple combination of the ASR and MT systems, we also re-segment the ASR outputs before feeding to the MT system, denoted as cascaded+re-seg. Specifically, we train a seq2seq model BIBREF19 on the MT dataset, where the source side is a no punctuation sentence and the target side is a natural sentence. After that, we use the seq2seq model to add sentence boundaries and punctuation on ASR outputs. Experimental results are shown in Table TABREF41. Our end-to-end model outperforms the simple cascaded model over 2 BLEU scores, and it achieves a comparable performance with the cascaded model combining with a sentence re-segment model.\nRelated Work\nEarly works conduct speech translation in a pipeline manner BIBREF2, BIBREF20, where the ASR output lattices are fed into an MT system to generate target sentences. HMM BIBREF21, DenseNet BIBREF22, TDNN BIBREF23 are commonly used ASR systems, while RNN with attention BIBREF19 and Transformer BIBREF10 are top choices for MT. To enhance the robustness of the NMT model towards ASR errors, BIBREF24 DBLP:conf/eacl/TsvetkovMD14 and BIBREF25 DBLP:conf/asru/ChenHHL17 propose to simulate the noise in training and inference.\nTo avoid error propagation and high latency issues, recent works propose translating the acoustic speech into text in target language without yielding the source transcription BIBREF4. Since ST data is scarce, pre-training BIBREF7, multi-task learning BIBREF4, BIBREF6, curriculum learning BIBREF26, attention-passing BIBREF27, and knowledge distillation BIBREF28, BIBREF29 strategies have been explored to utilize ASR data and MT data. Specifically, BIBREF5 DBLP:conf/interspeech/WeissCJWC17 show improvements of performance by training the ST model jointly with the ASR and the MT model. BIBREF6 berard2018end observe faster convergence and better results due to pre-training and multi-task learning on a larger dataset. BIBREF7 DBLP:conf/naacl/BansalKLLG19 show that pre-training a speech encoder on one language can improve ST quality on a different source language. All of them follow the traditional multi-task training strategies. BIBREF26 DBLP:journals/corr/abs-1802-06003 propose to use curriculum learning to improve ST performance on syntactically distant language pairs. To effectively leverage transcriptions in ST data, BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 augment the multi-task model where the target decoder receives information from the source decoder and they show improvements on low-resource speech translation. Their model just consumes ASR and ST data, in contrast, our work sufficiently utilizes the large-scale MT data to capture the rich semantic knowledge. BIBREF30 DBLP:conf/icassp/JiaJMWCCALW19 use pre-trained MT and text-to-speech (TTS) synthesis models to convert weakly supervised data into ST pairs and demonstrate that an end-to-end MT model can be trained using only synthesised data.\nConclusion\nThis paper has investigated the end-to-end method for ST. It has discussed why there is a huge gap between pre-training and fine-tuning in previous methods. To alleviate these issues, we have proposed a method, which is capable of reusing every sub-net and keeping the role of sub-net consistent between pre-training and fine-tuning. Empirical studies have demonstrated that our model significantly outperforms baselines.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "length": 4656, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "ebd4ae480fe1596841b2132e96f40eac8437c800db8ef59e"}
{"input": "What tasks are used for evaluation?", "context": "Introduction\nThe Transformer architecture BIBREF0 for deep neural networks has quickly risen to prominence in NLP through its efficiency and performance, leading to improvements in the state of the art of Neural Machine Translation BIBREF1, BIBREF2, as well as inspiring other powerful general-purpose models like BERT BIBREF3 and GPT-2 BIBREF4. At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention heads may learn to look for various relationships between tokens BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9.\nThe attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words – may improve performance and interpretability BIBREF12, BIBREF13, BIBREF14. Qualitative analysis of attention heads BIBREF0 suggests that, depending on what phenomena they capture, heads tend to favor flatter or more peaked distributions.\nRecent works have proposed sparse Transformers BIBREF10 and adaptive span Transformers BIBREF11. However, the “sparsity\" of those models only limits the attention to a contiguous span of past tokens, while in this work we propose a highly adaptive Transformer model that is capable of attending to a sparse set of words that are not necessarily contiguous. Figure FIGREF1 shows the relationship of these methods with ours.\nOur contributions are the following:\nWe introduce sparse attention into the Transformer architecture, showing that it eases interpretability and leads to slight accuracy gains.\nWe propose an adaptive version of sparse attention, where the shape of each attention head is learnable and can vary continuously and dynamically between the dense limit case of softmax and the sparse, piecewise-linear sparsemax case.\nWe make an extensive analysis of the added interpretability of these models, identifying both crisper examples of attention head behavior observed in previous work, as well as novel behaviors unraveled thanks to the sparsity and adaptivity of our proposed model.\nBackground ::: The Transformer\nIn NMT, the Transformer BIBREF0 is a sequence-to-sequence (seq2seq) model which maps an input sequence to an output sequence through hierarchical multi-head attention mechanisms, yielding a dynamic, context-dependent strategy for propagating information within and across sentences. It contrasts with previous seq2seq models, which usually rely either on costly gated recurrent operations BIBREF15, BIBREF16 or static convolutions BIBREF17.\nGiven $n$ query contexts and $m$ sequence items under consideration, attention mechanisms compute, for each query, a weighted representation of the items. The particular attention mechanism used in BIBREF0 is called scaled dot-product attention, and it is computed in the following way:\nwhere $\\mathbf {Q} \\in \\mathbb {R}^{n \\times d}$ contains representations of the queries, $\\mathbf {K}, \\mathbf {V} \\in \\mathbb {R}^{m \\times d}$ are the keys and values of the items attended over, and $d$ is the dimensionality of these representations. The $\\mathbf {\\pi }$ mapping normalizes row-wise using softmax, $\\mathbf {\\pi }(\\mathbf {Z})_{ij} = \\operatornamewithlimits{\\mathsf {softmax}}(\\mathbf {z}_i)_j$, where\nIn words, the keys are used to compute a relevance score between each item and query. Then, normalized attention weights are computed using softmax, and these are used to weight the values of each item at each query context.\nHowever, for complex tasks, different parts of a sequence may be relevant in different ways, motivating multi-head attention in Transformers. This is simply the application of Equation DISPLAY_FORM7 in parallel $H$ times, each with a different, learned linear transformation that allows specialization:\nIn the Transformer, there are three separate multi-head attention mechanisms for distinct purposes:\nEncoder self-attention: builds rich, layered representations of each input word, by attending on the entire input sentence.\nContext attention: selects a representative weighted average of the encodings of the input words, at each time step of the decoder.\nDecoder self-attention: attends over the partial output sentence fragment produced so far.\nTogether, these mechanisms enable the contextualized flow of information between the input sentence and the sequential decoder.\nBackground ::: Sparse Attention\nThe softmax mapping (Equation DISPLAY_FORM8) is elementwise proportional to $\\exp $, therefore it can never assign a weight of exactly zero. Thus, unnecessary items are still taken into consideration to some extent. Since its output sums to one, this invariably means less weight is assigned to the relevant items, potentially harming performance and interpretability BIBREF18. This has motivated a line of research on learning networks with sparse mappings BIBREF19, BIBREF20, BIBREF21, BIBREF22. We focus on a recently-introduced flexible family of transformations, $\\alpha $-entmax BIBREF23, BIBREF14, defined as:\nwhere $\\triangle ^d \\lbrace \\mathbf {p}\\in \\mathbb {R}^d:\\sum _{i} p_i = 1\\rbrace $ is the probability simplex, and, for $\\alpha \\ge 1$, $\\mathsf {H}^{\\textsc {T}}_\\alpha $ is the Tsallis continuous family of entropies BIBREF24:\nThis family contains the well-known Shannon and Gini entropies, corresponding to the cases $\\alpha =1$ and $\\alpha =2$, respectively.\nEquation DISPLAY_FORM14 involves a convex optimization subproblem. Using the definition of $\\mathsf {H}^{\\textsc {T}}_\\alpha $, the optimality conditions may be used to derive the following form for the solution (Appendix SECREF83):\nwhere $[\\cdot ]_+$ is the positive part (ReLU) function, $\\mathbf {1}$ denotes the vector of all ones, and $\\tau $ – which acts like a threshold – is the Lagrange multiplier corresponding to the $\\sum _i p_i=1$ constraint.\nBackground ::: Sparse Attention ::: Properties of @!START@$\\alpha $@!END@-entmax.\nThe appeal of $\\alpha $-entmax for attention rests on the following properties. For $\\alpha =1$ (i.e., when $\\mathsf {H}^{\\textsc {T}}_\\alpha $ becomes the Shannon entropy), it exactly recovers the softmax mapping (We provide a short derivation in Appendix SECREF89.). For all $\\alpha >1$ it permits sparse solutions, in stark contrast to softmax. In particular, for $\\alpha =2$, it recovers the sparsemax mapping BIBREF19, which is piecewise linear. In-between, as $\\alpha $ increases, the mapping continuously gets sparser as its curvature changes.\nTo compute the value of $\\alpha $-entmax, one must find the threshold $\\tau $ such that the r.h.s. in Equation DISPLAY_FORM16 sums to one. BIBREF23 propose a general bisection algorithm. BIBREF14 introduce a faster, exact algorithm for $\\alpha =1.5$, and enable using $\\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}$ with fixed $\\alpha $ within a neural network by showing that the $\\alpha $-entmax Jacobian w.r.t. $\\mathbf {z}$ for $\\mathbf {p}^\\star = \\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}(\\mathbf {z})$ is\nOur work furthers the study of $\\alpha $-entmax by providing a derivation of the Jacobian w.r.t. the hyper-parameter $\\alpha $ (Section SECREF3), thereby allowing the shape and sparsity of the mapping to be learned automatically. This is particularly appealing in the context of multi-head attention mechanisms, where we shall show in Section SECREF35 that different heads tend to learn different sparsity behaviors.\nAdaptively Sparse Transformers with @!START@$\\alpha $@!END@-entmax\nWe now propose a novel Transformer architecture wherein we simply replace softmax with $\\alpha $-entmax in the attention heads. Concretely, we replace the row normalization $\\mathbf {\\pi }$ in Equation DISPLAY_FORM7 by\nThis change leads to sparse attention weights, as long as $\\alpha >1$; in particular, $\\alpha =1.5$ is a sensible starting point BIBREF14.\nAdaptively Sparse Transformers with @!START@$\\alpha $@!END@-entmax ::: Different @!START@$\\alpha $@!END@ per head.\nUnlike LSTM-based seq2seq models, where $\\alpha $ can be more easily tuned by grid search, in a Transformer, there are many attention heads in multiple layers. Crucial to the power of such models, the different heads capture different linguistic phenomena, some of them isolating important words, others spreading out attention across phrases BIBREF0. This motivates using different, adaptive $\\alpha $ values for each attention head, such that some heads may learn to be sparser, and others may become closer to softmax. We propose doing so by treating the $\\alpha $ values as neural network parameters, optimized via stochastic gradients along with the other weights.\nAdaptively Sparse Transformers with @!START@$\\alpha $@!END@-entmax ::: Derivatives w.r.t. @!START@$\\alpha $@!END@.\nIn order to optimize $\\alpha $ automatically via gradient methods, we must compute the Jacobian of the entmax output w.r.t. $\\alpha $. Since entmax is defined through an optimization problem, this is non-trivial and cannot be simply handled through automatic differentiation; it falls within the domain of argmin differentiation, an active research topic in optimization BIBREF25, BIBREF26.\nOne of our key contributions is the derivation of a closed-form expression for this Jacobian. The next proposition provides such an expression, enabling entmax layers with adaptive $\\alpha $. To the best of our knowledge, ours is the first neural network module that can automatically, continuously vary in shape away from softmax and toward sparse mappings like sparsemax.\nProposition 1 Let $\\mathbf {p}^\\star \\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}(\\mathbf {z})$ be the solution of Equation DISPLAY_FORM14. Denote the distribution $\\tilde{p}_i {(p_i^\\star )^{2 - \\alpha }}{ \\sum _j(p_j^\\star )^{2-\\alpha }}$ and let $h_i -p^\\star _i \\log p^\\star _i$. The $i$th component of the Jacobian $\\mathbf {g} \\frac{\\partial \\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}(\\mathbf {z})}{\\partial \\alpha }$ is\nproof uses implicit function differentiation and is given in Appendix SECREF10.\nProposition UNKREF22 provides the remaining missing piece needed for training adaptively sparse Transformers. In the following section, we evaluate this strategy on neural machine translation, and analyze the behavior of the learned attention heads.\nExperiments\nWe apply our adaptively sparse Transformers on four machine translation tasks. For comparison, a natural baseline is the standard Transformer architecture using the softmax transform in its multi-head attention mechanisms. We consider two other model variants in our experiments that make use of different normalizing transformations:\n1.5-entmax: a Transformer with sparse entmax attention with fixed $\\alpha =1.5$ for all heads. This is a novel model, since 1.5-entmax had only been proposed for RNN-based NMT models BIBREF14, but never in Transformers, where attention modules are not just one single component of the seq2seq model but rather an integral part of all of the model components.\n$\\alpha $-entmax: an adaptive Transformer with sparse entmax attention with a different, learned $\\alpha _{i,j}^t$ for each head.\nThe adaptive model has an additional scalar parameter per attention head per layer for each of the three attention mechanisms (encoder self-attention, context attention, and decoder self-attention), i.e.,\nand we set $\\alpha _{i,j}^t = 1 + \\operatornamewithlimits{\\mathsf {sigmoid}}(a_{i,j}^t) \\in ]1, 2[$. All or some of the $\\alpha $ values can be tied if desired, but we keep them independent for analysis purposes.\nExperiments ::: Datasets.\nOur models were trained on 4 machine translation datasets of different training sizes:\n[itemsep=.5ex,leftmargin=2ex]\nIWSLT 2017 German $\\rightarrow $ English BIBREF27: 200K sentence pairs.\nKFTT Japanese $\\rightarrow $ English BIBREF28: 300K sentence pairs.\nWMT 2016 Romanian $\\rightarrow $ English BIBREF29: 600K sentence pairs.\nWMT 2014 English $\\rightarrow $ German BIBREF30: 4.5M sentence pairs.\nAll of these datasets were preprocessed with byte-pair encoding BIBREF31, using joint segmentations of 32k merge operations.\nExperiments ::: Training.\nWe follow the dimensions of the Transformer-Base model of BIBREF0: The number of layers is $L=6$ and number of heads is $H=8$ in the encoder self-attention, the context attention, and the decoder self-attention. We use a mini-batch size of 8192 tokens and warm up the learning rate linearly until 20k steps, after which it decays according to an inverse square root schedule. All models were trained until convergence of validation accuracy, and evaluation was done at each 10k steps for ro$\\rightarrow $en and en$\\rightarrow $de and at each 5k steps for de$\\rightarrow $en and ja$\\rightarrow $en. The end-to-end computational overhead of our methods, when compared to standard softmax, is relatively small; in training tokens per second, the models using $\\alpha $-entmax and $1.5$-entmax are, respectively, $75\\%$ and $90\\%$ the speed of the softmax model.\nExperiments ::: Results.\nWe report test set tokenized BLEU BIBREF32 results in Table TABREF27. We can see that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better potential for analysis. In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads.\nAnalysis\nWe conduct an analysis for the higher-resource dataset WMT 2014 English $\\rightarrow $ German of the attention in the sparse adaptive Transformer model ($\\alpha $-entmax) at multiple levels: we analyze high-level statistics as well as individual head behavior. Moreover, we make a qualitative analysis of the interpretability capabilities of our models.\nAnalysis ::: High-Level Statistics ::: What kind of @!START@$\\alpha $@!END@ values are learned?\nFigure FIGREF37 shows the learning trajectories of the $\\alpha $ parameters of a selected subset of heads. We generally observe a tendency for the randomly-initialized $\\alpha $ parameters to decrease initially, suggesting that softmax-like behavior may be preferable while the model is still very uncertain. After around one thousand steps, some heads change direction and become sparser, perhaps as they become more confident and specialized. This shows that the initialization of $\\alpha $ does not predetermine its sparsity level or the role the head will have throughout. In particular, head 8 in the encoder self-attention layer 2 first drops to around $\\alpha =1.3$ before becoming one of the sparsest heads, with $\\alpha \\approx 2$.\nThe overall distribution of $\\alpha $ values at convergence can be seen in Figure FIGREF38. We can observe that the encoder self-attention blocks learn to concentrate the $\\alpha $ values in two modes: a very sparse one around $\\alpha \\rightarrow 2$, and a dense one between softmax and 1.5-entmax . However, the decoder self and context attention only learn to distribute these parameters in a single mode. We show next that this is reflected in the average density of attention weight vectors as well.\nAnalysis ::: High-Level Statistics ::: Attention weight density when translating.\nFor any $\\alpha >1$, it would still be possible for the weight matrices in Equation DISPLAY_FORM9 to learn re-scalings so as to make attention sparser or denser. To visualize the impact of adaptive $\\alpha $ values, we compare the empirical attention weight density (the average number of tokens receiving non-zero attention) within each module, against sparse Transformers with fixed $\\alpha =1.5$.\nFigure FIGREF40 shows that, with fixed $\\alpha =1.5$, heads tend to be sparse and similarly-distributed in all three attention modules. With learned $\\alpha $, there are two notable changes: (i) a prominent mode corresponding to fully dense probabilities, showing that our models learn to combine sparse and dense attention, and (ii) a distinction between the encoder self-attention – whose background distribution tends toward extreme sparsity – and the other two modules, who exhibit more uniform background distributions. This suggests that perhaps entirely sparse Transformers are suboptimal.\nThe fact that the decoder seems to prefer denser attention distributions might be attributed to it being auto-regressive, only having access to past tokens and not the full sentence. We speculate that it might lose too much information if it assigned weights of zero to too many tokens in the self-attention, since there are fewer tokens to attend to in the first place.\nTeasing this down into separate layers, Figure FIGREF41 shows the average (sorted) density of each head for each layer. We observe that $\\alpha $-entmax is able to learn different sparsity patterns at each layer, leading to more variance in individual head behavior, to clearly-identified dense and sparse heads, and overall to different tendencies compared to the fixed case of $\\alpha =1.5$.\nAnalysis ::: High-Level Statistics ::: Head diversity.\nTo measure the overall disagreement between attention heads, as a measure of head diversity, we use the following generalization of the Jensen-Shannon divergence:\nwhere $\\mathbf {p}_j$ is the vector of attention weights assigned by head $j$ to each word in the sequence, and $\\mathsf {H}^\\textsc {S}$ is the Shannon entropy, base-adjusted based on the dimension of $\\mathbf {p}$ such that $JS \\le 1$. We average this measure over the entire validation set. The higher this metric is, the more the heads are taking different roles in the model.\nFigure FIGREF44 shows that both sparse Transformer variants show more diversity than the traditional softmax one. Interestingly, diversity seems to peak in the middle layers of the encoder self-attention and context attention, while this is not the case for the decoder self-attention.\nThe statistics shown in this section can be found for the other language pairs in Appendix SECREF8.\nAnalysis ::: Identifying Head Specializations\nPrevious work pointed out some specific roles played by different heads in the softmax Transformer model BIBREF33, BIBREF5, BIBREF9. Identifying the specialization of a head can be done by observing the type of tokens or sequences that the head often assigns most of its attention weight; this is facilitated by sparsity.\nAnalysis ::: Identifying Head Specializations ::: Positional heads.\nOne particular type of head, as noted by BIBREF9, is the positional head. These heads tend to focus their attention on either the previous or next token in the sequence, thus obtaining representations of the neighborhood of the current time step. In Figure FIGREF47, we show attention plots for such heads, found for each of the studied models. The sparsity of our models allows these heads to be more confident in their representations, by assigning the whole probability distribution to a single token in the sequence. Concretely, we may measure a positional head's confidence as the average attention weight assigned to the previous token. The softmax model has three heads for position $-1$, with median confidence $93.5\\%$. The $1.5$-entmax model also has three heads for this position, with median confidence $94.4\\%$. The adaptive model has four heads, with median confidences $95.9\\%$, the lowest-confidence head being dense with $\\alpha =1.18$, while the highest-confidence head being sparse ($\\alpha =1.91$).\nFor position $+1$, the models each dedicate one head, with confidence around $95\\%$, slightly higher for entmax. The adaptive model sets $\\alpha =1.96$ for this head.\nAnalysis ::: Identifying Head Specializations ::: BPE-merging head.\nDue to the sparsity of our models, we are able to identify other head specializations, easily identifying which heads should be further analysed. In Figure FIGREF51 we show one such head where the $\\alpha $ value is particularly high (in the encoder, layer 1, head 4 depicted in Figure FIGREF37). We found that this head most often looks at the current time step with high confidence, making it a positional head with offset 0. However, this head often spreads weight sparsely over 2-3 neighboring tokens, when the tokens are part of the same BPE cluster or hyphenated words. As this head is in the first layer, it provides a useful service to the higher layers by combining information evenly within some BPE clusters.\nFor each BPE cluster or cluster of hyphenated words, we computed a score between 0 and 1 that corresponds to the maximum attention mass assigned by any token to the rest of the tokens inside the cluster in order to quantify the BPE-merging capabilities of these heads. There are not any attention heads in the softmax model that are able to obtain a score over $80\\%$, while for $1.5$-entmax and $\\alpha $-entmax there are two heads in each ($83.3\\%$ and $85.6\\%$ for $1.5$-entmax and $88.5\\%$ and $89.8\\%$ for $\\alpha $-entmax).\nAnalysis ::: Identifying Head Specializations ::: Interrogation head.\nOn the other hand, in Figure FIGREF52 we show a head for which our adaptively sparse model chose an $\\alpha $ close to 1, making it closer to softmax (also shown in encoder, layer 1, head 3 depicted in Figure FIGREF37). We observe that this head assigns a high probability to question marks at the end of the sentence in time steps where the current token is interrogative, thus making it an interrogation-detecting head. We also observe this type of heads in the other models, which we also depict in Figure FIGREF52. The average attention weight placed on the question mark when the current token is an interrogative word is $98.5\\%$ for softmax, $97.0\\%$ for $1.5$-entmax, and $99.5\\%$ for $\\alpha $-entmax.\nFurthermore, we can examine sentences where some tendentially sparse heads become less so, thus identifying sources of ambiguity where the head is less confident in its prediction. An example is shown in Figure FIGREF55 where sparsity in the same head differs for sentences of similar length.\nRelated Work ::: Sparse attention.\nPrior work has developed sparse attention mechanisms, including applications to NMT BIBREF19, BIBREF12, BIBREF20, BIBREF22, BIBREF34. BIBREF14 introduced the entmax function this work builds upon. In their work, there is a single attention mechanism which is controlled by a fixed $\\alpha $. In contrast, this is the first work to allow such attention mappings to dynamically adapt their curvature and sparsity, by automatically adjusting the continuous $\\alpha $ parameter. We also provide the first results using sparse attention in a Transformer model.\nRelated Work ::: Fixed sparsity patterns.\nRecent research improves the scalability of Transformer-like networks through static, fixed sparsity patterns BIBREF10, BIBREF35. Our adaptively-sparse Transformer can dynamically select a sparsity pattern that finds relevant words regardless of their position (e.g., Figure FIGREF52). Moreover, the two strategies could be combined. In a concurrent line of research, BIBREF11 propose an adaptive attention span for Transformer language models. While their work has each head learn a different contiguous span of context tokens to attend to, our work finds different sparsity patterns in the same span. Interestingly, some of their findings mirror ours – we found that attention heads in the last layers tend to be denser on average when compared to the ones in the first layers, while their work has found that lower layers tend to have a shorter attention span compared to higher layers.\nRelated Work ::: Transformer interpretability.\nThe original Transformer paper BIBREF0 shows attention visualizations, from which some speculation can be made of the roles the several attention heads have. BIBREF7 study the syntactic abilities of the Transformer self-attention, while BIBREF6 extract dependency relations from the attention weights. BIBREF8 find that the self-attentions in BERT BIBREF3 follow a sequence of processes that resembles a classical NLP pipeline. Regarding redundancy of heads, BIBREF9 develop a method that is able to prune heads of the multi-head attention module and make an empirical study of the role that each head has in self-attention (positional, syntactic and rare words). BIBREF36 also aim to reduce head redundancy by adding a regularization term to the loss that maximizes head disagreement and obtain improved results. While not considering Transformer attentions, BIBREF18 show that traditional attention mechanisms do not necessarily improve interpretability since softmax attention is vulnerable to an adversarial attack leading to wildly different model predictions for the same attention weights. Sparse attention may mitigate these issues; however, our work focuses mostly on a more mechanical aspect of interpretation by analyzing head behavior, rather than on explanations for predictions.\nConclusion and Future Work\nWe contribute a novel strategy for adaptively sparse attention, and, in particular, for adaptively sparse Transformers. We present the first empirical analysis of Transformers with sparse attention mappings (i.e., entmax), showing potential in both translation accuracy as well as in model interpretability.\nIn particular, we analyzed how the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence. Our adaptivity strategy relies only on gradient-based optimization, side-stepping costly per-head hyper-parameter searches. Further speed-ups are possible by leveraging more parallelism in the bisection algorithm for computing $\\alpha $-entmax.\nFinally, some of the automatically-learned behaviors of our adaptively sparse Transformers – for instance, the near-deterministic positional heads or the subword joining head – may provide new ideas for designing static variations of the Transformer.\nAcknowledgments\nThis work was supported by the European Research Council (ERC StG DeepSPIN 758969), and by the Fundação para a Ciência e Tecnologia through contracts UID/EEA/50008/2019 and CMUPERI/TIC/0046/2014 (GoLocal). We are grateful to Ben Peters for the $\\alpha $-entmax code and Erick Fonseca, Marcos Treviso, Pedro Martins, and Tsvetomila Mihaylova for insightful group discussion. We thank Mathieu Blondel for the idea to learn $\\alpha $. We would also like to thank the anonymous reviewers for their helpful feedback.\nSupplementary Material\nBackground ::: Regularized Fenchel-Young prediction functions\nDefinition 1 (BIBREF23)\nLet $\\Omega \\colon \\triangle ^d \\rightarrow {\\mathbb {R}}\\cup \\lbrace \\infty \\rbrace $ be a strictly convex regularization function. We define the prediction function $\\mathbf {\\pi }_{\\Omega }$ as\nBackground ::: Characterizing the @!START@$\\alpha $@!END@-entmax mapping\nLemma 1 (BIBREF14) For any $\\mathbf {z}$, there exists a unique $\\tau ^\\star $ such that\nProof: From the definition of $\\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}$,\nwe may easily identify it with a regularized prediction function (Def. UNKREF81):\nWe first note that for all $\\mathbf {p}\\in \\triangle ^d$,\nFrom the constant invariance and scaling properties of $\\mathbf {\\pi }_{\\Omega }$ BIBREF23,\nUsing BIBREF23, noting that $g^{\\prime }(t) = t^{\\alpha - 1}$ and $(g^{\\prime })^{-1}(u) = u^{{1}{\\alpha -1}}$, yields\nSince $\\mathsf {H}^{\\textsc {T}}_\\alpha $ is strictly convex on the simplex, $\\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}$ has a unique solution $\\mathbf {p}^\\star $. Equation DISPLAY_FORM88 implicitly defines a one-to-one mapping between $\\mathbf {p}^\\star $ and $\\tau ^\\star $ as long as $\\mathbf {p}^\\star \\in \\triangle $, therefore $\\tau ^\\star $ is also unique.\nBackground ::: Connections to softmax and sparsemax\nThe Euclidean projection onto the simplex, sometimes referred to, in the context of neural attention, as sparsemax BIBREF19, is defined as\nThe solution can be characterized through the unique threshold $\\tau $ such that $\\sum _i \\operatornamewithlimits{\\mathsf {sparsemax}}(\\mathbf {z})_i = 1$ and BIBREF38\nThus, each coordinate of the sparsemax solution is a piecewise-linear function. Visibly, this expression is recovered when setting $\\alpha =2$ in the $\\alpha $-entmax expression (Equation DISPLAY_FORM85); for other values of $\\alpha $, the exponent induces curvature.\nOn the other hand, the well-known softmax is usually defined through the expression\nwhich can be shown to be the unique solution of the optimization problem\nwhere $\\mathsf {H}^\\textsc {S}(\\mathbf {p}) -\\sum _i p_i \\log p_i$ is the Shannon entropy. Indeed, setting the gradient to 0 yields the condition $\\log p_i = z_j - \\nu _i - \\tau - 1$, where $\\tau $ and $\\nu > 0$ are Lagrange multipliers for the simplex constraints $\\sum _i p_i = 1$ and $p_i \\ge 0$, respectively. Since the l.h.s. is only finite for $p_i>0$, we must have $\\nu _i=0$ for all $i$, by complementary slackness. Thus, the solution must have the form $p_i = {\\exp (z_i)}{Z}$, yielding Equation DISPLAY_FORM92.\nJacobian of @!START@$\\alpha $@!END@-entmax w.r.t. the shape parameter @!START@$\\alpha $@!END@: Proof of Proposition @!START@UID22@!END@\nRecall that the entmax transformation is defined as:\nwhere $\\alpha \\ge 1$ and $\\mathsf {H}^{\\textsc {T}}_{\\alpha }$ is the Tsallis entropy,\nand $\\mathsf {H}^\\textsc {S}(\\mathbf {p}):= -\\sum _j p_j \\log p_j$ is the Shannon entropy.\nIn this section, we derive the Jacobian of $\\operatornamewithlimits{\\mathsf {entmax }}$ with respect to the scalar parameter $\\alpha $.\nJacobian of @!START@$\\alpha $@!END@-entmax w.r.t. the shape parameter @!START@$\\alpha $@!END@: Proof of Proposition @!START@UID22@!END@ ::: General case of @!START@$\\alpha >1$@!END@\nFrom the KKT conditions associated with the optimization problem in Eq. DISPLAY_FORM85, we have that the solution $\\mathbf {p}^{\\star }$ has the following form, coordinate-wise:\nwhere $\\tau ^{\\star }$ is a scalar Lagrange multiplier that ensures that $\\mathbf {p}^{\\star }$ normalizes to 1, i.e., it is defined implicitly by the condition:\nFor general values of $\\alpha $, Eq. DISPLAY_FORM98 lacks a closed form solution. This makes the computation of the Jacobian\nnon-trivial. Fortunately, we can use the technique of implicit differentiation to obtain this Jacobian.\nThe Jacobian exists almost everywhere, and the expressions we derive expressions yield a generalized Jacobian BIBREF37 at any non-differentiable points that may occur for certain ($\\alpha $, $\\mathbf {z}$) pairs. We begin by noting that $\\frac{\\partial p_i^{\\star }}{\\partial \\alpha } = 0$ if $p_i^{\\star } = 0$, because increasing $\\alpha $ keeps sparse coordinates sparse. Therefore we need to worry only about coordinates that are in the support of $\\mathbf {p}^\\star $. We will assume hereafter that the $i$th coordinate of $\\mathbf {p}^\\star $ is non-zero. We have:\nWe can see that this Jacobian depends on $\\frac{\\partial \\tau ^{\\star }}{\\partial \\alpha }$, which we now compute using implicit differentiation.\nLet $\\mathcal {S} = \\lbrace i: p^\\star _i > 0 \\rbrace $). By differentiating both sides of Eq. DISPLAY_FORM98, re-using some of the steps in Eq. DISPLAY_FORM101, and recalling Eq. DISPLAY_FORM97, we get\nfrom which we obtain:\nFinally, plugging Eq. DISPLAY_FORM103 into Eq. DISPLAY_FORM101, we get:\nwhere we denote by\nThe distribution $\\tilde{\\mathbf {p}}(\\alpha )$ can be interpreted as a “skewed” distribution obtained from $\\mathbf {p}^{\\star }$, which appears in the Jacobian of $\\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}(\\mathbf {z})$ w.r.t. $\\mathbf {z}$ as well BIBREF14.\nJacobian of @!START@$\\alpha $@!END@-entmax w.r.t. the shape parameter @!START@$\\alpha $@!END@: Proof of Proposition @!START@UID22@!END@ ::: Solving the indetermination for @!START@$\\alpha =1$@!END@\nWe can write Eq. DISPLAY_FORM104 as\nWhen $\\alpha \\rightarrow 1^+$, we have $\\tilde{\\mathbf {p}}(\\alpha ) \\rightarrow \\mathbf {p}^{\\star }$, which leads to a $\\frac{0}{0}$ indetermination.\nTo solve this indetermination, we will need to apply L'Hôpital's rule twice. Let us first compute the derivative of $\\tilde{p}_i(\\alpha )$ with respect to $\\alpha $. We have\ntherefore\nDifferentiating the numerator and denominator in Eq. DISPLAY_FORM107, we get:\nwith\nand\nWhen $\\alpha \\rightarrow 1^+$, $B$ becomes again a $\\frac{0}{0}$ indetermination, which we can solve by applying again L'Hôpital's rule. Differentiating the numerator and denominator in Eq. DISPLAY_FORM112:\nFinally, summing Eq. DISPLAY_FORM111 and Eq. DISPLAY_FORM113, we get\nJacobian of @!START@$\\alpha $@!END@-entmax w.r.t. the shape parameter @!START@$\\alpha $@!END@: Proof of Proposition @!START@UID22@!END@ ::: Summary\nTo sum up, we have the following expression for the Jacobian of $\\mathop {\\mathsf {\\alpha }\\textnormal {-}\\mathsf {entmax }}$ with respect to $\\alpha $:", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "length": 4898, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "11be2f14f540e957e9797cc962203b8186ca10561228f81f"}
{"input": "What can word subspace represent?", "context": "Introduction\nText classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information. Over the past three decades, many methods based on machine learning and statistical models have been applied to perform this task, such as latent semantic analysis (LSA), support vector machines (SVM), and multinomial naive Bayes (MNB).\nThe first step in utilizing such methods to categorize textual data is to convert the texts into a vector representation. One of the most popular text representation models is the bag-of-words model BIBREF0 , which represents each document in a collection as a vector in a vector space. Each dimension of the vectors represents a term (e.g., a word, a sequence of words), and its value encodes a weight, which can be how many times the term occurs in the document.\nDespite showing positive results in tasks such as language modeling and classification BIBREF1 , BIBREF2 , BIBREF3 , the BOW representation has limitations: first, feature vectors are commonly very high-dimensional, resulting in sparse document representations, which are hard to model due to space and time complexity. Second, BOW does not consider the proximity of words and their position in the text and consequently cannot encode the words semantic meanings.\nTo solve these problems, neural networks have been employed to learn vector representations of words BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . In particular, the word2vec representation BIBREF8 has gained attention. Given a training corpus, word2vec can generate a vector for each word in the corpus that encodes its semantic information. These word vectors are distributed in such a way that words from similar contexts are represented by word vectors with high correlation, while words from different contexts are represented by word vectors with low correlation.\nOne crucial aspect of the word2vec representation is that arithmetic and distance calculation between two word vectors can be performed, giving information about their semantic relationship. However, rather than looking at pairs of word vectors, we are interested in studying the relationship between sets of vectors as a whole and, therefore, it is desirable to have a text representation based on a set of these word vectors.\nTo tackle this problem, we introduce the novel concept of word subspace. It is mathematically defined as a low dimensional linear subspace in a word vector space with high dimensionality. Given that words from texts of the same class belong to the same context, it is possible to model word vectors of each class as word subspaces and efficiently compare them in terms of similarity by using canonical angles between the word subspaces. Through this representation, most of the variability of the class is retained. Consequently, a word subspace can effectively and compactly represent the context of the corresponding text. We achieve this framework through the mutual subspace method (MSM) BIBREF9 .\nThe word subspace of each text class is modeled by applying PCA without data centering to the set of word vectors of the class. When modeling the word subspaces, we assume only one occurrence of each word inside the class.\nHowever, as seen in the BOW approach, the frequency of words inside a text is an informative feature that should be considered. In order to introduce this feature in the word subspace modeling and enhance its performance, we further extend the concept of word subspace to the term-frequency (TF) weighted word subspace.\nIn this extension, we consider a set of weights, which encodes the words frequencies, when performing the PCA. Text classification with TF weighted word subspace can also be performed under the framework of MSM. We show the validity of our modeling through experiments on the Reuters database, an established database for natural language processing tasks. We demonstrate the effectiveness of the word subspace formulation and its extension, comparing our methods' performance to various state-of-art methods.\nThe main contributions of our work are:\nThe remainder of this paper is organized as follows. In Section \"Related Work\" , we describe the main works related to text classification. In Section \"Word subspace\" , we present the formulation of our proposed word subspace. In Section \"Conventional text classification methods\" , we explain how text classification with word subspaces is performed under the MSM framework. Then, we present the TF weighted word subspace extension in Section \"TF weighted word subspace\" . Evaluation experiments and their results are described in Section \"Experimental Evaluation\" . Further discussion is then presented in Section \"Discussion\" , and our conclusions are described in Section \"Conclusions and Future Work\" .\nRelated Work\nIn this section, we outline relevant work towards text classification. We start by describing how text data is conventionally represented using the bag-of-words model and then follow to describe the conventional methods utilized in text classification.\nText Representation with bag-of-words\nThe bag-of-words representation comes from the hypothesis that frequencies of words in a document can indicate the relevance of the document to a query BIBREF0 , that is, if documents and a query have similar frequencies for the same words, they might have a similar meaning. This representation is based on the vector space model (VSM), that was developed for the SMART information retrieval system BIBREF10 . In the VSM, the main idea is that documents in a collection can be represented as a vector in a vector space, where vectors close to each other represent semantically similar documents.\nMore formally, a document $d$ can be represented by a vector in $\\mathbb {R}^{n}$ , where each dimension represents a different term. A term can be a single word, constituting the conventional bag-of-words, or combinations of $N$ words, constituting the bag-of-N-grams. If a term occurs in the document, its position in the vector will have a non-zero value, also known as term weight. Two documents in the VSM can be compared to each other by taking the cosine distance between them BIBREF1 .\nThere are several ways to compute the term weights. Among them, we can highlight some: Binary weights, term-frequency (TF) weights, and term-frequency inverse document-frequency (TF-IDF) weights.\nConsider a corpus with documents $D = \\lbrace d_i\\rbrace _{i=1}^{|D|}$ and a vocabulary with all terms in the corpus $V = \\lbrace w_i\\rbrace _{i=1}^{|V|}$ . The term weights can be defined as:\nBinary weight: If a term occurs in the document, its weight is 1. Otherwise, it is zero.\nTerm-frequency weight (TF): The weight of a term $w$ is defined by the number of times it occurs in the document $d$ .\n$$TF(w,d) = n_d^w$$   (Eq. 8)\nInverse document-frequency: The weight of a term $w$ , given the corpus $D$ , is defined as the total number of documents $|D|$ divided by the number of documents that have the term $w$ , $|D^w|$ .\n$$IDF(w | D) = \\frac{|D|}{|D^w|}$$   (Eq. 10)\nTerm-frequency inverse document-frequency (TF-IDF): The weight of a term $w$ is defined by the multiplication of its term-frequency and its inverse document-frequency. When considering only the TF weights, all terms have the same importance among the corpus. By using the IDF weight, words that are more common across all documents in $D$ receive a smaller weight, giving more importance to rare terms in the corpus.\n$$TFIDF(w,d | D)=TF \\times IDF$$   (Eq. 12)\nIn very large corpus, it is common to consider the logarithm of the IDF in order to dampen its effect.\n$$TFIDF(w,d | D)=TF \\times log_{10}(IDF)$$   (Eq. 13)\nConventional text classification methods\nMulti-variate Bernoulli (MVB) and multinomial naive Bayes (MNB) are two generative models based on the naive Bayes assumption. In other words, they assume that all attributes (e.g., the frequency of each word, the presence or absence of a word) of each text are independent of each other given the context of the class BIBREF11 .\nIn the MVB model, a document is represented by a vector generated by a bag-of-words with binary weights. In this case, a document can be considered an event, and the presence or the absence of the words to be the attributes of the event. On the other hand, the MNB model represents each document as a vector generated by a bag-of-words with TF weights. Here, the individual word occurrences are considered as events and the document is a collection of word events.\nBoth these models use the Bayes rule to classify a document. Consider that each document should be classified into one of the classes in $C=\\lbrace c_j\\rbrace _{j=1}^{|C|}$ . The probability of each class given the document is defined as:\n$$P(c_j|d_i) = \\frac{P(d_i|c_j)P(c_j)}{P(d_i)}.$$   (Eq. 16)\nThe prior $P(d_i)$ is the same for all classes, so to determine the class to which $d_i$ belongs to, the following equation can be used:\n$$prediction(d_i) = argmax_{c_j}P(d_i|c_j)P(c_j)$$   (Eq. 17)\nThe prior $P(c_j)$ can be obtained by the following equation:\n$$P(c_j) = \\frac{1+|D_j|}{|C|+|D|},$$   (Eq. 18)\nwhere $|D_j|$ is the number of documents in class $c_j$ .\nAs for the posterior $P(d_i|c_j)$ , different calculations are performed for each model. For MVB, it is defined as:\n$$P(d_i|c_j) = \\prod _{k=1}^{|V|}P(w_k|c_j)^{t_i^k}(1-P(w_k|c_j))^{1-t_i^k},$$   (Eq. 19)\nwhere $w_k$ is the k-th word in the vocabulary $V$ , and $t_i^k$ is the value (0 or 1) of the k-th element of the vector of document $d_i$ .\nFor the MNB, it is defined as:\n$$P(d_i|c_j) = P(|d_i|)|d_i|!\\prod _{k=1}^{|V|}\\frac{P(w_k|c_j)^{n_i^k}}{n_i^k!},$$   (Eq. 20)\nwhere $|d_i|$ is the number of words in document $d_i$ and $n_i^k$ is the k-th element of the vector of document $d_i$ and it represents how many times word $w_k$ occurs in $d_i$ .\nFinally, the posterior $P(w_k|c_j)$ can be obtained by the following equation:\n$$P(w_k|c_j) = \\frac{1+|D_j^k|}{|C|+|D|},$$   (Eq. 21)\nwhere $|D_j^k|$ is the number of documents in class $c_j$ that contain the word $w_k$ .\nIn general, MVB tends to perform better than MNB at small vocabulary sizes whereas MNB is more efficient on large vocabularies.\nDespite being robust tools for text classification, both these models depend directly on the bag-of-words features and do not naturally work with representations such as word2vec.\nLatent semantic analysis (LSA), or latent semantic indexing (LSI), was proposed in BIBREF12 , and it extends the vector space model by using singular value decomposition (SVD) to find a set of underlying latent variables which spans the meaning of texts.\nIt is built from a term-document matrix, in which each row represents a term, and each column represents a document. This matrix can be built by concatenating the vectors of all documents in a corpus, obtained using the bag-of-words model, that is, $ {X} = [ {v}_1, {v}_2, ..., {v}_{|D|}]$ , where ${v}_i$ is the vector representation obtained using the bag-of-words model.\nIn this method, the term-document matrix is decomposed using the singular value decomposition,\n$${X} = {U\\Sigma V}^\\top ,$$   (Eq. 23)\nwhere $U$ and $V$ are orthogonal matrices and correspond to the left singular vectors and right singular vectors of $X$ , respectively. $\\Sigma $ is a diagonal matrix, and it contains the square roots of the eigenvalues of $X^TX$ and $XX^T$ . LSA finds a low-rank approximation of $X$ by selecting only the $k$ largest singular values and its respective singular vectors,\n$${X}_k = {U}_k{\\Sigma }_k {V}_k^{\\top }.$$   (Eq. 24)\nTo compare two documents, we project both of them into this lower dimension space and calculate the cosine distance between them. The projection ${\\hat{d}}$ of document ${d}$ is obtained by the following equation:\n$${\\hat{d}} = {\\Sigma }_k^{-1} {U}_k^\\top {d}.$$   (Eq. 25)\nDespite its extensive application on text classification BIBREF13 , BIBREF14 , BIBREF15 , this method was initially proposed for document indexing and, therefore, does not encode any class information when modeling the low-rank approximation. To perform classification, 1-nearest neighbor is usually performed, placing a query document into the class of the nearest training document.\nThe support vector machine (SVM) was first presented in BIBREF16 and performs the separation between samples of two different classes by projecting them onto a higher dimensionality space. It was first applied in text classification by BIBREF17 and have since been successfully applied in many tasks related to natural language processing BIBREF18 , BIBREF19 .\nConsider a training data set $D$ , with $n$ samples\n$$D = \\lbrace ({x}_i,c_i)|{x}_i\\in \\mathbb {R}^p, c_i \\in \\lbrace -1,1\\rbrace  \\rbrace _{i=1}^{n},$$   (Eq. 27)\nwhere $c_i$ represents the class to which ${x}_i$ belongs to. Each ${x}_i$ is a $p$ -dimensional vector. The goal is to find the hyperplane that divides the points from $c_i = 1$ from the points from $c_i = -1$ . This hyperplane can be written as a set of points $x$ satisfying:\n$${w} \\cdot {x} - b = 0,$$   (Eq. 28)\nwhere $\\cdot $ denotes the dot product. The vector ${w}$ is perpendicular to the hyperplane. The parameter $\\frac{b}{\\Vert {w}\\Vert }$ determines the offset of the hyperplane from the origin along the normal vector ${w}$ .\nWe wish to choose ${w}$ and $b$ , so they maximize the distance between the parallel hyperplanes that are as far apart as possible, while still separating the data.\nIf the training data is linearly separable, we can select two hyperplanes in a way that there are no points between them and then try to maximize the distance. In other words, minimize $\\Vert {w}\\Vert $ subject to $c_i({w}\\cdot {x}_u-b) \\ge 1, i=\\lbrace 1,2,...,n\\rbrace $ . If the training data is not linearly separable, the kernel trick can be applied, where every dot product is replaced by a non-linear kernel function.\nWord subspace\nAll methods mentioned above utilize the BOW features to represent a document. Although this representation is simple and powerful, its main problem lies on disregarding the word semantics within a document, where the context and meaning could offer many benefits to the model such as identification of synonyms.\nIn our formulation, words are represented as vectors in a real-valued feature vector space $\\mathbb {R}^{p}$ , by using word2vec BIBREF8 . Through this representation, it is possible to calculate the distance between two words, where words from similar contexts are represented by vectors close to each other, while words from different contexts are represented as far apart vectors. Also, this representation brings the new concept of arithmetic operations between words, where operations such as addition and subtraction carry meaning (eg., “king”-“man”+“woman”=“queen”) BIBREF20 .\nConsider a set of documents which belong to the same context $D_c = \\lbrace d_i\\rbrace _{i=1}^{|D_c|}$ . Each document $d_i$ is represented by a set of $N_i$ words, $d_i = \\lbrace w_k\\rbrace _{k=1}^{N_i}$ . By considering that all words from documents of the same context belong to the same distribution, a set of words $W_c = \\lbrace w_k\\rbrace _{k=1}^{N_c}$ with the words in the context $c$ is obtained.\nWe then translate these words into word vectors using word2vec, resulting in a set of word vectors $X_c = \\lbrace {x}^k_c\\rbrace _{k=1}^{N_c} \\in \\mathbb {R}^p$ . This set of word vectors is modeled into a word subspace, which is a compact, scalable and meaningful representation of the whole set. Such a word subspace is generated by applying PCA to the set of word vectors.\nFirst, we compute an autocorrelation matrix, ${R}_c$ :\n$${R}_c = \\frac{1}{N_c}\\sum _{i=1}^{N_c}{x}^{i}_c{x}_c^{i^{\\top }}.$$   (Eq. 29)\nThe orthonormal basis vectors of $m_c$ -dimensional subspace ${Y}_c$ are obtained as the eigenvectors with the $m_c$ largest eigenvalues of the matrix ${R}_c$ . We represent a subspace ${Y}_c$ by the matrix ${Y}_c \\in \\mathbb {R}^{p \\times m_c}$ , which has the corresponding orthonormal basis vectors as its column vectors.\nText classification based on word subspace\nWe formulate our problem as a single label classification problem. Given a set of training documents, which we will refer as corpus, $D = \\lbrace d_i\\rbrace _{i=1}^{|D|}$ , with known classes $C = \\lbrace c_j\\rbrace _{j=1}^{|C|}$ , we wish to classify a query document $d_q$ into one of the classes in $C$ .\nText classification based on word subspace can be performed under the framework of mutual subspace method (MSM). This task involves two different stages: A learning stage, where the word subspace for each class is modeled, and a classification stage, where the word subspace for a query is modeled and compared to the word subspaces of the classes.\nIn the learning stage, it is assumed that all documents of the same class belong to the same context, resulting in a set of words $W_c = \\lbrace w_c^k\\rbrace _{k=1}^{N_c}$ . This set assumes that each word appears only once in each class. Each set $\\lbrace W_c\\rbrace _{c=1}^{|C|}$ is then modeled into a word subspace ${Y}_c$ , as explained in Section \"Word subspace\" . As the number of words in each class may vary largely, the dimension $m_c$ of each class word subspace is not set to the same value.\nIn the classification stage, for a query document $d_q$ , it is also assumed that each word occurs only once, generating a subspace ${Y}_q$ .\nTo measure the similarity between a class word subspace ${Y}_c$ and a query word subspace ${Y}_q$ , the canonical angles between the two word subspaces are used BIBREF21 . There are several methods for calculating canonical angles BIBREF22 , BIBREF23 , and BIBREF24 , among which the simplest and most practical is the singular value decomposition (SVD). Consider, for example, two subspaces, one from the training data and another from the query, represented as matrices of bases, ${Y}_{c} = [{\\Phi }_{1} \\ldots {\\Phi }_{m_c}] \\in \\mathbb {R}^{p \\times m_c}$ and ${Y}_{q} = [{\\Psi }_{1} \\ldots {\\Psi }_{m_q}] \\in \\mathbb {R}^{p \\times m_q}$ , where ${\\Phi }_{i}$ are the bases for ${Y}_c$ and ${\\Psi }_{i}$ are the bases for ${Y}_q$ . Let the SVD of ${Y}_c^{\\top }{Y}_q \\in \\mathbb {R}^{m_c \\times m_q}$ be ${Y}_c^{\\top }{Y}_q = {U \\Sigma V}^{\\top }$ , where ${Y}_q$0 , ${Y}_q$1 represents the set of singular values. The canonical angles ${Y}_q$2 can be obtained as ${Y}_q$3 ${Y}_q$4 . The similarity between the two subspaces is measured by ${Y}_q$5 angles as follows:\n$$S_{({Y}_c,{Y}_q)}[t] = \\frac{1}{t}\\sum _{i = 1}^{t} \\cos ^{2} \\theta _{i},\\; 1 \\le t \\le m_q, \\; m_q \\le m_c.$$   (Eq. 30)\nFig. 1 shows the modeling and comparison of sets of words by MSM. This method can compare sets of different sizes, and naturally encodes proximity between sets with related words.\nFinally, the class with the highest similarity with $d_q$ is assigned as the class of $d_q$ :\n$$prediction(d_q) = argmax_c(S_{({Y}_c,{Y}_q)}).$$   (Eq. 32)\nTF weighted word subspace\nThe word subspace formulation presented in Section \"Word subspace\" is a practical and compact way to represent sets of word vectors, retaining most of the variability of features. However, as seen in the BOW features, the frequency of words is relevant information that can improve the characterization of a text. To incorporate this information into the word subspace modeling, we propose an extension of the word subspace, called the term-frequency (TF) weighted word subspace.\nLike the word subspace, the TF weighted word subspace is mathematically defined as a low-dimensional linear subspace in a word vector space with high dimensionality. However, a weighted version of the PCA BIBREF25 , BIBREF26 is utilized to incorporate the information given by the frequencies of words (term-frequencies). This TF weighted word subspace is equivalent to the word subspace if we consider all occurrences of the words.\nConsider the set of word vectors $\\lbrace {x}_c^k\\rbrace _{k=1}^{N_c} \\in \\mathbb {R}^{p}$ , which represents each word in the context $c$ , and the set of weights $\\lbrace \\omega _i\\rbrace _{i=1}^{N_c}$ , which represent the frequencies of the words in the context $c$ .\nWe incorporate these frequencies into the subspace calculation by weighting the data matrix ${X}$ as follows:\n$${\\widetilde{X}}={X}{\\Omega }^{1/2},$$   (Eq. 33)\nwhere ${X} \\in \\mathbb {R}^{p \\times N_c}$ is a matrix containing the word vectors $\\lbrace {x}_c^k\\rbrace _{k=1}^{N_c}$ and ${\\Omega }$ is a diagonal matrix containing the weights $\\lbrace \\omega _i\\rbrace _{i=1}^{N_c}$ .\nWe then perform PCA by solving the SVD of the matrix ${\\widetilde{X}}$ :\n$${\\widetilde{X}}={AMB}^{\\top },$$   (Eq. 34)\nwhere the columns of the orthogonal matrices ${A}$ and ${B}$ are, respectively, the left-singular vectors and right-singular vectors of the matrix ${\\widetilde{X}}$ , and the diagonal matrix ${M}$ contains singular values of ${\\widetilde{X}}$ .\nFinally, the orthonormal basis vectors of the $m_c$ -dimensional TF weighted subspace ${W}$ are the column vectors in ${A}$ corresponding to the $m_c$ largest singular values in ${M}$ .\nText classification with TF weighted word subspace can also be performed under the framework of MSM. In this paper, we will refer to MSM with TF weighted word subspace as TF-MSM.\nExperimental Evaluation\nIn this section we describe the experiments performed to demonstrate the validity of our proposed method and its extension. We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table 1 .\nTo obtain the vector representation of words, we used a freely available word2vec model, trained by BIBREF8 , on approximately 100 billion words, which encodes the vector representation in $\\mathbb {R}^{300}$ of over 3 million words from several different languages. Since we decided to focus on English words only, we filtered these vectors to about 800 thousand words, excluding all words with non-roman characters.\nTo show the validity of our word subspace representation for text classification and the proposed extension, we divided our experiment section into two parts: The first one aims to verify if sets of word vectors are suitable for subspace representation, and the second one puts our methods in practice in a text classification test, comparing our results with the conventional methods described in Section \"Related Work\" .\nEvaluation of the word subspace representation\nIn this experiment, we modeled the word vectors from each class in the Reuters-8 database into a word subspace. The primary goal is to visualize how much of the text data can be represented by a lower dimensional subspace.\nSubspace representations are very efficient in compactly represent data that is close to a normal distribution. This characteristic is due to the application of the PCA, that is optimal to find the direction with the highest variation within the data.\nIn PCA, the principal components give the directions of maximum variance, while their corresponding eigenvalues give the variance of the data in each of them. Therefore, by observing the distribution of the eigenvalues computed when performing PCA in the modeling of the subspaces, we can suggest if the data is suitable or not for subspace representation.\nFor each class, we normalized the eigenvalues by the largest one of the class. Fig. 2 shows the mean of the eigenvalues and the standard deviation among classes. It is possible to see that the first largest eigenvalues retain larger variance than the smallest ones. In fact, looking at the first 150 largest eigenvalues, we can see that they retain, on average, 86.37% of the data variance. Also, by observing the standard deviation, we can understand that the eigenvalues distribution among classes follows the same pattern, that is, most of the variance is in the first dimensions. This plot indicates that text data represented by vectors generated with word2vec is suitable for subspace representation.\nText classification experiment\nIn this experiment, we performed text classification among the classes in the Reuters-8 database. We compared the classification using the word subspace, and its weighted extension, based on MSM (to which we will refer as MSM and TF-MSM, respectively) with the baselines presented in Section \"Related Work\" : MVB, MNB, LSA, and SVM. Since none of the baseline methods work with vector set classification, we also compared to a simple baseline for comparing sets of vectors, defined as the average of similarities between all vector pair combinations of two given sets. For two matrices ${A}$ and ${B}$ , containing the sets of vectors $\\lbrace  {x}^{i}_a \\rbrace _{i = 1}^{N_A}$ and $\\lbrace  {x}^{i}_b \\rbrace _{i = 1}^{N_B}$ , respectively, where $N_A$ and $N_B$ are the number of main words in each set, the similarity is defined as:\n$$Sim_{(A,B)} = \\frac{1}{N_A N_B}\\sum _{i}^{N_A}\\sum _{j}^{N_B}{{x}_a^i}^{\\top }{x}_b^j.$$   (Eq. 41)\nWe refer to this baseline as similarity average (SA). For this method, we only considered one occurrence of each word in each set.\nDifferent features were used, depending on the method. Classification with SA, MSM, and TF-MSM was performed using word2vec features, to which we refer as w2v. For MVB, due to its nature, only bag-of-words features with binary weights were used (binBOW). For the same reason, we only used bag-of-words features with term-frequency weights (tfBOW) with MNB. Classification with LSA and SVM is usually performed using bag-of-words features and, therefore, we tested with binBOW, tfBOW, and with the term-frequency inverse document-frequency weight, tfidfBOW. We also tested them using word2vec vectors. In this case, we considered each word vector from all documents in each class to be a single sample.\nTo determine the dimensions of the class subspaces and query subspace of MSM and TF-MSM, and the dimension of the approximation performed by LSA, we performed a 10-fold cross validation, wherein each fold, the data were randomly divided into train (60%), validation (20%) and test set (20%).\nThe results can be seen in Table 2 . The simplest baseline, SA with w2v, achieved an accuracy rate of 78.73%. This result is important because it shows the validity of the word2vec representation, performing better than more elaborate methods based on BOW, such as MVB with binBOW.\nLSA with BOW features was almost 10% more accurate than SA, where the best results with binary weights were achieved with an approximation with 130 dimensions, with TF weights were achieved with 50 dimensions, and with TF-IDF weights were achieved with 30 dimensions. SVM with BOW features was about 3% more accurate than LSA, with binary weights leading to a higher accuracy rate.\nIt is interesting to note that despite the reasonably high accuracy rates achieved using LSA and SVM with BOW features, they poorly performed when using w2v features.\nAmong the baselines, the best method was MNB with tfBOW features, with an accuracy of 91.47%, being the only conventional method to outperform MSM. MSM with w2v had an accuracy rate of 90.62%, with the best results achieved with word subspace dimensions for the training classes ranging from 150 to 181, and for the query ranging from 3 to 217. Incorporating the frequency information in the subspace modeling resulted in higher accuracy, with TF-MSM achieving 92.01%, with dimensions of word subspaces for training classes ranging from 150 to 172, and for the query, ranging from 2 to 109. To confirm that TF-MSM is significantly more accurate than MNB, we performed a t-test to compare their results. It resulted in a p-value of 0.031, which shows that at a 95% significance level, TF-MSM has produced better results.\nDiscussion\nGiven the observation of the eigenvalues distribution of word vectors, we could see that word vectors that belong to the same context, i.e., same class, are suitable for subspace representation. Our analysis showed that half of the word vector space dimensions suffice to represent most of the variability of the data in each class of the Reuters-8 database.\nThe results from the text classification experiment showed that subspace-based methods performed better than the text classification methods discussed in this work. Ultimately, our proposed TF weighted word subspace with MSM surpassed all the other methods. word2vec features are reliable tools to represent the semantic meaning of the words and when treated as sets of word vectors, they are capable of representing the content of texts. However, despite the fact that word vectors can be treated separately, conventional methods such as SVM and LSA may not be suitable for text classification using word vectors.\nAmong the conventional methods, LSA and SVM achieved about 86% and 89%, respectively, when using bag-of-words features. Interestingly, both methods had better performance when using binary weights. For LSA, we can see that despite the slight differences in the performance, tfidfBOW required approximations with smaller dimensions. SVM had the lowest accuracy rate when using the tfidfBOW features. One possible explanation for this is that TF-IDF weights are useful when rare words and very frequent words exist in the corpus, giving higher weights for rare words and lower weights for common words. Since we removed the stop words, the most frequent words among the training documents were not considered and, therefore, using TF-IDF weights did not improve the results.\nOnly MNB with tfBOW performed better than MSM. This result may be because tfBOW features encode the word frequencies, while MSM only considers a single occurrence of words. When incorporating the word frequencies with our TF weighted word subspace, we achieved a higher accuracy of 92.01%, performing better than MNB at a significance level of 95%.\nConclusions and Future Work\nIn this paper, we proposed a new method for text classification, based on the novel concept of word subspace under the MSM framework. We also proposed the term-frequency weighted word subspace which can incorporate the frequency of words directly in the modeling of the subspace by using a weighted version of PCA.\nMost of the conventional text classification methods are based on the bag-of-words features, which are very simple to compute and had been proved to produce positive results. However, bag-of-words are commonly high dimensional models, with a sparse representation, which is computationally heavy to model. Also, bag-of-words fail to convey the semantic meaning of words inside a text. Due to these problems, neural networks started to be applied to generate a vector representation of words. Despite the fact that these representations can encode the semantic meaning of words, conventional methods do not work well when considering word vectors separately.\nIn our work, we focused on the word2vec representation, which can embed the semantic structure of words, rendering vector angles as a useful metric to show meaningful similarities between words. Our experiments showed that our word subspace modeling along with the MSM outperforms most of the conventional methods. Ultimately, our TF weighted subspace formulation resulted in significantly higher accuracy when compared to all conventional text classification methods discussed in this work. It is important to note that our method does not consider the order of the words in a text, resulting in a loss of context information. As a future work, we wish to extend our word subspace concept further in mainly two directions. First, we seek to encode word order, which may enrich the representation of context information. Second, we wish to model dynamic context change, enabling analysis of large documents, by having a long-short memory to interpret information using cues from different parts of a text.\nAcknowledgment\nThis work is supported by JSPS KAKENHI Grant Number JP16H02842 and the Japanese Ministry of Education, Culture, Sports, Science, and Technology (MEXT) scholarship.", "answers": ["Word vectors, usually in the context of others within the same class"], "length": 5151, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "26eccf323630722b689e91abd27fbbae0da4097e74865c33"}
{"input": "Which baselines did they compare against?", "context": "Introduction\nOne of the most fundamental topics in natural language processing is how best to derive high-level representations from constituent parts, as natural language meanings are a function of their constituent parts. How best to construct a sentence representation from distributed word embeddings is an example domain of this larger issue. Even though sequential neural models such as recurrent neural networks (RNN) BIBREF0 and their variants including Long Short-Term Memory (LSTM) BIBREF1 and Gated Recurrent Unit (GRU) BIBREF2 have become the de-facto standard for condensing sentence-level information from a sequence of words into a fixed vector, there have been many lines of research towards better sentence representation using other neural architectures, e.g. convolutional neural networks (CNN) BIBREF3 or self-attention based models BIBREF4 .\nFrom a linguistic point of view, the underlying tree structure—as expressed by its constituency and dependency trees—of a sentence is an integral part of its meaning. Inspired by this fact, some recursive neural network (RvNN) models are designed to reflect the syntactic tree structure, achieving impressive results on several sentence-level tasks such as sentiment analysis BIBREF5 , BIBREF6 , machine translation BIBREF7 , natural language inference BIBREF8 , and discourse relation classification BIBREF9 .\nHowever, some recent works have BIBREF10 , BIBREF11 proposed latent tree models, which learn to construct task-specific tree structures without explicit supervision, bringing into question the value of linguistically-motivated recursive neural models. Witnessing the surprising performance of the latent tree models on some sentence-level tasks, there arises a natural question: Are linguistic tree structures the optimal way of composing sentence representations for NLP tasks?\nIn this paper, we demonstrate that linguistic priors are in fact useful for devising effective neural models for sentence representations, showing that our novel architecture based on constituency trees and their tag information obtains superior performance on several sentence-level tasks, including sentiment analysis and natural language inference.\nA chief novelty of our approach is that we introduce a small separate tag-level tree-LSTM to control the composition function of the existing word-level tree-LSTM, which is in charge of extracting helpful syntactic signals for meaningful semantic composition of constituents by considering both the structures and linguistic tags of constituency trees simultaneously. In addition, we demonstrate that applying a typical LSTM to preprocess the leaf nodes of a tree-LSTM greatly improves the performance of the tree models. Moreover, we propose a clustered tag set to replace the existing tags on the assumption that the original syntactic tags are too fined-grained to be useful in neural models.\nIn short, our contributions in this work are as follows:\nRelated Work\nRecursive neural networks (RvNN) are a kind of neural architecture which model sentences by exploiting syntactic structure. While earlier RvNN models proposed utilizing diverse composition functions, including feed-forward neural networks BIBREF12 , matrix-vector multiplication BIBREF5 , and tensor computation BIBREF6 , tree-LSTMs BIBREF13 remain the standard for several sentence-level tasks.\nEven though classic RvNNs have demonstrated superior performance on a variety of tasks, their inflexibility, i.e. their inability to handle dynamic compositionality for different syntactic configurations, is a considerable weakness. For instance, it would be desirable if our model could distinguish e.g. adjective-noun composition from that of verb-noun or preposition-noun composition, as models failing to make such a distinction ignore real-world syntactic considerations such as `-arity' of function words (i.e. types), and the adjunct/argument distinction.\nTo enable dynamic compositionality in recursive neural networks, many previous works BIBREF14 , BIBREF15 , BIBREF16 , BIBREF9 , BIBREF17 , BIBREF18 , BIBREF19 have proposed various methods.\nOne main direction of research leverages tag information, which is produced as a by-product of parsing. In detail, BIBREF16 ( BIBREF16 ) suggested TG-RNN, a model employing different composition functions according to POS tags, and TE-RNN/TE-RNTN, models which leverage tag embeddings as additional inputs for the existing tree-structured models. Despite the novelty of utilizing tag information, the explosion of the number of parameters (in case of the TG-RNN) and the limited performance of the original models (in case of the TE-RNN/TE-RNTN) have prevented these models from being widely adopted. Meanwhile, BIBREF9 ( BIBREF9 ) and BIBREF18 ( BIBREF18 ) proposed models based on a tree-LSTM which also uses the tag vectors to control the gate functions of the tree-LSTM. In spite of their impressive results, there is a limitation that the trained tag embeddings are too simple to reflect the rich information which tags provide in different syntactic structures. To alleviate this problem, we introduce structure-aware tag representations in the next section.\nAnother way of building dynamic compositionality into RvNNs is to take advantage of a meta-network (or hyper-network). Inspired by recent works on dynamic parameter prediction, DC-TreeLSTMs BIBREF17 dynamically create the parameters for compositional functions in a tree-LSTM. Specifically, the model has two separate tree-LSTM networks whose architectures are similar, but the smaller of the two is utilized to calculate the weights of the bigger one. A possible problem for this model is that it may be easy to be trained such that the role of each tree-LSTM is ambiguous, as they share the same input, i.e. word information. Therefore, we design two disentangled tree-LSTMs in our model so that one focuses on extracting useful features from only syntactic information while the other composes semantic units with the aid of the features. Furthermore, our model reduces the complexity of computation by utilizing typical tree-LSTM frameworks instead of computing the weights for each example.\nFinally, some recent works BIBREF10 , BIBREF11 have proposed latent tree-structured models that learn how to formulate tree structures from only sequences of tokens, without the aid of syntactic trees or linguistic information. The latent tree models have the advantage of being able to find the optimized task-specific order of composition rather than a sequential or syntactic one. In experiments, we compare our model with not only syntactic tree-based models but also latent tree models, demonstrating that modeling with explicit linguistic knowledge can be an attractive option.\nModel\nIn this section, we introduce a novel RvNN architecture, called SATA Tree-LSTM (Structure-Aware Tag Augmented Tree-LSTM). This model is similar to typical Tree-LSTMs, but provides dynamic compositionality by augmenting a separate tag-level tree-LSTM which produces structure-aware tag representations for each node in a tree. In other words, our model has two independent tree-structured modules based on the same constituency tree, one of which (word-level tree-LSTM) is responsible for constructing sentence representations given a sequence of words as usual, while the other (tag-level tree-LSTM) provides supplementary syntactic information to the former.\nIn section 3.1, we first review tree-LSTM architectures. Then in section 3.2, we introduce a tag-level tree-LSTM and structure-aware tag representations. In section 3.3, we discuss an additional technique to boost the performance of tree-structured models, and in section 3.4, we describe the entire architecture of our model in detail.\nTree-LSTM\nThe LSTM BIBREF1 architecture was first introduced as an extension of the RNN architecture to mitigate the vanishing and exploding gradient problems. In addition, several works have discovered that applying the LSTM cell into tree structures can be an effective means of modeling sentence representations.\nTo be formal, the composition function of the cell in a tree-LSTM can be formulated as follows:\n$$  \\begin{bmatrix} \\mathbf {i} \\\\ \\mathbf {f}_l \\\\ \\mathbf {f}_r \\\\ \\mathbf {o} \\\\ \\mathbf {g} \\end{bmatrix} = \\begin{bmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh \\end{bmatrix} \\Bigg ( \\mathbf {W} \\begin{bmatrix} \\mathbf {h}_l \\\\ \\mathbf {h}_r \\\\ \\end{bmatrix} + \\mathbf {b} \\Bigg )$$   (Eq. 8)\n$$  \\mathbf {c} = \\mathbf {f}_l \\odot \\mathbf {c}_l + \\mathbf {f}_r \\odot \\mathbf {c}_r + \\mathbf {i} \\odot \\mathbf {g}\\\\$$   (Eq. 9)\nwhere $\\mathbf {h}, \\mathbf {c} \\in \\mathbb {R}^{d}$ indicate the hidden state and cell state of the LSTM cell, and $\\mathbf {h}_l, \\mathbf {h}_r, \\mathbf {c}_l, \\mathbf {c}_r \\in \\mathbb {R}^{d}$ the hidden states and cell states of a left and right child. $\\mathbf {g} \\in \\mathbb {R}^{d}$ is the newly composed input for the cell and $\\mathbf {i}, \\mathbf {f}_{l}, \\mathbf {f}_{r}, \\mathbf {o} \\in \\mathbb {R}^{d}$ represent an input gate, two forget gates (left, right), and an output gate respectively. $\\mathbf {W} \\in \\mathbb {R}^{5d\\times 2d}$ and $\\mathbf {b} \\in \\mathbb {R}^{5d}$ are trainable parameters. $\\sigma $ corresponds to the sigmoid function, $\\tanh $ to the hyperbolic tangent, and $\\odot $ to element-wise multiplication.\nNote the equations assume that there are only two children for each node, i.e. binary or binarized trees, following the standard in the literature. While RvNN models can be constructed on any tree structure, in this work we only consider constituency trees as inputs.\nIn spite of the obvious upside that recursive models have in being so flexible, they are known for being difficult to fully utilize with batch computations as compared to other neural architectures because of the diversity of structure found across sentences. To alleviate this problem, BIBREF8 ( BIBREF8 ) proposed the SPINN model, which brings a shift-reduce algorithm to the tree-LSTM. As SPINN simplifies the process of constructing a tree into only two operations, i.e. shift and reduce, it can support more effective parallel computations while enjoying the advantages of tree structures. For efficiency, our model also starts from our own SPINN re-implementation, whose function is exactly the same as that of the tree-LSTM.\nStructure-aware Tag Representation\nIn most previous works using linguistic tag information BIBREF16 , BIBREF9 , BIBREF18 , tags are usually represented as simple low-dimensional dense vectors, similar to word embeddings. This approach seems reasonable in the case of POS tags that are attached to the corresponding words, but phrase-level constituent tags (e.g. NP, VP, ADJP) vary greatly in size and shape, making them less amenable to uniform treatment. For instance, even the same phrase tags within different syntactic contexts can vary greatly in size and internal structure, as the case of NP tags in Figure 1 shows. Here, the NP consisting of DT[the]-NN[stories] has a different internal structure than the NP consisting of NP[the film 's]-NNS[shortcomings].\nOne way of deriving structure-aware tag representations from the original tag embeddings is to introduce a separate tag-level tree-LSTM which accepts the typical tag embeddings at each node of a tree and outputs the computed structure-aware tag representations for the nodes. Note that the module concentrates on extracting useful syntactic features by considering only the tags and structures of the trees, excluding word information.\nFormally, we denote a tag embedding for the tag attached to each node in a tree as $\\textbf {e} \\in \\mathbb {R}^{d_\\text{T}}$ . Then, the function of each cell in the tag tree-LSTM is defined in the following way. Leaf nodes are defined by the following:\n$$  \\begin{bmatrix} \\hat{\\mathbf {c}} \\\\ \\hat{\\mathbf {h}} \\\\ \\end{bmatrix} = \\tanh {\\left(\\mathbf {U}_\\text{T} \\mathbf {e} + \\mathbf {a}_\\text{T}\\right)}$$   (Eq. 13)\nwhile non-leaf nodes are defined by the following:\n$$  \\begin{bmatrix} \\hat{\\mathbf {i}} \\\\ \\hat{\\mathbf {f}}_l \\\\ \\hat{\\mathbf {f}}_r \\\\ \\hat{\\mathbf {o}} \\\\ \\hat{\\mathbf {g}} \\end{bmatrix} = \\begin{bmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh \\end{bmatrix} \\Bigg ( \\mathbf {W_\\text{T}} \\begin{bmatrix} \\hat{\\mathbf {h}}_l \\\\ \\hat{\\mathbf {h}}_r \\\\ \\mathbf {e} \\\\ \\end{bmatrix} + \\mathbf {b}_\\text{T} \\Bigg )$$   (Eq. 14)\n$$  \\hat{\\mathbf {c}} = \\hat{\\mathbf {f}}_l \\odot \\hat{\\mathbf {c}}_l + \\hat{\\mathbf {f}}_r \\odot \\hat{\\mathbf {c}}_r + \\hat{\\mathbf {i}} \\odot \\hat{\\mathbf {g}}\\\\$$   (Eq. 15)\nwhere $\\hat{\\mathbf {h}}, \\hat{\\mathbf {c}} \\in \\mathbb {R}^{d_\\text{T}}$ represent the hidden state and cell state of each node in the tag tree-LSTM. We regard the hidden state ( $\\hat{\\mathbf {h}}$ ) as a structure-aware tag representation for the node. $ \\mathbf {U}_\\text{T} \\in \\mathbb {R}^{2d_\\text{T} \\times d_\\text{T}}, \\textbf {a}_\\text{T} \\in \\mathbb {R}^{2d_\\text{T}}, \\mathbf {W}_\\text{T} \\in \\mathbb {R}^{5d_\\text{T} \\times 3d_\\text{T}}$ , and $\\mathbf {b}_\\text{T} \\in \\mathbb {R}^{5d_\\text{T}}$ are trainable parameters. The rest of the notation follows equations 8 , 9 , and 10 . In case of leaf nodes, the states are computed by a simple non-linear transformation. Meanwhile, the composition function in a non-leaf node absorbs the tag embedding ( $\\mathbf {e}$ ) as an additional input as well as the hidden states of the two children nodes. The benefit of revising tag representations according to the internal structure is that the derived embedding is a function of the corresponding makeup of the node, rather than a monolithic, categorical tag.\nWith regard to the tags themselves, we conjecture that the taxonomy of the tags currently in use in many NLP systems is too complex to be utilized effectively in deep neural models, considering the specificity of many tag sets and the limited amount of data with which to train. Thus, we cluster POS (word-level) tags into 12 groups following the universal POS tagset BIBREF20 and phrase-level tags into 11 groups according to criteria analogous to the case of words, resulting in 23 tag categories in total. In this work, we use the revised coarse-grained tags instead of the original ones. For more details, we refer readers to the supplemental materials.\nLeaf-LSTM\nAn inherent shortcoming of RvNNs relative to sequential models is that each intermediate representation in a tree is unaware of its external context until all the information is gathered together at the root node. In other words, each composition process is prone to be locally optimized rather than globally optimized.\nTo mitigate this problem, we propose using a leaf-LSTM following the convention of some previous works BIBREF21 , BIBREF7 , BIBREF11 , which is a typical LSTM that accepts a sequence of words in order. Instead of leveraging word embeddings directly, we can use each hidden state and cell state of the leaf-LSTM as input tokens for leaf nodes in a tree-LSTM, anticipating the proper contextualization of the input sequence.\nFormally, we denote a sequence of words in an input sentence as $w_{1:n}$ ( $n$ : the length of the sentence), and the corresponding word embeddings as $\\mathbf {x}_{1:n}$ . Then, the operation of the leaf-LSTM at time $t$ can be formulated as,\n$$  \\begin{bmatrix} \\tilde{\\mathbf {i}} \\\\ \\tilde{\\mathbf {f}} \\\\ \\tilde{\\mathbf {o}} \\\\ \\tilde{\\mathbf {g}} \\end{bmatrix} = \\begin{bmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh \\end{bmatrix} \\Bigg ( \\mathbf {W}_\\text{L} \\begin{bmatrix} \\tilde{\\mathbf {h}}_{t-1} \\\\ \\mathbf {x}_t \\\\ \\end{bmatrix} + \\mathbf {b}_\\text{L} \\Bigg )$$   (Eq. 18)\n$$  \\tilde{\\mathbf {c}}_t = \\tilde{\\mathbf {f}} \\odot \\tilde{\\mathbf {c}}_{t-1} + \\tilde{\\mathbf {i}} \\odot \\tilde{\\mathbf {g}}\\\\$$   (Eq. 19)\nwhere $\\mathbf {x}_t \\in \\mathbb {R}^{d_w}$ indicates an input word vector and $\\tilde{\\mathbf {h}}_t$ , $\\tilde{\\mathbf {c}}_t \\in \\mathbb {R}^{d_h}$ represent the hidden and cell state of the LSTM at time $t$ ( $\\tilde{\\mathbf {h}}_{t-1}$ corresponds to the hidden state at time $t$ -1). $\\mathbf {W}_\\text{L}$ and $\\mathbf {b}_\\text{L} $ are learnable parameters. The remaining notation follows that of the tree-LSTM above.\nIn experiments, we demonstrate that introducing a leaf-LSTM fares better at processing the input words of a tree-LSTM compared to using a feed-forward neural network. We also explore the possibility of its bidirectional setting in ablation study.\nSATA Tree-LSTM\nIn this section, we define SATA Tree-LSTM (Structure-Aware Tag Augmented Tree-LSTM, see Figure 2 ) which joins a tag-level tree-LSTM (section 3.2), a leaf-LSTM (section 3.3), and the original word tree-LSTM together.\nAs above we denote a sequence of words in an input sentence as $w_{1:n}$ and the corresponding word embeddings as $\\mathbf {x}_{1:n}$ . In addition, a tag embedding for the tag attached to each node in a tree is denoted by $\\textbf {e} \\in \\mathbb {R}^{d_\\text{T}}$ . Then, we derive the final sentence representation for the input sentence with our model in two steps.\nFirst, we compute structure-aware tag representations ( $\\hat{\\mathbf {h}}$ ) for each node of a tree using the tag tree-LSTM (the right side of Figure 2 ) as follows:\n$$  \\begin{bmatrix} \\hat{\\mathbf {c}} \\\\ \\hat{\\mathbf {h}} \\\\ \\end{bmatrix} = {\\left\\lbrace \\begin{array}{ll} \\text{Tag-Tree-LSTM}(\\mathbf {e}) & \\text{if a leaf node} \\\\ \\text{Tag-Tree-LSTM}(\\hat{\\mathbf {h}}_l, \\hat{\\mathbf {h}}_r, \\mathbf {e}) & \\text{otherwise} \\end{array}\\right.}$$   (Eq. 23)\nwhere Tag-Tree-LSTM indicates the module we described in section 3.2.\nSecond, we combine semantic units recursively on the word tree-LSTM in a bottom-up fashion. For leaf nodes, we leverage the Leaf-LSTM (the bottom-left of Figure 2 , explained in section 3.3) to compute $\\tilde{\\mathbf {c}}_{t}$ and $\\tilde{\\mathbf {h}}_{t}$ in sequential order, with the corresponding input $\\mathbf {x}_t$ .\n$$  \\begin{bmatrix} \\tilde{\\mathbf {c}}_{t} \\\\ \\tilde{\\mathbf {h}}_{t} \\\\ \\end{bmatrix} = \\text{Leaf-LSTM}(\\tilde{\\textbf {h}}_{t-1}, \\textbf {x}_t)$$   (Eq. 24)\nThen, the $\\tilde{\\mathbf {c}}_{t}$ and $\\tilde{\\mathbf {h}}_{t}$ can be utilized as input tokens to the word tree-LSTM, with the left (right) child of the target node corresponding to the $t$ th word in the input sentence.\n$$  \\begin{bmatrix} \\check{\\textbf {c}}_{\\lbrace l, r\\rbrace } \\\\ \\check{\\textbf {h}}_{\\lbrace l, r\\rbrace } \\end{bmatrix} = \\begin{bmatrix} \\tilde{\\textbf {c}}_{t} \\\\ \\tilde{\\textbf {h}}_{t} \\end{bmatrix}$$   (Eq. 25)\nIn the non-leaf node case, we calculate phrase representations for each node in the word tree-LSTM (the upper-left of Figure 2 ) recursively as follows:\n$$  \\check{\\mathbf {g}} = \\tanh {\\left( \\mathbf {U}_\\text{w} \\begin{bmatrix} \\check{\\mathbf {h}}_l \\\\ \\check{\\mathbf {h}}_r \\\\ \\end{bmatrix} + \\mathbf {a}_\\text{w} \\right)}$$   (Eq. 26)\n$$  \\begin{bmatrix} \\check{\\mathbf {i}} \\\\ \\check{\\mathbf {f}}_l \\\\ \\check{\\mathbf {f}}_r \\\\ \\check{\\mathbf {o}} \\end{bmatrix} = \\begin{bmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma \\end{bmatrix} \\Bigg ( \\mathbf {W_\\text{w}} \\begin{bmatrix} \\check{\\mathbf {h}}_l \\\\ \\check{\\mathbf {h}}_r \\\\ \\hat{\\mathbf {h}} \\\\ \\end{bmatrix} + \\mathbf {b}_\\text{w} \\Bigg )$$   (Eq. 27)\nwhere $\\check{\\mathbf {h}}$ , $\\check{\\mathbf {c}} \\in \\mathbb {R}^{d_h}$ represent the hidden and cell state of each node in the word tree-LSTM. $\\mathbf {U}_\\text{w} \\in \\mathbb {R}^{d_h \\times 2d_h}$ , $\\mathbf {W}_\\text{w} \\in \\mathbb {R}^{4d_h \\times \\left(2d_h+d_\\text{T}\\right)}$ , $\\mathbf {a}_\\text{w} \\in \\mathbb {R}^{d_h}$ , $\\mathbf {b}_\\text{w} \\in \\mathbb {R}^{4d_h}$ are learned parameters. The remaining notation follows those of the previous sections. Note that the structure-aware tag representations ( $\\hat{\\mathbf {h}}$ ) are only utilized to control the gate functions of the word tree-LSTM in the form of additional inputs, and are not involved in the semantic composition ( $\\check{\\mathbf {g}}$ ) directly.\nFinally, the hidden state of the root node ( $\\check{\\mathbf {h}}_\\text{root}$ ) in the word-level tree-LSTM becomes the final sentence representation of the input sentence.\nQuantitative Analysis\nOne of the most basic approaches to evaluate a sentence encoder is to measure the classification performance with the sentence representations made by the encoder. Thus, we conduct experiments on the following five datasets. (Summary statistics for the datasets are reported in the supplemental materials.)\nMR: A group of movie reviews with binary (positive / negative) classes. BIBREF22\nSST-2: Stanford Sentiment Treebank BIBREF6 . Similar to MR, but each review is provided in the form of a binary parse tree whose nodes are annotated with numeric sentiment values. For SST-2, we only consider binary (positive / negative) classes.\nSST-5: Identical to SST-2, but the reviews are grouped into fine-grained (very negative, negative, neutral, positive, very positive) classes.\nSUBJ: Sentences grouped as being either subjective or objective (binary classes). BIBREF23\nTREC: A dataset which groups questions into six different question types (classes). BIBREF24\nAs a preprocessing step, we construct parse trees for the sentences in the datasets using the Stanford PCFG parser BIBREF25 . Because syntactic tags are by-products of constituency parsing, we do not need further preprocessing.\nTo classify the sentence given our sentence representation ( $\\check{\\mathbf {h}}_\\text{root}$ ), we use one fully-connected layer with a ReLU activation, followed by a softmax classifier. The final predicted probability distribution of the class $y$ given the sentence $w_{1:n}$ is defined as follows,\n$$\\mathbf {s} = \\text{ReLU}(\\mathbf {W}_\\text{s} \\check{\\mathbf {h}}_\\text{root}+ \\mathbf {b}_\\text{s})$$   (Eq. 37)\n$$p(y|w_{1:n}) = \\text{softmax}(\\mathbf {W}_\\text{c}\\mathbf {s} + \\mathbf {b}_\\text{c})$$   (Eq. 38)\nwhere $\\textbf {s} \\in \\mathbb {R}^{d_\\text{s}}$ is the computed task-specific sentence representation for the classifier, and $\\textbf {W}_\\text{s} \\in \\mathbb {R}^{d_\\text{s} \\times d_h}$ , $\\textbf {W}_\\text{c} \\in \\mathbb {R}^{d_\\text{c} \\times d_s}$ , $\\textbf {b}_\\text{s} \\in \\mathbb {R}^{d_s}$ , $\\textbf {b}_\\text{c} \\in \\mathbb {R}^{d_c}$ are trainable parameters. As an objective function, we use the cross entropy of the predicted and true class distributions.\nThe results of the experiments on the five datasets are shown in table 1 . In this table, we report the test accuracy of our model and various other models on each dataset in terms of percentage. To consider the effects of random initialization, we report the best numbers obtained from each several runs with hyper-parameters fixed.\nCompared with the previous syntactic tree-based models as well as other neural models, our SATA Tree-LSTM shows superior or competitive performance on all tasks. Specifically, our model achieves new state-of-the-art results within the tree-structured model class on 4 out of 5 sentence classification tasks—SST-2, SST-5, MR, and TREC. The model shows its strength, in particular, when the datasets provide phrase-level supervision to facilitate tree structure learning (i.e. SST-2, SST-5). Moreover, the numbers we report for SST-5 and TREC are competitive to the existing state-of-the-art results including ones from structurally pre-trained models such as ELMo BIBREF26 , proving our model's superiority. Note that the SATA Tree-LSTM also outperforms the recent latent tree-based model, indicating that modeling a neural model with explicit linguistic knowledge can be an attractive option.\nOn the other hand, a remaining concern is that our SATA Tree-LSTM is not robust to random seeds when the size of a dataset is relatively small, as tag embeddings are randomly initialized rather than relying on pre-trained ones in contrast with the case of words. From this observation, we could find out there needs a direction of research towards pre-trained tag embeddings.\nTo estimate the performance of our model beyond the tasks requiring only one sentence at a time, we conduct an experiment on the Stanford Natural Language Inference BIBREF34 dataset, each example of which consists of two sentences, the premise and the hypothesis. Our objective given the data is to predict the correct relationship between the two sentences among three options— contradiction, neutral, or entailment.\nWe use the siamese architecture to encode both the premise ( $p_{1:m}$ ) and hypothesis ( $h_{1:n}$ ) following the standard of sentence-encoding models in the literature. (Specifically, $p_{1:m}$ is encoded as $\\check{\\mathbf {h}}_\\text{root}^p \\in \\mathbb {R}^{d_h}$ and $h_{1:n}$ is encoded as $\\check{\\mathbf {h}}_\\text{root}^h \\in \\mathbb {R}^{d_h}$ with the same encoder.) Then, we leverage some heuristics BIBREF35 , followed by one fully-connected layer with a ReLU activation and a softmax classifier. Specifically,\n$$\\mathbf {z} = \\left[ \\check{\\mathbf {h}}_\\text{root}^p; \\check{\\mathbf {h}}_\\text{root}^h; | \\check{\\mathbf {h}}_\\text{root}^p - \\check{\\mathbf {h}}_\\text{root}^h |; \\check{\\mathbf {h}}_\\text{root}^p \\odot \\check{\\mathbf {h}}_\\text{root}^h \\right]$$   (Eq. 41)\n$$\\mathbf {s} = \\text{ReLU}(\\mathbf {W}_\\text{s} \\mathbf {z} + \\mathbf {b}_\\text{s})$$   (Eq. 42)\nwhere $\\textbf {z} \\in \\mathbb {R}^{4d_h}$ , $\\textbf {s} \\in \\mathbb {R}^{d_s}$ are intermediate features for the classifier and $\\textbf {W}_\\text{s} \\in \\mathbb {R}^{d_\\text{s} \\times 4d_h}$ , $\\textbf {W}_\\text{c} \\in \\mathbb {R}^{d_\\text{c} \\times d_s}$ , $\\textbf {b}_\\text{s} \\in \\mathbb {R}^{d_s}$ , $\\textbf {b}_\\text{c} \\in \\mathbb {R}^{d_c}$ are again trainable parameters.\nOur experimental results on the SNLI dataset are shown in table 2 . In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA-LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syntax Tree-LSTM: BIBREF10 ( BIBREF10 ), Tree-based CNN: BIBREF35 ( BIBREF35 ), Gumbel Tree-LSTM: BIBREF11 ( BIBREF11 ), NSE: BIBREF36 ( BIBREF36 ), Reinforced Self-Attention Network: BIBREF4 ( BIBREF4 ), Residual stacked encoders: BIBREF37 ( BIBREF37 ), BiLSTM with generalized pooling: BIBREF38 ( BIBREF38 ).) Note that the number of learned parameters in our model is also comparable to other sophisticated models, showing the efficiency of our model.\nEven though our model has proven its mettle, the effect of tag information seems relatively weak in the case of SNLI, which contains a large amount of data compared to the others. One possible explanation is that neural models may learn some syntactic rules from large amounts of text when the text size is large enough, reducing the necessity of external linguistic knowledge. We leave the exploration of the effectiveness of tags relative to data size for future work.\nHere we go over the settings common across our models during experimentation. For more task-specific details, refer to the supplemental materials.\nFor our input embeddings, we used 300 dimensional 840B GloVe BIBREF39 as pre-trained word embeddings, and tag representations were randomly sampled from the uniform distribution [-0.005, 0.005]. Tag vectors are revised during training while the fine-tuning of the word embedding depends on the task. Our models were trained using the Adam BIBREF40 or Adadelta BIBREF41 optimizer, depending on task. For regularization, weight decay is added to the loss function except for SNLI following BIBREF42 ( BIBREF42 ) and Dropout BIBREF43 is also applied for the word embeddings and task-specific classifiers. Moreover, batch normalization BIBREF44 is adopted for the classifiers. As a default, all the weights in the model are initialized following BIBREF45 ( BIBREF45 ) and the biases are set to 0. The total norm of the gradients of the parameters is clipped not to be over 5 during training.\nOur best models for each dataset were chosen by validation accuracy in cases where a validation set was provided as a part of the dataset. Otherwise, we perform a grid search on probable hyper-parameter settings, or run 10-fold cross-validation in cases where even a test set does not exist.\nAblation Study\nIn this section, we design an ablation study on the core modules of our model to explore their effectiveness. The dataset used in this experiment is SST-2. To conduct the experiment, we only replace the target module with other candidates while maintaining the other settings. To be specific, we focus on two modules, the leaf-LSTM and structure-aware tag embeddings (tag-level tree-LSTM). In the first case, the leaf-LSTM is replaced with a fully-connected layer with a $\\tanh $ activation or Bi-LSTM. In the second case, we replace the structure-aware tag embeddings with naive tag embeddings or do not employ them at all.\nThe experimental results are depicted in Figure 3 . As the chart shows, our model outperforms all the other options we have considered. In detail, the left part of the chart shows that the leaf-LSTM is the most effective option compared to its competitors. Note that the sequential leaf-LSTM is somewhat superior or competitive than the bidirectional leaf-LSTM when both have a comparable number of parameters. We conjecture this may because a backward LSTM does not add additional useful knowledge when the structure of a sentence is already known. In conclusion, we use the uni-directional LSTM as a leaf module because of its simplicity and remarkable performance.\nMeanwhile, the right part of the figure demonstrates that our newly introduced structure-aware embeddings have a real impact on improving the model performance. Interestingly, employing the naive tag embeddings made no difference in terms of the test accuracy, even though the absolute validation accuracy increased (not reported in the figure). This result supports our assumption that tag information should be considered in the structure.\nQualitative Analysis\nIn previous sections, we have numerically demonstrated that our model is effective in encouraging useful composition of semantic units. Here, we directly investigate the computed representations for each node of a tree, showing that the remarkable performance of our model is mainly due to the gradual and recursive composition of the intermediate representations on the syntactic structure.\nTo observe the phrase-level embeddings at a glance, we draw a scatter plot in which a point represents the corresponding intermediate representation. We utilize PCA (Principal Component Analysis) to project the representations into a two-dimensional vector space. As a target parse tree, we reuse the one seen in Figure 1 . The result is shown in Figure 4 .\nFrom this figure, we confirm that the intermediate representations have a hierarchy in the semantic space, which is very similar to that of the parse tree. In other words, as many tree-structured models pursue, we can see the tendency of constructing the representations from the low-level (the bottom of the figure) to the high-level (the top-left and top-right of the figure), integrating the meaning of the constituents recursively. An interesting thing to note is that the final sentence representation is near that of the phrase `, the stories are quietly moving.' rather than that of `Despite the film's shortcomings', catching the main meaning of the sentence.\nConclusion\nWe have proposed a novel RvNN architecture to fully utilize linguistic priors. A newly introduced tag-level tree-LSTM demonstrates that it can effectively control the composition function of the corresponding word-level tree-LSTM. In addition, the proper contextualization of the input word vectors results in significant performance improvements on several sentence-level tasks. For future work, we plan to explore a new way of exploiting dependency trees effectively, similar to the case of constituency trees.\nAcknowledgments\nWe thank anonymous reviewers for their constructive and fruitful comments. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF2016M3C4A7952587).", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "length": 4781, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "debdd11c3c5802df0fbef5055ef8d57888c833e2a230e0b9"}
{"input": "Do they report results only on English data?", "context": "Introduction\nDistributed word representations, commonly referred to as word embeddings BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , serve as elementary building blocks in the course of algorithm design for an expanding range of applications in natural language processing (NLP), including named entity recognition BIBREF4 , BIBREF5 , parsing BIBREF6 , sentiment analysis BIBREF7 , BIBREF8 , and word-sense disambiguation BIBREF9 . Although the empirical utility of word embeddings as an unsupervised method for capturing the semantic or syntactic features of a certain word as it is used in a given lexical resource is well-established BIBREF10 , BIBREF11 , BIBREF12 , an understanding of what these features mean remains an open problem BIBREF13 , BIBREF14 and as such word embeddings mostly remain a black box. It is desirable to be able to develop insight into this black box and be able to interpret what it means, while retaining the utility of word embeddings as semantically-rich intermediate representations. Other than the intrinsic value of this insight, this would not only allow us to explain and understand how algorithms work BIBREF15 , but also set a ground that would facilitate the design of new algorithms in a more deliberate way.\nRecent approaches to generating word embeddings (e.g. BIBREF0 , BIBREF2 ) are rooted linguistically in the field of distributed semantics BIBREF16 , where words are taken to assume meaning mainly by their degree of interaction (or lack thereof) with other words in the lexicon BIBREF17 , BIBREF18 . Under this paradigm, dense, continuous vector representations are learned in an unsupervised manner from a large corpus, using the word cooccurrence statistics directly or indirectly, and such an approach is shown to result in vector representations that mathematically capture various semantic and syntactic relations between words BIBREF0 , BIBREF2 , BIBREF3 . However, the dense nature of the learned embeddings obfuscate the distinct concepts encoded in the different dimensions, which renders the resulting vectors virtually uninterpretable. The learned embeddings make sense only in relation to each other and their specific dimensions do not carry explicit information that can be interpreted. However, being able to interpret a word embedding would illuminate the semantic concepts implicitly represented along the various dimensions of the embedding, and reveal its hidden semantic structures.\nIn the literature, researchers tackled interpretability problem of the word embeddings using different approaches. Several researchers BIBREF19 , BIBREF20 , BIBREF21 proposed algorithms based on non-negative matrix factorization (NMF) applied to cooccurrence variant matrices. Other researchers suggested to obtain interpretable word vectors from existing uninterpretable word vectors by applying sparse coding BIBREF22 , BIBREF23 , by training a sparse auto-encoder to transform the embedding space BIBREF24 , by rotating the original embeddings BIBREF25 , BIBREF26 or by applying transformations based on external semantic datasets BIBREF27 .\nAlthough the above-mentioned approaches provide better interpretability that is measured using a particular method such as word intrusion test, usually the improved interpretability comes with a cost of performance in the benchmark tests such as word similarity or word analogy. One possible explanation for this performance decrease is that the proposed transformations from the original embedding space distort the underlying semantic structure constructed by the original embedding algorithm. Therefore, it can be claimed that a method that learns dense and interpretable word embeddings without inflicting any damage to the underlying semantic learning mechanism is the key to achieve both high performing and interpretable word embeddings.\nEspecially after the introduction of the word2vec algorithm by Mikolov BIBREF0 , BIBREF1 , there has been a growing interest in algorithms that generate improved word representations under some performance metric. Significant effort is spent on appropriately modifying the objective functions of the algorithms in order to incorporate knowledge from external resources, with the purpose of increasing the performance of the resulting word representations BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 , BIBREF37 . Inspired by the line of work reported in these studies, we propose to use modified objective functions for a different purpose: learning more interpretable dense word embeddings. By doing this, we aim to incorporate semantic information from an external lexical resource into the word embedding so that the embedding dimensions are aligned along predefined concepts. This alignment is achieved by introducing a modification to the embedding learning process. In our proposed method, which is built on top of the GloVe algorithm BIBREF2 , the cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to. For words that do not belong to any one of the word-groups, the cost term is left untouched. Specifically, Roget's Thesaurus BIBREF38 , BIBREF39 is used to derive the concepts and concept word-groups to be used as the external lexical resource for our proposed method. We quantitatively demonstrate the increase in interpretability by using the measure given in BIBREF27 , BIBREF40 as well as demonstrating qualitative results. We also show that the semantic structure of the original embedding has not been harmed in the process since there is no performance loss with standard word-similarity or word-analogy tests.\nThe paper is organized as follows. In Section SECREF2 , we discuss previous studies related to our work under two main categories: interpretability of word embeddings and joint-learning frameworks where the objective function is modified. In Section SECREF3 , we present the problem framework and provide the formulation within the GloVe BIBREF2 algorithm setting. In Section SECREF4 where our approach is proposed, we motivate and develop a modification to the original objective function with the aim of increasing representation interpretability. In Section SECREF5 , experimental results are provided and the proposed method is quantitatively and qualitatively evaluated. Additionally, in Section SECREF5 , results demonstrating the extent to which the original semantic structure of the embedding space is affected are presented by using word-analogy and word-similarity tests. We conclude the paper in Section SECREF6 .\nRelated Work\nMethodologically, our work is related to prior studies that aim to obtain “improved” word embeddings using external lexical resources, under some performance metric. Previous work in this area can be divided into two main categories: works that i) modify the word embedding learning algorithm to incorporate lexical information, ii) operate on pre-trained embeddings with a post-processing step.\nAmong works that follow the first approach, BIBREF28 extend the Skip-Gram model by incorporating the word similarity relations extracted from the Paraphrase Database (PPDB) and WordNet BIBREF29 , into the Skip-Gram predictive model as an additional cost term. In BIBREF30 , the authors extend the CBOW model by considering two types of semantic information, termed relational and categorical, to be incorporated into the embeddings during training. For the former type of semantic information, the authors propose the learning of explicit vectors for the different relations extracted from a semantic lexicon such that the word pairs that satisfy the same relation are distributed more homogeneously. For the latter, the authors modify the learning objective such that some weighted average distance is minimized for words under the same semantic category. In BIBREF31 , the authors represent the synonymy and hypernymy-hyponymy relations in terms of inequality constraints, where the pairwise similarity rankings over word triplets are forced to follow an order extracted from a lexical resource. Following their extraction from WordNet, the authors impose these constraints in the form of an additive cost term to the Skip-Gram formulation. Finally, BIBREF32 builds on top of the GloVe algorithm by introducing a regularization term to the objective function that encourages the vector representations of similar words as dictated by WordNet to be similar as well.\nTurning our attention to the post-processing approach for enriching word embeddings with external lexical knowledge, BIBREF33 has introduced the retrofitting algorithm that acts on pre-trained embeddings such as Skip-Gram or GloVe. The authors propose an objective function that aims to balance out the semantic information captured in the pre-trained embeddings with the constraints derived from lexical resources such as WordNet, PPDB and FrameNet. One of the models proposed in BIBREF34 extends the retrofitting approach to incorporate the word sense information from WordNet. Similarly, BIBREF35 creates multi-sense embeddings by gathering the word sense information from a lexical resource and learning to decompose the pre-trained embeddings into a convex combination of sense embeddings. In BIBREF36 , the authors focus on improving word embeddings for capturing word similarity, as opposed to mere relatedness. To this end, they introduce the counter-fitting technique which acts on the input word vectors such that synonymous words are attracted to one another whereas antonymous words are repelled, where the synonymy-antonymy relations are extracted from a lexical resource. More recently, the ATTRACT-REPEL algorithm proposed by BIBREF37 improves on counter-fitting by a formulation which imparts the word vectors with external lexical information in mini-batches.\nMost of the studies discussed above ( BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF36 , BIBREF37 ) report performance improvements in benchmark tests such as word similarity or word analogy, while BIBREF29 uses a different analysis method (mean reciprocal rank). In sum, the literature is rich with studies aiming to obtain word embeddings that perform better under specific performance metrics. However, less attention has been directed to the issue of interpretability of the word embeddings. In the literature, the problem of interpretability has been tackled using different approaches. BIBREF19 proposed non-negative matrix factorization (NMF) for learning sparse, interpretable word vectors from co-occurrence variant matrices where the resulting vector space is called non-negative sparse embeddigns (NNSE). However, since NMF methods require maintaining a global matrix for learning, they suffer from memory and scale issue. This problem has been addressed in BIBREF20 where an online method of learning interpretable word embeddings from corpora using a modified version of skip-gram model BIBREF0 is proposed. As a different approach, BIBREF21 combined text-based similarity information among words with brain activity based similarity information to improve interpretability using joint non-negative sparse embedding (JNNSE).\nA common alternative approach for learning interpretable embeddings is to learn transformations that map pre-trained state-of-the-art embeddings to new interpretable semantic spaces. To obtain sparse, higher dimensional and more interpretable vector spaces, BIBREF22 and BIBREF23 use sparse coding on conventional dense word embeddings. However, these methods learn the projection vectors that are used for the transformation from the word embeddings without supervision. For this reason, labels describing the corresponding semantic categories cannot be provided. An alternative approach was proposed in BIBREF25 , where orthogonal transformations were utilized to increase interpretability while preserving the performance of the underlying embedding. However, BIBREF25 has also shown that total interpretability of an embedding is kept constant under any orthogonal transformation and it can only be redistributed across the dimensions. Rotation algorithms based on exploratory factor analysis (EFA) to preserve the performance of the original word embeddings while improving their interpretability was proposed in BIBREF26 . BIBREF24 proposed to deploy a sparse auto-encoder using pre-trained dense word embeddings to improve interpretability. More detailed investigation of semantic structure and interpretability of word embeddings can be found in BIBREF27 , where a metric was proposed to quantitatively measure the degree of interpretability already present in the embedding vector spaces.\nPrevious works on interpretability mentioned above, except BIBREF21 , BIBREF27 and our proposed method, do not need external resources, utilization of which has both advantages and disadvantages. Methods that do not use external resources require fewer resources but they also lack the aid of information extracted from these resources.\nProblem Description\nFor the task of unsupervised word embedding extraction, we operate on a discrete collection of lexical units (words) INLINEFORM0 that is part of an input corpus INLINEFORM1 , with number of tokens INLINEFORM2 , sourced from a vocabulary INLINEFORM3 of size INLINEFORM4 . In the setting of distributional semantics, the objective of a word embedding algorithm is to maximize some aggregate utility over the entire corpus so that some measure of “closeness” is maximized for pairs of vector representations INLINEFORM14 for words which, on the average, appear in proximity to one another. In the GloVe algorithm BIBREF2 , which we base our improvements upon, the following objective function is considered: DISPLAYFORM0\nIn ( EQREF6 ), INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3 , while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5 . Intuitively, ( EQREF6 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7 , then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( EQREF6 ) is used as a discounting factor that prohibits rare cooccurrences from disproportionately influencing the resulting embeddings.\nThe objective ( EQREF6 ) is minimized using stochastic gradient descent by iterating over the matrix of cooccurrence records INLINEFORM0 . In the GloVe algorithm, for a given word INLINEFORM1 , the final word representation is taken to be the average of the two intermediate vector representations obtained from ( EQREF6 ); i.e, INLINEFORM2 . In the next section, we detail the enhancements made to ( EQREF6 ) for the purposes of enhanced interpretability, using the aforementioned framework as our basis.\nImparting Interpretability\nOur approach falls into a joint-learning framework where the distributional information extracted from the corpus is allowed to fuse with the external lexicon-based information. Word-groups extracted from Roget's Thesaurus are directly mapped to individual dimensions of word embeddings. Specifically, the vector representations of words that belong to a particular group are encouraged to have deliberately increased values in a particular dimension that corresponds to the word-group under consideration. This can be achieved by modifying the objective function of the embedding algorithm to partially influence vector representation distributions across their dimensions over an input vocabulary. To do this, we propose the following modification to the GloVe objective in ( EQREF6 ): rCl J = i,j=1V f(Xij)[ (wiTwj + bi + bj -Xij)2\n+ k(l=1D INLINEFORM0 iFl g(wi,l) + l=1D INLINEFORM1 j Fl g(wj,l) ) ]. In ( SECREF4 ), INLINEFORM2 denotes the indices for the elements of the INLINEFORM3 th concept word-group which we wish to assign in the vector dimension INLINEFORM4 . The objective ( SECREF4 ) is designed as a mixture of two individual cost terms: the original GloVe cost term along with a second term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension INLINEFORM5 . The relative weight of the second term is controlled by the parameter INLINEFORM6 . The simultaneous minimization of both objectives ensures that words that are similar to, but not included in, one of these concept word-groups are also “nudged” towards the associated dimension INLINEFORM7 . The trained word vectors are thus encouraged to form a distribution where the individual vector dimensions align with certain semantic concepts represented by a collection of concept word-groups, one assigned to each vector dimension. To facilitate this behaviour, ( SECREF4 ) introduces a monotone decreasing function INLINEFORM8 defined as INLINEFORM9\nwhich serves to increase the total cost incurred if the value of the INLINEFORM0 th dimension for the two vector representations INLINEFORM1 and INLINEFORM2 for a concept word INLINEFORM3 with INLINEFORM4 fails to be large enough. INLINEFORM5 is also shown in Fig. FIGREF7 .\nThe objective ( SECREF4 ) is minimized using stochastic gradient descent over the cooccurrence records INLINEFORM0 . Intuitively, the terms added to ( SECREF4 ) in comparison with ( EQREF6 ) introduce the effect of selectively applying a positive step-type input to the original descent updates of ( EQREF6 ) for concept words along their respective vector dimensions, which influences the dimension value in the positive direction. The parameter INLINEFORM1 in ( SECREF4 ) allows for the adjustment of the magnitude of this influence as needed.\nIn the next section, we demonstrate the feasibility of this approach by experiments with an example collection of concept word-groups extracted from Roget's Thesaurus.\nExperiments and Results\nWe first identified 300 concepts, one for each dimension of the 300-dimensional vector representation, by employing Roget's Thesaurus. This thesaurus follows a tree structure which starts with a Root node that contains all the words and phrases in the thesaurus. The root node is successively split into Classes and Sections, which are then (optionally) split into Subsections of various depths, finally ending in Categories, which constitute the smallest unit of word/phrase collections in the structure. The actual words and phrases descend from these Categories, and make up the leaves of the tree structure. We note that a given word typically appears in multiple categories corresponding to the different senses of the word. We constructed concept word-groups from Roget's Thesaurus as follows: We first filtered out the multi-word phrases and the relatively obscure terms from the thesaurus. The obscure terms were identified by checking them against a vocabulary extracted from Wikipedia. We then obtained 300 word-groups as the result of a partitioning operation applied to the subtree that ends with categories as its leaves. The partition boundaries, hence the resulting word-groups, can be chosen in many different ways. In our proposed approach, we have chosen to determine this partitioning by traversing this tree structure from the root node in breadth-first order, and by employing a parameter INLINEFORM0 for the maximum size of a node. Here, the size of a node is defined as the number of unique words that ever-descend from that node. During the traversal, if the size of a given node is less than this threshold, we designate the words that ultimately descend from that node as a concept word-group. Otherwise, if the node has children, we discard the node, and queue up all its children for further consideration. If this node does not have any children, on the other hand, the node is truncated to INLINEFORM1 elements with the highest frequency-ranks, and the resulting words are designated as a concept word-group. We note that the choice of INLINEFORM2 greatly affects the resulting collection of word-groups: Excessively large values result in few word-groups that greatly overlap with one another, while overly small values result in numerous tiny word-groups that fail to adequately represent a concept. We experimentally determined that a INLINEFORM3 value of 452 results in the most healthy number of relatively large word-groups (113 groups with size INLINEFORM4 100), while yielding a preferably small overlap amongst the resulting word-groups (with average overlap size not exceeding 3 words). A total of 566 word-groups were thus obtained. 259 smallest word-groups (with size INLINEFORM5 38) were discarded to bring down the number of word-groups to 307. Out of these, 7 groups with the lowest median frequency-rank were further discarded, which yields the final 300 concept word-groups used in the experiments. We present some of the resulting word-groups in Table TABREF9 .\nBy using the concept word-groups, we have trained the GloVe algorithm with the proposed modification given in Section SECREF4 on a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out. Using the parameters given in Table TABREF10 , this resulted in a vocabulary size of 287,847. For the weighting parameter in Eq. SECREF4 , we used a value of INLINEFORM0 . The algorithm was trained over 20 iterations. The GloVe algorithm without any modifications was also trained as a baseline with the same parameters. In addition to the original GloVe algorithm, we compare our proposed method with previous studies that aim to obtain interpretable word vectors. We train the improved projected gradient model proposed in BIBREF20 to obtain word vectors (called OIWE-IPG) using the same corpus we use to train GloVe and our proposed method. Using the methods proposed in BIBREF23 , BIBREF26 , BIBREF24 on our baseline GloVe embeddings, we obtain SOV, SPINE and Parsimax (orthogonal) word representations, respectively. We train all the models with the proposed parameters. However, in BIBREF26 , the authors show results for a relatively small vocabulary of 15,000 words. When we trained their model on our baseline GloVe embeddings with a large vocabulary of size 287,847, the resulting vectors performed significantly poor on word similarity tasks compared to the results presented in their paper. In addition, Parsimax (orthogonal) word vectors obtained using method in BIBREF26 are nearly identical to the baseline vectors (i.e. learned orthogonal transformation matrix is very close to identity). Therefore, Parsimax (orthogonal) yields almost same results with baseline vectors in all evaluations. We evaluate the interpretability of the resulting embeddings qualitatively and quantitatively. We also test the performance of the embeddings on word similarity and word analogy tests.\nIn our experiments, vocabulary size is close to 300,000 while only 16,242 unique words of the vocabulary are present in the concept groups. Furthermore, only dimensions that correspond to the concept group of the word will be updated due to the additional cost term. Given that these concept words can belong to multiple concept groups (2 on average), only 33,319 parameters are updated. There are 90 million individual parameters present for the 300,000 word vectors of size 300. Of these parameters, only approximately 33,000 are updated by the additional cost term.\nQualitative Evaluation for Interpretability\nIn Fig. FIGREF13 , we demonstrate the particular way in which the proposed algorithm ( SECREF4 ) influences the vector representation distributions. Specifically, we consider, for illustration, the 32nd dimension values for the original GloVe algorithm and our modified version, restricting the plots to the top-1000 words with respect to their frequency ranks for clarity of presentation. In Fig. FIGREF13 , the words in the horizontal axis are sorted in descending order with respect to the values at the 32nd dimension of their word embedding vectors coming from the original GloVe algorithm. The dimension values are denoted with blue and red/green markers for the original and the proposed algorithms, respectively. Additionally, the top-50 words that achieve the greatest 32nd dimension values among the considered 1000 words are emphasized with enlarged markers, along with text annotations. In the presented simulation of the proposed algorithm, the 32nd dimension values are encoded with the concept JUDGMENT, which is reflected as an increase in the dimension values for words such as committee, academy, and article. We note that these words (red) are not part of the pre-determined word-group for the concept JUDGMENT, in contrast to words such as award, review and account (green) which are. This implies that the increase in the corresponding dimension values seen for these words is attributable to the joint effect of the first term in ( SECREF4 ) which is inherited from the original GloVe algorithm, in conjunction with the remaining terms in the proposed objective expression ( SECREF4 ). This experiment illustrates that the proposed algorithm is able to impart the concept of JUDGMENT on its designated vector dimension above and beyond the supplied list of words belonging to the concept word-group for that dimension. We also present the list of words with the greatest dimension value for the dimensions 11, 13, 16, 31, 36, 39, 41, 43 and 79 in Table TABREF11 . These dimensions are aligned/imparted with the concepts that are given in the column headers. In Table TABREF11 , the words that are highlighted with green denote the words that exist in the corresponding word-group obtained from Roget's Thesaurus (and are thus explicitly forced to achieve increased dimension values), while the red words denote the words that achieve increased dimension values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced). This again illustrates that a semantic concept can indeed be coded to a vector dimension provided that a sensible lexical resource is used to guide semantically related words to the desired vector dimension via the proposed objective function in ( SECREF4 ). Even the words that do not appear in, but are semantically related to, the word-groups that we formed using Roget's Thesaurus, are indirectly affected by the proposed algorithm. They also reflect the associated concepts at their respective dimensions even though the objective functions for their particular vectors are not modified. This point cannot be overemphasized. Although the word-groups extracted from Roget's Thesaurus impose a degree of supervision to the process, the fact that the remaining words in the entire vocabulary are also indirectly affected makes the proposed method a semi-supervised approach that can handle words that are not in these chosen word-groups. A qualitative example of this result can be seen in the last column of Table TABREF11 . It is interesting to note the appearance of words such as guerilla, insurgency, mujahideen, Wehrmacht and Luftwaffe in addition to the more obvious and straightforward army, soldiers and troops, all of which are not present in the associated word-group WARFARE.\nMost of the dimensions we investigated exhibit similar behaviour to the ones presented in Table TABREF11 . Thus generally speaking, we can say that the entries in Table TABREF11 are representative of the great majority. However, we have also specifically looked for dimensions that make less sense and determined a few such dimensions which are relatively less satisfactory. These less satisfactory examples are given in Table TABREF14 . These examples are also interesting in that they shed insight into the limitations posed by polysemy and existence of very rare outlier words.\nQuantitative Evaluation for Interpretability\nOne of the main goals of this study is to improve the interpretability of dense word embeddings by aligning the dimensions with predefined concepts from a suitable lexicon. A quantitative measure is required to reliably evaluate the achieved improvement. One of the methods proposed to measure the interpretability is the word intrusion test BIBREF41 . But, this method is expensive to apply since it requires evaluations from multiple human evaluators for each embedding dimension. In this study, we use a semantic category-based approach based on the method and category dataset (SEMCAT) introduced in BIBREF27 to quantify interpretability. Specifically, we apply a modified version of the approach presented in BIBREF40 in order to consider possible sub-groupings within the categories in SEMCAT. Interpretability scores are calculated using Interpretability Score (IS) as given below:\nDISPLAYFORM0\nIn ( EQREF17 ), INLINEFORM0 and INLINEFORM1 represents the interpretability scores in the positive and negative directions of the INLINEFORM2 dimension ( INLINEFORM3 , INLINEFORM4 number of dimensions in the embedding space) of word embedding space for the INLINEFORM5 category ( INLINEFORM6 , INLINEFORM7 is number of categories in SEMCAT, INLINEFORM8 ) in SEMCAT respectively. INLINEFORM9 is the set of words in the INLINEFORM10 category in SEMCAT and INLINEFORM11 is the number of words in INLINEFORM12 . INLINEFORM13 corresponds to the minimum number of words required to construct a semantic category (i.e. represent a concept). INLINEFORM14 represents the set of INLINEFORM15 words that have the highest ( INLINEFORM16 ) and lowest ( INLINEFORM17 ) values in INLINEFORM18 dimension of the embedding space. INLINEFORM19 is the intersection operator and INLINEFORM20 is the cardinality operator (number of elements) for the intersecting set. In ( EQREF17 ), INLINEFORM21 gives the interpretability score for the INLINEFORM22 dimension and INLINEFORM23 gives the average interpretability score of the embedding space.\nFig. FIGREF18 presents the measured average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods we compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1 . Our proposed method significantly improves the interpretability for all INLINEFORM2 compared to the original GloVe approach. Our proposed method is second to only SPINE in increasing interpretability. However, as we will experimentally demonstrate in the next subsection, in doing this, SPINE almost entirely destroys the underlying semantic structure of the word embeddings, which is the primary function of a word embedding.\nThe proposed method and interpretability measurements are both based on utilizing concepts represented by word-groups. Therefore it is expected that there will be higher interpretability scores for some of the dimensions for which the imparted concepts are also contained in SEMCAT. However, by design, word groups that they use are formed by using different sources and are independent. Interpretability measurements use SEMCAT while our proposed method utilizes Roget's Thesaurus.\nIntrinsic Evaluation of the Embeddings\nIt is necessary to show that the semantic structure of the original embedding has not been damaged or distorted as a result of aligning the dimensions with given concepts, and that there is no substantial sacrifice involved from the performance that can be obtained with the original GloVe. To check this, we evaluate performances of the proposed embeddings on word similarity BIBREF42 and word analogy BIBREF0 tests. We compare the results with the original embeddings and the three alternatives excluding Parsimax BIBREF26 since orthogonal transformations will not affect the performance of the original embeddings on these tests.\nWord similarity test measures the correlation between word similarity scores obtained from human evaluation (i.e. true similarities) and from word embeddings (usually using cosine similarity). In other words, this test quantifies how well the embedding space reflects human judgements in terms of similarities between different words. The correlation scores for 13 different similarity test sets are reported in Table TABREF20 . We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm, outperforming all the alternatives in almost all test sets. Categories from Roget's thesaurus are groupings of words that are similar in some sense which the original embedding algorithm may fail to capture. These test results signify that the semantic information injected into the algorithm by the additional cost term is significant enough to result in a measurable improvement. It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions.\nWord analogy test is introduced in BIBREF1 and looks for the answers of the questions that are in the form of \"X is to Y, what Z is to ?\" by applying simple arithmetic operations to vectors of words X, Y and Z. We present precision scores for the word analogy tests in Table TABREF21 . It can be seen that the alternative approaches that aim to improve interpretability, have poor performance on the word analogy tests. However, our proposed method has comparable performance with the original GloVe embeddings. Our method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set. This comparable performance is mainly due to the cost function of our proposed method that includes the original objective of the GloVe.\nTo investigate the effect of the additional cost term on the performance improvement in the semantic analogy test, we present Table TABREF22 . In particular, we present results for the cases where i) all questions in the dataset are considered, ii) only the questions that contains at least one concept word are considered, iii) only the questions that consist entirely of concept words are considered. We note specifically that for the last case, only a subset of the questions under the semantic category family.txt ended up being included. We observe that for all three scenarios, our proposed algorithm results in an improvement in the precision scores. However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept word-groups were derived.\nConclusion\nWe presented a novel approach to impart interpretability into word embeddings. We achieved this by encouraging different dimensions of the vector representation to align with predefined concepts, through the addition of an additional cost term in the optimization objective of the GloVe algorithm that favors a selective increase for a pre-specified input of concept words along each dimension.\nWe demonstrated the efficacy of this approach by applying qualitative and quantitative evaluations for interpretability. We also showed via standard word-analogy and word-similarity tests that the semantic coherence of the original vector space is preserved, even slightly improved. We have also performed and reported quantitative comparisons with several other methods for both interpretabilty increase and preservation of semantic coherence. Upon inspection of Fig. FIGREF18 and Tables TABREF20 , TABREF21 , and TABREF22 altogether, it should be noted that our proposed method achieves both of the objectives simultaneously, increased interpretability and preservation of the intrinsic semantic structure.\nAn important point was that, while it is expected for words that are already included in the concept word-groups to be aligned together since their dimensions are directly updated with the proposed cost term, it was also observed that words not in these groups also aligned in a meaningful manner without any direct modification to their cost function. This indicates that the cost term we added works productively with the original cost function of GloVe to handle words that are not included in the original concept word-groups, but are semantically related to those word-groups. The underlying mechanism can be explained as follows. While the outside lexical resource we introduce contains a relatively small number of words compared to the total number of words, these words and the categories they represent have been carefully chosen and in a sense, \"densely span\" all the words in the language. By saying \"span\", we mean they cover most of the concepts and ideas in the language without leaving too many uncovered areas. With \"densely\" we mean all areas are covered with sufficient strength. In other words, this subset of words is able to constitute a sufficiently strong skeleton, or scaffold. Now remember that GloVe works to align or bring closer related groups of words, which will include words from the lexical source. So the joint action of aligning the words with the predefined categories (introduced by us) and aligning related words (handled by GloVe) allows words not in the lexical groups to also be aligned meaningfully. We may say that the non-included words are \"pulled along\" with the included words by virtue of the \"strings\" or \"glue\" that is provided by GloVe. In numbers, the desired effect is achieved by manipulating less than only 0.05% of parameters of the entire word vectors. Thus, while there is a degree of supervision coming from the external lexical resource, the rest of the vocabulary is also aligned indirectly in an unsupervised way. This may be the reason why, unlike earlier proposed approaches, our method is able to achieve increasing interpretability without destroying underlying semantic structure, and consequently without sacrificing performance in benchmark tests.\nUpon inspecting the 2nd column of Table TABREF14 , where qualitative results for concept TASTE are presented, another insight regarding the learning mechanism of our proposed approach can be made. Here it seems understandable that our proposed approach, along with GloVe, brought together the words taste and polish, and then the words Polish and, for instance, Warsaw are brought together by GloVe. These examples are interesting in that they shed insight into how GloVe works and the limitations posed by polysemy. It should be underlined that the present approach is not totally incapable of handling polysemy, but cannot do so perfectly. Since related words are being clustered, sufficiently well-connected words that do not meaningfully belong along with others will be appropriately \"pulled away\" from that group by several words, against the less effective, inappropriate pull of a particular word. Even though polish with lowercase \"p\" belongs where it is, it is attracting Warsaw to itself through polysemy and this is not meaningful. Perhaps because Warsaw is not a sufficiently well-connected word, it ends being dragged along, although words with greater connectedness to a concept group might have better resisted such inappropriate attractions.\nIn this study, we used the GloVe algorithm as the underlying dense word embedding scheme to demonstrate our approach. However, we stress that it is possible for our approach to be extended to other word embedding algorithms which have a learning routine consisting of iterations over cooccurrence records, by making suitable adjustments in the objective function. Since word2vec model is also based on the coocurrences of words in a sliding window through a large corpus, we expect that our approach can also be applied to word2vec after making suitable adjustments, which can be considered as an immediate future work for our approach. Although the semantic concepts are encoded in only one direction (positive) within the embedding dimensions, it might be beneficial to pursue future work that also encodes opposite concepts, such as good and bad, in two opposite directions of the same dimension.\nThe proposed methodology can also be helpful in computational cross-lingual studies, where the similarities are explored across the vector spaces of different languages BIBREF43 , BIBREF44 .", "answers": ["Yes", "Unanswerable"], "length": 6169, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "08034d93200eb0b2207fc07921f81e95f4a801c0961f3724"}
{"input": "What previous methods is their model compared to?", "context": "Introduction\nUnderstanding what a question is asking is one of the first steps that humans use to work towards an answer. In the context of question answering, question classification allows automated systems to intelligently target their inference systems to domain-specific solvers capable of addressing specific kinds of questions and problem solving methods with high confidence and answer accuracy BIBREF0 , BIBREF1 .\nTo date, question classification has primarily been studied in the context of open-domain TREC questions BIBREF2 , with smaller recent datasets available in the biomedical BIBREF3 , BIBREF4 and education BIBREF5 domains. The open-domain TREC question corpus is a set of 5,952 short factoid questions paired with a taxonomy developed by Li and Roth BIBREF6 that includes 6 coarse answer types (such as entities, locations, and numbers), and 50 fine-grained types (e.g. specific kinds of entities, such as animals or vehicles). While a wide variety of syntactic, semantic, and other features and classification methods have been applied to this task, culminating in near-perfect classification performance BIBREF7 , recent work has demonstrated that QC methods developed on TREC questions generally fail to transfer to datasets with more complex questions such as those in the biomedical domain BIBREF3 , likely due in part to the simplicity and syntactic regularity of the questions, and the ability for simpler term-frequency models to achieve near-ceiling performance BIBREF8 . In this work we explore question classification in the context of multiple choice science exams. Standardized science exams have been proposed as a challenge task for question answering BIBREF9 , as most questions contain a variety of challenging inference problems BIBREF10 , BIBREF11 , require detailed scientific and common-sense knowledge to answer and explain the reasoning behind those answers BIBREF12 , and questions are often embedded in complex examples or other distractors. Question classification taxonomies and annotation are difficult and expensive to generate, and because of the unavailability of this data, to date most models for science questions use one or a small number of generic solvers that perform little or no question decomposition BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 . Our long-term interest is in developing methods that intelligently target their inferences to generate both correct answers and compelling human-readable explanations for the reasoning behind those answers. The lack of targeted solving – using the same methods for inferring answers to spatial questions about planetary motion, chemical questions about photosynthesis, and electrical questions about circuit continuity – is a substantial barrier to increasing performance (see Figure FIGREF1 ).\nTo address this need for developing methods of targetted inference, this work makes the following contributions:\nRelated work\nQuestion classification typically makes use of a combination of syntactic, semantic, surface, and embedding methods. Syntactic patterns BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 and syntactic dependencies BIBREF3 have been shown to improve performance, while syntactically or semantically important words are often expanding using Wordnet hypernyms or Unified Medical Language System categories (for the medical domain) to help mitigate sparsity BIBREF22 , BIBREF23 , BIBREF24 . Keyword identification helps identify specific terms useful for classification BIBREF25 , BIBREF3 , BIBREF26 . Similarly, named entity recognizers BIBREF6 , BIBREF27 or lists of semantically related words BIBREF6 , BIBREF24 can also be used to establish broad topics or entity categories and mitigate sparsity, as can word embeddings BIBREF28 , BIBREF29 . Here, we empirically demonstrate many of these existing methods do not transfer to the science domain.\nThe highest performing question classification systems tend to make use of customized rule-based pattern matching BIBREF30 , BIBREF7 , or a combination of rule-based and machine learning approaches BIBREF19 , at the expense of increased model construction time. A recent emphasis on learned methods has shown a large set of CNN BIBREF29 and LSTM BIBREF8 variants achieve similar accuracy on TREC question classification, with these models exhibiting at best small gains over simple term frequency models. These recent developments echo the observations of Roberts et al. BIBREF3 , who showed that existing methods beyond term frequency models failed to generalize to medical domain questions. Here we show that strong performance across multiple datasets is possible using a single learned model.\nDue to the cost involved in their construction, question classification datasets and classification taxonomies tend to be small, which can create methodological challenges. Roberts et al. BIBREF3 generated the next-largest dataset from TREC, containing 2,936 consumer health questions classified into 13 question categories. More recently, Wasim et al. BIBREF4 generated a small corpus of 780 biomedical domain questions organized into 88 categories. In the education domain, Godea et al. BIBREF5 collected a set of 1,155 classroom questions and organized these into 16 categories. To enable a detailed study of science domain question classification, here we construct a large-scale challenge dataset that exceeds the size and classification specificity of other datasets, in many cases by nearly an order of magnitude.\nQuestions and Classification Taxonomy\nQuestions: We make use of the 7,787 science exam questions of the Aristo Reasoning Challenge (ARC) corpus BIBREF31 , which contains standardized 3rd to 9th grade science questions from 12 US states from the past decade. Each question is a 4-choice multiple choice question. Summary statistics comparing the complexity of ARC and TREC questions are shown in Table TABREF5 .\nTaxonomy: Starting with the syllabus for the NY Regents exam, we identified 9 coarse question categories (Astronomy, Earth Science, Energy, Forces, Life Science, Matter, Safety, Scientific Method, Other), then through a data-driven analysis of 3 exam study guides and the 3,370 training questions, expanded the taxonomy to include 462 fine-grained categories across 6 hierarchical levels of granularity. The taxonomy is designed to allow categorizing questions into broad curriculum topics at it's coarsest level, while labels at full specificity separate questions into narrow problem domains suitable for targetted inference methods. Because of its size, a subset of the classification taxonomy is shown in Table TABREF6 , with the full taxonomy and class definitions included in the supplementary material.\nAnnotation: Because of the complexity of the questions, it is possible for one question to bridge multiple categories – for example, a wind power generation question may span both renewable energy and energy conversion. We allow up to 2 labels per question, and found that 16% of questions required multiple labels. Each question was independently annotated by two annotators, with the lead annotator a domain expert in standardized exams. Annotators first independently annotated the entire question set, then questions without complete agreement were discussed until resolution. Before resolution, interannotator agreement (Cohen's Kappa) was INLINEFORM0 = 0.58 at the finest level of granularity, and INLINEFORM1 = 0.85 when considering only the coarsest 9 categories. This is considered moderate to strong agreement BIBREF32 . Based on the results of our error analysis (see Section SECREF21 ), we estimate the overall accuracy of the question classification labels after resolution to be approximately 96%. While the full taxonomy contains 462 fine-grained categories derived from both standardized questions, study guides, and exam syllabi, we observed only 406 of these categories are tested in the ARC question set.\nQuestion Classification on Science Exams\nWe identified 5 common models in previous work primarily intended for learned classifiers rather than hand-crafted rules. We adapt these models to a multi-label hierarchical classification task by training a series of one-vs-all binary classifiers BIBREF34 , one for each label in the taxonomy. With the exception of the CNN and BERT models, following previous work BIBREF19 , BIBREF3 , BIBREF8 we make use of an SVM classifier using the LIBSvM framework BIBREF35 with a linear kernel. Models are trained and evaluated from coarse to fine levels of taxonomic specificity. At each level of taxonomic evaluation, a set of non-overlapping confidence scores for each binary classifier are generated and sorted to produce a list of ranked label predictions. We evaluate these ranks using Mean Average Precision BIBREF36 . ARC questions are evaluated using the standard 3,370 questions for training, 869 for development, and 3,548 for testing.\nN-grams, POS, Hierarchical features: A baseline bag-of-words model incorporating both tagged and untagged unigrams and bigams. We also implement the hierarchical classification feature of Li and Roth BIBREF6 , where for a given question, the output of the classifier at coarser levels of granularity serves as input to the classifier at the current level of granularity.\nDependencies: Bigrams of Stanford dependencies BIBREF37 . For each word, we create one unlabeled bigram for each outgoing link from that word to it's dependency BIBREF20 , BIBREF3 .\nQuestion Expansion with Hypernyms: We perform hypernym expansion BIBREF22 , BIBREF19 , BIBREF3 by including WordNet hypernyms BIBREF38 for the root dependency word, and words on it's direct outgoing links. WordNet sense is identified using Lesk word-sense disambiguation BIBREF39 , using question text for context. We implement the heuristic of Van-tu et al. BIBREF24 , where more distant hypernyms receive less weight.\nEssential Terms: Though not previously reported for QC, we make use of unigrams of keywords extracted using the Science Exam Essential Term Extractor of Khashabi et al. BIBREF26 . For each keyword, we create one binary unigram feature.\nCNN: Kim BIBREF28 demonstrated near state-of-the-art performance on a number of sentence classification tasks (including TREC question classification) by using pre-trained word embeddings BIBREF40 as feature extractors in a CNN model. Lei et al. BIBREF29 showed that 10 CNN variants perform within +/-2% of Kim's BIBREF28 model on TREC QC. We report performance of our best CNN model based on the MP-CNN architecture of Rao et al. BIBREF41 , which works to establish the similarity between question text and the definition text of the question classes. We adapt the MP-CNN model, which uses a “Siamese” structure BIBREF33 , to create separate representations for both the question and the question class. The model then makes use of a triple ranking loss function to minimize the distance between the representations of questions and the correct class while simultaneously maximising the distance between questions and incorrect classes. We optimize the network using the method of Tu BIBREF42 .\nBERT-QC (This work): We make use of BERT BIBREF43 , a language model using bidirectional encoder representations from transformers, in a sentence-classification configuration. As the original settings of BERT do not support multi-label classification scenarios, and training a series of 406 binary classifiers would be computationally expensive, we use the duplication method of Tsoumakas et al. BIBREF34 where we enumerate multi-label questions as multiple single-label instances during training by duplicating question text, and assigning each instance one of the multiple labels. Evaluation follows the standard procedure where we generate a list of ranked class predictions based on class probabilities, and use this to calculate Mean Average Precision (MAP) and Precision@1 (P@1). As shown in Table TABREF7 , this BERT-QC model achieves our best question classification performance, significantly exceeding baseline performance on ARC by 0.12 MAP and 13.5% P@1.\nComparison with Benchmark Datasets\nApart from term frequency methods, question classification methods developed on one dataset generally do not exhibit strong transfer performance to other datasets BIBREF3 . While BERT-QC achieves large gains over existing methods on the ARC dataset, here we demonstrate that BERT-QC also matches state-of-the-art performance on TREC BIBREF6 , while surpassing state-of-the-art performance on the GARD corpus of consumer health questions BIBREF3 and MLBioMedLAT corpus of biomedical questions BIBREF4 . As such, BERT-QC is the first model to achieve strong performance across more than one question classification dataset.\nTREC question classification is divided into separate coarse and fine-grained tasks centered around inferring the expected answer types of short open-domain factoid questions. TREC-6 includes 6 coarse question classes (abbreviation, entity, description, human, location, numeric), while TREC-50 expands these into 50 more fine-grained types. TREC question classification methods can be divided into those that learn the question classification task, and those that make use of either hand-crafted or semi-automated syntactic or semantic extraction rules to infer question classes. To date, the best reported accuracy for learned methods is 98.0% by Xia et al. BIBREF8 for TREC-6, and 91.6% by Van-tu et al. BIBREF24 for TREC-50. Madabushi et al. BIBREF7 achieve the highest to-date performance on TREC-50 at 97.2%, using rules that leverage the strong syntactic regularities in the short TREC factoid questions.\nWe compare the performance of BERT-QC with recently reported performance on this dataset in Table TABREF11 . BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy.\nBecause of the challenges with collecting biomedical questions, the datasets and classification taxonomies tend to be small, and rule-based methods often achieve strong results BIBREF45 . Roberts et al. BIBREF3 created the largest biomedical question classification dataset to date, annotating 2,937 consumer health questions drawn from the Genetic and Rare Diseases (GARD) question database with 13 question types, such as anatomy, disease cause, diagnosis, disease management, and prognoses. Roberts et al. BIBREF3 found these questions largely resistant to learning-based methods developed for TREC questions. Their best model (CPT2), shown in Table TABREF17 , makes use of stemming and lists of semantically related words and cue phrases to achieve 80.4% accuracy. BERT-QC reaches 84.9% accuracy on this dataset, an increase of +4.5% over the best previous model. We also compare performance on the recently released MLBioMedLAT dataset BIBREF4 , a multi-label biomedical question classification dataset with 780 questions labeled using 88 classification types drawn from 133 Unified Medical Language System (UMLS) categories. Table TABREF18 shows BERT-QC exceeds their best model, focus-driven semantic features (FDSF), by +0.05 Micro-F1 and +3% accuracy.\nError Analysis\nWe performed an error analysis on 50 ARC questions where the BERT-QC system did not predict the correct label, with a summary of major error categories listed in Table TABREF20 .\nAssociative Errors: In 35% of cases, predicted labels were nearly correct, differing from the correct label only by the finest-grained (leaf) element of the hierarchical label (for example, predicting Matter INLINEFORM0 Changes of State INLINEFORM1 Boiling instead of Matter INLINEFORM2 Changes of State INLINEFORM3 Freezing). The bulk of the remaining errors were due to questions containing highly correlated words with a different class, or classes themselves being highly correlated. For example, a specific question about Weather Models discusses “environments” changing over “millions of years”, where discussions of environments and long time periods tend to be associated with questions about Locations of Fossils. Similarly, a question containing the word “evaporation” could be primarily focused on either Changes of State or the Water Cycle (cloud generation), and must rely on knowledge from the entire question text to determine the correct problem domain. We believe these associative errors are addressable technical challenges that could ultimately lead to increased performance in subsequent models.\nErrors specific to the multiple-choice domain: We observed that using both question and all multiple choice answer text produced large gains in question classification performance – for example, BERT-QC performance increases from 0.516 (question only) to 0.654 (question and all four answer candidates), an increase of 0.138 MAP. Our error analysis observed that while this substantially increases QC performance, it changes the distribution of errors made by the system. Specifically, 25% of errors become highly correlated with an incorrect answer candidate, which (we show in Section SECREF5 ) can reduce the performance of QA solvers.\nQuestion Answering with QC Labels\nBecause of the challenges of errorful label predictions correlating with incorrect answers, it is difficult to determine the ultimate benefit a QA model might receive from reporting QC performance in isolation. Coupling QA and QC systems can often be laborious – either a large number of independent solvers targeted to specific question types must be constructed BIBREF46 , or an existing single model must be able to productively incorporate question classification information. Here we demostrate the latter – that a BERT QA model is able to incorporate question classification information through query expansion. BERT BIBREF43 recently demonstrated state-of-the-art performance on benchmark question answering datasets such as SQUaD BIBREF47 , and near human-level performance on SWAG BIBREF48 . Similarly, Pan et al. BIBREF49 demonstrated that BERT achieves the highest accuracy on the most challenging subset of ARC science questions. We make use of a BERT QA model using the same QA paradigm described by Pan et al. BIBREF49 , where QA is modeled as a next-sentence prediction task that predicts the likelihood of a given multiple choice answer candidate following the question text. We evaluate the question text and the text of each multiple choice answer candidate separately, where the answer candidate with the highest probablity is selected as the predicted answer for a given question. Performance is evaluated using Precision@1 BIBREF36 . Additional model details and hyperparameters are included in the Appendix.\nWe incorporate QC information into the QA process by implementing a variant of a query expansion model BIBREF50 . Specifically, for a given {question, QC_label} pair, we expand the question text by concatenating the definition text of the question classification label to the start of the question. We use of the top predicted question classification label for each question. Because QC labels are hierarchical, we append the label definition text for each level of the label INLINEFORM0 . An exampe of this process is shown in Table TABREF23 .\nFigure FIGREF24 shows QA peformance using predicted labels from the BERT-QC model, compared to a baseline model that does not contain question classification information. As predicted by the error analysis, while a model trained with question and answer candidate text performs better at QC than a model using question text alone, a large proportion of the incorrect predictions become associated with a negative answer candidate, reducing overall QA performance, and highlighting the importance of evaluating QC and QA models together. When using BERT-QC trained on question text alone, at the finest level of specificity (L6) where overall question classification accuracy is 57.8% P@1, question classification significantly improves QA performance by +1.7% P@1 INLINEFORM0 . Using gold labels shows ceiling QA performance can reach +10.0% P@1 over baseline, demonstrating that as question classification model performance improves, substantial future gains are possible. An analysis of expected gains for a given level of QC performance is included in the Appendix, showing approximately linear gains in QA performance above baseline for QC systems able to achieve over 40% classification accuracy. Below this level, the decreased performance from noise induced by incorrect labels surpasses gains from correct labels.\nHyperparameters: Pilot experiments on both pre-trained BERT-Base and BERT-Large checkpoints showed similar performance benefits at the finest levels of question classification granularity (L6), but the BERT-Large model demonstrated higher overall baseline performance, and larger incremental benefits at lower levels of QC granularity, so we evaluated using that model. We lightly tuned hyperparameters on the development set surrounding those reported by Devlin et al. BIBREF43 , and ultimately settled on parameters similar to their original work, tempered by technical limitations in running the BERT-Large model on available hardware: maximum sequence length = 128, batch size = 16, learning rate: 1e-5. We report performance as the average of 10 runs for each datapoint. The number of epochs were tuned on each run on the development set (to a maximum of 8 epochs), where most models converged to maximum performance within 5 epochs.\nPreference for uncorrelated errors in multiple choice question classification: We primarily report QA performance using BERT-QC trained using text from only the multiple choice questions and not their answer candidates. While this model achieved lower overall QC performance compared to the model trained with both question and multiple choice answer candidate text, it achieved slightly higher performance in the QA+QC setting. Our error analysis in Section SECREF21 shows that though models trained on both question and answer text can achieve higher QC performance, when they make QC errors, the errors tend to be highly correlated with an incorrect answer candidate, which can substantially reduce QA performance. This is an important result for question classification in the context of multiple choice exams.In the context of multiple choice exams, correlated noise can substantially reduce QA performance, meaning the kinds of errors that a model makes are important, and evaluating QC performance in context with QA models that make use of those QC systems is critical.\nRelated to this result, we provide an analysis of the noise sensitivity of the QA+QC model for different levels of question classification prediction accuracy. Here, we perturb gold question labels by randomly selecting a proportion of questions (between 5% and 40%) and randomly assigning that question a different label. Figure FIGREF36 shows that this uncorrelated noise provides roughly linear decreases in performance, and still shows moderate gains at 60% accuracy (40% noise) with uncorrelated noise. This suggests that when making errors, making random errors (that are not correlated to incorrect multiple choice answers) is preferential.\nTraining with predicted labels: We observed small gains when training the BERT-QA model with predicted QC labels. We generate predicted labels for the training set using 5-fold crossvalidation over only the training questions.\nStatistics: We use non-parametric bootstrap resampling to compare baseline (no label) and experimental (QC labeled) runs of the QA+QC experiment. Because the BERT-QA model produces different performance values across successive runs, we perform 10 runs of each condition. We then compute pairwise p-values for each of the 10 no label and QC labeled runs (generating 100 comparisons), then use Fisher's method to combine these into a final statistic.\nQuestion classification paired with question answering shows statistically significant gains of +1.7% P@1 at L6 using predicted labels, and a ceiling gain of up to +10% P@1 using gold labels. The QA performance graph in Figure FIGREF24 contains two deviations from the expectation of linear gains with increasing specificity, at L1 and L3. Region at INLINEFORM0 On gold labels, L3 provides small gains over L2, where as L4 provides large gains over L3. We hypothesize that this is because approximately 57% of question labels belong to the Earth Science or Life Science categories which have much more depth than breadth in the standardized science curriculum, and as such these categories are primarily differentiated from broad topics into detailed problem types at levels L4 through L6. Most other curriculum categories have more breadth than depth, and show strong (but not necessarily full) differentiation at L2. Region at INLINEFORM1 Predicted performance at L1 is higher than gold performance at L1. We hypothesize this is because we train using predicted rather than gold labels, which provides a boost in performance. Training on gold labels and testing on predicted labels substantially reduces the difference between gold and predicted performance.\nThough initial raw interannotator agreement was measured at INLINEFORM0 , to maximize the quality of the annotation the annotators performed a second pass where all disagreements were manually resolved. Table TABREF30 shows question classification performance of the BERT-QC model at 57.8% P@1, meaning 42.2% of the predicted labels were different than the gold labels. The question classification error analysis in Table TABREF20 found that of these 42.2% of errorful predictions, 10% of errors (4.2% of total labels) were caused by the gold labels being incorrect. This allows us to estimate that the overall quality of the annotation (the proportion of questions that have a correct human authored label) is approximately 96%.\nAutomating Error Analyses with QC\nDetailed error analyses for question answering systems are typically labor intensive, often requiring hours or days to perform manually. As a result error analyses are typically completed infrequently, in spite of their utility to key decisions in the algortithm or knowledge construction process. Here we show having access to detailed question classification labels specifying fine-grained problem domains provides a mechanism to automatically generate error analyses in seconds instead of days.\nTo illustrate the utility of this approach, Table TABREF26 shows the performance of the BERT QA+QC model broken down by specific question classes. This allows automatically identifying a given model's strengths – for example, here questions about Human Health, Material Properties, and Earth's Inner Core are well addressed by the BERT-QA model, and achieve well above the average QA performance of 49%. Similarly, areas of deficit include Changes of State, Reproduction, and Food Chain Processes questions, which see below-average QA performance. The lowest performing class, Safety Procedures, demonstrates that while this model has strong performance in many areas of scientific reasoning, it is worse than chance at answering questions about safety, and would be inappropriate to deploy for safety-critical tasks.\nWhile this analysis is shown at an intermediate (L2) level of specificity for space, more detailed analyses are possible. For example, overall QA performance on Scientific Inference questions is near average (47%), but increasing granularity to L3 we observe that questions addressing Experiment Design or Making Inferences – challenging questions even for humans – perform poorly (33% and 20%) when answered by the QA system. This allows a system designer to intelligently target problem-specific knowledge resources and inference methods to address deficits in specific areas.\nConclusion\nQuestion classification can enable targetting question answering models, but is challenging to implement with high performance without using rule-based methods. In this work we generate the most fine-grained challenge dataset for question classification, using complex and syntactically diverse questions, and show gains of up to 12% are possible with our question classification model across datasets in open, science, and medical domains. This model is the first demonstration of a question classification model achieving state-of-the-art results across benchmark datasets in open, science, and medical domains. We further demonstrate attending to question type can significantly improve question answering performance, with large gains possible as quesion classification performance improves. Our error analysis suggests that developing high-precision methods of question classification independent of their recall can offer the opportunity to incrementally make use of the benefits of question classification without suffering the consequences of classification errors on QA performance.\nResources\nOur Appendix and supplementary material (available at http://www.cognitiveai.org/explanationbank/) includes data, code, experiment details, and negative results.\nAcknowledgements\nThe authors wish to thank Elizabeth Wainwright and Stephen Marmorstein for piloting an earlier version of the question classification annotation. We thank the Allen Insitute for Artificial Intelligence and National Science Founation (NSF 1815948 to PJ) for funding this work.\nAnnotation\nClassification Taxonomy: The full classification taxonomy is included in separate files, both coupled with definitions, and as a graphical visualization.\nAnnotation Procedure: Primary annotation took place over approximately 8 weeks. Annotators were instructed to provide up to 2 labels from the full classification taxonomy (462 labels) that were appropriate for each question, and to provide the most specific label available in the taxonomy for a given question. Of the 462 labels in the classification taxonomy, the ARC questions had non-zero counts in 406 question types. Rarely, questions were encountered by annotators that did not clearly fit into a label at the end of the taxonomy, and in these cases the annotators were instructed to choose a more generic label higher up the taxonomy that was appropriate. This occurred when the production taxonomy failed to have specific categories for infrequent questions testing knowledge that is not a standard part of the science curriculum. For example, the question:\nWhich material is the best natural resource to use for making water-resistant shoes? (A) cotton (B) leather (C) plastic (D) wool\ntests a student's knowledge of the water resistance of different materials. Because this is not a standard part of the curriculum, and wasn't identified as a common topic in the training questions, the annotators tag this question as belonging to Matter INLINEFORM0 Properties of Materials, rather than a more specific category.\nQuestions from the training, development, and test sets were randomly shuffled to counterbalance any learning effects during the annotation procedure, but were presented in grade order (3rd to 9th grade) to reduce context switching (a given grade level tends to use a similar subset of the taxonomy – for example, 3rd grade questions generally do not address Chemical Equations or Newtons 1st Law of Motion).\nInterannotator Agreement: To increase quality and consistency, each annotator annotated the entire dataset of 7,787 questions. Two annotators were used, with the lead annotator possessing previous professional domain expertise. Annotation proceeded in a two-stage process, where in stage 1 annotators completed their annotation independently, and in stage 2 each of the questions where the annotators did not have complete agreement were manually resolved by the annotators, resulting in high-quality classification annotation.\nBecause each question can have up to two labels, we treat each label for a given question as a separate evaluation of interannotator agreement. That is, for questions where both annotators labeled each question as having 1 or 2 labels, we treat this as 1 or 2 separate evaluations of interannotator agreement. For cases where one annotator labeled as question as having 1 label, and the other annotator labeled that same question as having 2 labels, we conservatively treat this as two separate interannotator agreements where one annotator failed to specify the second label and had zero agreement on that unspecified label.\nThough the classification procedure was fine-grained compared to other question classification taxonomies, containing an unusually large number of classes (406), overall raw interannotator agreement before resolution was high (Cohen's INLINEFORM0 = 0.58). When labels are truncated to a maximum taxonomy depth of N, raw interannotator increases to INLINEFORM1 = 0.85 at the coarsest (9 class) level (see Table TABREF28 ). This is considered moderate to strong agreement (see McHugh BIBREF32 for a discussion of the interpretation of the Kappa statistic). Based on the results of an error analysis on the question classification system (see Section UID38 ), we estimate that the overall accuracy of the question classification labels after resolution is approximately 96% .\nAnnotators disagreed on 3441 (44.2%) of questions. Primary sources of disagreement before resolution included each annotator choosing a single category for questions requiring multiple labels (e.g. annotator 1 assigning a label of X, and annotator 2 assigning a label of Y, when the gold label was multilabel X, Y), which was observed in 18% of disagreements. Similarly, we observed annotators choosing similar labels but at different levels of specificity in the taxonomy (e.g. annotator 1 assigning a label of Matter INLINEFORM0 Changes of State INLINEFORM1 Boiling, where annotator 2 assigned Matter INLINEFORM2 Changes of State), which occurred in 12% of disagreements before resolution.\nQuestion Classification\nBecause of space limitations the question classification results are reported in Table TABREF7 only using Mean Average Precision (MAP). We also include Precision@1 (P@1), the overall accuracy of the highest-ranked prediction for each question classification model, in Table TABREF30 .\nCNN: We implemented the CNN sentence classifier of Kim BIBREF28 , which demonstrated near state-of-the-art performance on a number of sentence classification tasks (including TREC question classification) by using pre-trained word embeddings BIBREF40 as feature extractors in a CNN model. We adapted the original CNN non-static model to multi-label classification by changing the fully connected softmax layer to sigmoid layer to produce a sigmoid output for each label simultaneously. We followed the same parameter settings reported by Kim et al. except the learning rate, which was tuned based on the development set. Pilot experiments did not show a performance improvement over the baseline model.\nLabel Definitions: Question terms can be mapped to categories using manual heuristics BIBREF19 . To mitigate sparsity and limit heuristic use, here we generated a feature comparing the cosine similarity of composite embedding vectors BIBREF51 representing question text and category definition text, using pretrained GloVe embeddings BIBREF52 . Pilot experiments showed that performance did not significantly improve.\nQuestion Expansion with Hypernyms (Probase Version): One of the challenges of hypernym expansion BIBREF22 , BIBREF19 , BIBREF3 is determining a heuristic for the termination depth of hypernym expansion, as in Van-tu et al. BIBREF24 . Because science exam questions are often grounded in specific examples (e.g. a car rolling down a hill coming to a stop due to friction), we hypothesized that knowing certain categories of entities can be important for identifying specific question types – for example, observing that a question contains a kind of animal may be suggestive of a Life Science question, where similarly vehicles or materials present in questions may suggest questions about Forces or Matter, respectively. The challenge with WordNet is that key hypernyms can be at very different depths from query terms – for example, “cat” is distance 10 away from living thing, “car” is distance 4 away from vehicle, and “copper” is distance 2 away from material. Choosing a static threshold (or decaying threshold, as in Van-tu et al. BIBREF24 ) will inheriently reduce recall and limit the utility of this method of query expansion.\nTo address this, we piloted a hypernym expansion experiment using the Probase taxonomy BIBREF53 , a collection of 20.7M is-a pairs mined from the web, in place of WordNet. Because the taxonomic pairs in Probase come from use in naturalistic settings, links tend to jump levels in the WordNet taxonomy and be expressed in common forms. For example, INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 , are each distance 1 in the Probase taxonomy, and high-frequency (i.e. high-confidence) taxonomic pairs.\nSimilar to query expansion using WordNet Hypernyms, our pilot experiments did not observe a benefit to using Probase hypernyms over the baseline model. An error analysis suggested that the large number of noisy and out-of-context links present in Probase may have reduced performance, and in response we constructed a filtered list of 710 key hypernym categories manually filtered from a list of hypernyms seeded using high-frequency words from an in-house corpus of 250 in-domain science textbooks. We also did not observe a benefit to question classification over the baseline model when expanding only to this manually curated list of key hypernyms.\nTopic words: We made use of the 77 TREC word lists of Li and Roth BIBREF6 , containing a total of 3,257 terms, as well as an in-house set of 144 word lists on general and elementary science topics mined from the web, such as ANIMALS, VEGETABLES, and VEHICLES, containing a total of 29,059 words. To mitigate sparsity, features take the form of counts for a specific topic – detecting the words turtle and giraffe in a question would provide a count of 2 for the ANIMAL feature. This provides a light form of domain-specific entity and action (e.g. types of changes) recognition. Pilot experiments showed that this wordlist feature did add a modest performance benefit of approximately 2% to question classification accuracy. Taken together with our results on hypernym expansion, this suggests that manually curated wordlists can show modest benefits for question classification performance, but at the expense of substantial effort in authoring or collecting these extensive wordlists.\nHyperparameters: For each layer of the class label hierarchy, we tune the hyperparameters based on the development set. We use the pretrained BERT-Base (uncased) checkpoint. We use the following hyperparameters: maximum sequence length = 256, batch size = 16, learning rates: 2e-5 (L1), 5e-5 (L2-L6), epochs: 5 (L1), 25 (L2-L6).\nStatistics: We use non-parametric bootstrap resampling to compare the baseline (Li and Roth BIBREF6 model) to all experimental models to determine significance, using 10,000 bootstrap resamples.", "answers": ["bag-of-words model, CNN"], "length": 5838, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "b5481f9dfee2d9dd2154b3396a702c72f00b86508b001651"}
{"input": "What is the date of birth of William Paulet, 3Rd Marquess Of Winchester's father?", "context": "Passage 1:\nHenry, Lord Paulet\nLord Henry Paulet (1602–1672) was an English courtier who sat briefly in the House of Commons in the 2nd Parliament of Charles I, from February to June 1626. \nPaulet was a son of William Paulet, 4th Marquess of Winchester. On 6 March 1618, he was admitted to Peterhouse, Cambridge. He was created Knight of the Bath at the Coronation of Charles I and was of Amport, Hampshire. In 1626, he was elected as one of the two members of parliament for Andover.Paulet married Lucy Philpot, a daughter of Sir George Philpot. Their son Francis was the grandfather of the twelfth Marquess of Winchester.\nPassage 2:\nLewis Gordon, 3rd Marquess of Huntly\nLewis Gordon, 3rd Marquess of Huntly (c. 1626–1653) was a Scottish nobleman.\nHe was the third son of George Gordon, 2nd Marquess of Huntly.\n\nBiography\nBorn when his father was commander of the Garde Écossaise, he was named after Louis XIII of France, and brought up until the age of ten by his grandfather, George Gordon, 1st Marquess of Huntly. From an early age, he showed himself to be a reckless romantic – while still a child, he stole some jewels and attempted to take ship to Holland, presumably to join the army. When he was thirteen, the First Bishops' War broke out, and the young nobleman sneaked out of Gordon Castle (one account says he climbed over the wall) and hurried to the Highlands, where he raised a brigade of clansmen from his father's estates to fight the Covenanters. His first experience of war was at Megray Hill, where his Highlanders scattered in the face of enemy cannon fire.\nFollowing the peace, Lord Lewis travelled to France, where he enlisted as an ordinary pikeman in an infantry regiment, in order to learn his soldiering from the ground up. After three years, he traveled to England, working his way north by serving on both sides in the English Civil War, first in the royalist army and then in the Scottish Covenanter forces of his uncle, the Earl of Argyll, the same army he had fought against in 1639.\nEventually returning home, the sixteen-year-old nobleman seduced and married the fiancée of his absent elder brother, Viscount Aboyne. He served on both sides in the Scottish Civil War, playing an important role in his father's occupation of Aberdeen in 1646, where he engaged an enemy cavalry commander in single combat and then storming the town. Going into exile after the defeat of the royalists, he traveled again to France; in rapid succession, he succeeded his brother and father as Earl of Enzie and Marquess of Gordon, and by 1651, he was allowed to return to Scotland, even though he refused to conform to the Presbyterian Church of Scotland (he was probably a Roman Catholic).\nIn 1645 Lord Lewis attacked Brodie Castle in Moray and setting it afire destroyed important archives and documents detailing the origins of the illustrious Clan Brodie. This despicable act secured Clan Brodie's place among the great mysteries of Scotland.\nHe died aged 26 or 27, leaving a young widow (whom he had apparently converted to Catholicism), three daughters, and a four-year-old son who would eventually become the 1st Duke of Gordon. Miles Gourdon, a cavalry commander in the French army known as the chevalier or count of \"Crolis\", was perhaps an illegitimate son, as he is said to have been a brother of the duke.His reputation among historians has varied; he is the clearest hero in the Civil War narrative of his kinsman Patrick Gordon of Ruthven, while John Buchan regarded him as wild and headstrong to the point of insanity.\nPassage 3:\nChristopher Lambert (MP)\nChristopher Lambert, of Winchester, Hampshire, was an English politician.\nHe was the second son of William Lambert of Winchester and a servant of Sir William Paulet, 3rd Marquess of Winchester.\nLambert was a Member of Parliament for Bridport in 1593.\nPassage 4:\nWilliam Paulet, 3rd Marquess of Winchester\nWilliam Paulet, 3rd Marquess of Winchester  (c. 1532 – 24 November 1598) was an English nobleman, the son of John Paulet, 2nd Marquess of Winchester and his first wife, Elizabeth Willoughby. His maternal grandfather was Robert Willoughby, 2nd Baron Willoughby de Broke.\nHe was made a Knight of the Bath at the coronation of Mary I on 30 November 1553.\n\nCareer\nThe offices he held during his career included:\nJustice of the Peace, Hampshire from c.1559\nSheriff of Hampshire 1560–61\nJustice of the Peace, Dorset from 1564\nCommissioner for the Musters, Dorset 1569\nHigh Steward, Dorchester by 1570\nJoint Lord Lieutenant of Dorset 1569 and 1585/6-98\nMember of Parliament for Dorset 1571\nJoint Lord Lieutenant of Hampshire 1585\nLord Lieutenant of Hampshire 1585–86\nLord High Steward for the funeral of Mary, Queen of Scots, 1 August 1587\nLord Lieutenant of Hampshire 1596\nCommissioner for Ecclesiastical Causes, Diocese of Winchester 1597Paulet was summoned to Parliament on 5 May 1572 in his father's Barony of St John. He succeeded his father as 3rd Marquess of Winchester on 4 November 1576. During October 1586, he was one of the judges at the trial of Mary, Queen of Scots, later acting as Lord High Steward at her funeral on 1 August 1587.\nHe is known as the author of The Lord Marquess Idleness, a remarkable and most ingenious acrostic of six Latin verses. It was published in 1586 and 1587.\n\nMarriage and issue\nBetween 20 June 1544 and 10 February 1547/1548 he married Anne or Agnes Howard, daughter of William Howard, 1st Baron Howard of Effingham and his first wife, Katherine Broughton and had issue:\nWilliam Paulet, 4th Marquess of Winchester, died 4 February 1629, married Lucy Cecil, daughter of Thomas Cecil, 1st Earl of Exeter\nAnne Paulet, born 1552, married Sir Thomas Denys (modern spelling: Dennis), of Holcombe Burnell, Devon; grandparents of the prodigy Denys Rolle\nKatherine Paulet, married Sir Giles Wroughton\nElizabeth Paulet, married Sir Edward HobyThe marriage was not a happy one, and the couple were only reconciled, on one occasion, by Elizabeth I's intervention.Paulet also had children with his recognised mistress Jane Lambert, who later married the much younger Sir Gerrard Fleetwood:\nSir William Paulet, died 1628, lawyer, London, later of Edington, Wiltshire. High Sheriff of Wiltshire 1613, married Elizabeth, daughter of Sir John Seymour\nSir John Paulet, lawyer, Winchester, married Elizabeth, daughter of John Stump\nSir Hercules Paulet, born 1574, married Bridgett, daughter of Sir Henry Gifford\nHector Paulet, born 1578, married Joan Butler\nSusan or Susanna Paulet, married firstly Thomas Kirkby and secondly Launcelott Warnfford\n\nDeath\nHe died on 24 November 1598 and was buried at Basing, Hampshire. His widow, Anne Paulet, died on 18 November 1601. The date of Jane Lambert's death is not recorded.\nPassage 5:\nWilliam Paulet, 4th Marquess of Winchester\nWilliam Paulet, 4th Marquess of Winchester  (bef. 1560 – 4 February 1629) was an English nobleman, the son of William Paulet, 3rd Marquess of Winchester and Anne or Agnes Howard. He was styled Lord St. John from 1576 to 1598. He was summoned to Parliament on 16 January 1581 in his father's barony as Lord St. John. On 24 November 1598, he succeeded his father as 4th Marquess of Winchester. Paulet experienced great financial difficulties arising from his magnificent style of living and his lavish entertainment of Elizabeth I at Basing House.\n\nMarriage and issue\nOn 28 February 1587 at St Martin-in-the-Fields, he married Lady Lucy Cecil, daughter of Sir Thomas Cecil, 1st Earl of Exeter and his first wife, Dorothy Neville. Lucy and William had six children:\nWilliam Paulet, Lord St John (1587/8–1621), married Mary Browne, daughter of Anthony-Maria Browne, 2nd Viscount Montagu\nThomas Paulet, died before 1621\nJohn Paulet, 5th Marquess of Winchester (c.1598–5 March 1675) married three times:\nJane Savage, daughter of Thomas Savage, 1st Viscount Savage\nHonora de Burgh, daughter of Richard Burke, 4th Earl of Clanricarde\nIsabel Howard, daughter of William Howard, 1st Viscount Stafford and Mary Stafford\nLord Henry Paulet, of Amport, married Lucy Philpot, daughter of Sir George Philpot of Thruxton\nCharles Paulet, died c.1654, had issue\nEdward PauletHis wife, Lucy, was treated for cancer in 1614 by the court physician Théodore de Mayerne. She died 1 October 1614 and was buried a month later in the Cecil vault in Westminster Abbey.\n\nDeath\nWilliam Paulet died at Hackwood, near Basingstoke, on 4 February 1629, and was buried at Basing, Hampshire.\n\nFootnotes\nSources\nExternal links\nWilliam Paulet, Marquess of Winchester Family tree\nHistory of Basing House\nPassage 6:\nArchibald Kennedy, 3rd Marquess of Ailsa\nArchibald Kennedy, 3rd Marquess of Ailsa (1 September 1847 – 9 April 1938) was a Scottish peer.\n\nEarly life\nArchibald was born on 1 September 1847, the eldest of three sons born to Julia (née Jephson), Marchioness of Ailsa, and Archibald Kennedy, 2nd Marquess of Ailsa. Among his siblings was Maj Lord Alexander Kennedy, Lord John Kennedy, Lady Julia Alice Kennedy, Lady Evelyn Anne Kennedy, and Lady Constance Eleanor Kennedy.His father was the eldest son of Archibald Kennedy, Earl of Cassilis, himself the oldest son of Archibald Kennedy, 1st Marquess of Ailsa.  His mother was the second daughter of Sir Richard Jephson, 1st Baronet and the former Charlotte Rochfort Smith.\n\nCareer\nAs a young man, he served as an officer in the Coldstream Guards. In 1885, he founded the Ailsa Shipbuilding Company, which was based in Troon and Ayr, Ayrshire.\n\nPeerage\nUpon the death of his father on 20 March 1870, he succeeded to the titles of 14th Earl of Cassilis, 16th Lord Kennedy, 3rd Marquess of Ailsa and 3rd Baron Ailsa. Along with the title came 76,000 acres in Ayrshire. He held the office of Lord-Lieutenant of Ayrshire between 1919 and 1937.\n\nPersonal life\nLord Ailsa was twice married. His first marriage took place on 7 March 1871 to Hon. Evelyn Stuart, daughter of Charles Stuart, 12th Lord Blantyre and Lady Evelyn Sutherland-Leveson-Gower (herself a daughter of George Sutherland-Leveson-Gower, 2nd Duke of Sutherland). Together, they were the parents of five children:\nArchibald Kennedy, 4th Marquess of Ailsa (1872–1943), who married Frances Stewart, daughter of Sir Mark MacTaggart-Stewart, 1st Baronet.\nCharles Kennedy, 5th Marquess of Ailsa (1875–1956), who married Constance Clarke, widow of Sir John Baird.\nLady Evelyn Kennedy (1876–1886), who died young.\nLady Aline Kennedy (1877–1957), who married John Edward Browne, 5th Baron Kilmaine (1878–1946) in 1901.\nAngus Kennedy, 6th Marquess of Ailsa (1882–1957), who married Gertrude Millicent Cooper.He married secondly on 3 November 1891 to Isabella MacMaster, the only daughter of Hugh MacMaster, a market gardener of Kausani, India. Together, they had two more children:\nLt.-Col. Lord Hugh Kennedy (1895–1970), who married Katharine Louisa Clare Atherton, daughter of Francis Henry Atherton.\nLady Marjory Kennedy (b. 1898), who married Sir Laurence Pierce Brooke Merriam, MC.Lord Ailsa died at his home, Culzean Castle, overlooking the Firth of Clyde where he was known as one of the foremost floriculturists, on 9 April 1938.\n\nSailing\nHe was a keen sailor, having studied navigation, and had William Fife build him Foxhound in 1870, Bloodhound in 1874 and Sleuthhound in 1881. He had his own shipyard at Culzean Castle, where he built the 5-ton Cocker.\nPassage 7:\nRichard Paulet, 17th Marquess of Winchester\nRichard Charles Paulet, 17th Marquess of Winchester (born on 8 July 1905; died 5 March 1968) was the son of Charles Standish Paulet and Lillian Jane Charlotte Fosbery. He was the great-grandson of Lord Charles Paulet, a younger son of the 13th Marquess. He inherited the title from Henry Paulet, 16th Marquess of Winchester, in 1962. He died unmarried, and the title was passed to his cousin Nigel Paulet, 18th Marquess of Winchester.\nPassage 8:\nWilliam Paulet, 1st Marquess of Winchester\nWilliam Paulet, 1st Marquess of Winchester  (c. 1483/1485 – 10 March 1572), styled Lord St John between 1539 and 1550 and Earl of Wiltshire between 1550 and 1551, was an English Lord High Treasurer, Lord Keeper of the Great Seal, and statesman.\n\nFamily origins and early career in Hampshire\nPaulet was the eldest son of Sir John Paulet (1460 – 5 January 1525) of Basing Castle in the parish of Old Basing, near Basingstoke in Hampshire, and of Nunney Castle in Somerset (inherited from the Delamere family in 1415), a cadet branch of Paulet of Hinton St George in Somerset. His mother Alice Paulet was his father's second cousin-once-removed the daughter of Sir William Paulet by his wife Elizabeth Denebaud. William had six siblings, including Sir George Paulet of Crondall Manor in Hampshire and Eleanor Paulet (born 1479), wife of William Giffard of Itchell Manor at Ewshot, also in Hampshire.\nThe family originated at the manor of Paulet (now Pawlett), near Bridgwater in Somerset. The senior branch of the Paulet/Powlet/Poulett family was seated at Hinton St George in Somerset, and had lived in that county since the early thirteenth century; the first Member of Parliament from that line represented Devon in 1385.There is some disagreement over his date of birth, with different authorities quoting 1483 or 1485. A claim that he was ninety-seven at his death would place his birth in 1474 or 1475. There is also uncertainty about where he was born, but it may have been at Fisherton Delamere in Wiltshire, one of his father's manors.His father, who had held a command against the Cornish rebels in 1497, was the head of the branch seated at Paulet and Road, close to Bridgwater, being the son of John Paulet and Elizabeth Roos. William's great-grandfather John Paulet acquired the Hampshire estates by his marriage with Constance Poynings, granddaughter and coheiress of Thomas Poynings, 5th Baron St John of Basing; his barony became abeyant upon his death in 1428/1429.\nWilliam Paulet was High Sheriff of Hampshire in 1512, 1519, 1523, and again in 1527. Knighted before the end of 1525, he was appointed Master of the King's Wards in November 1526 and appeared in the Privy Council in the same year.\n\nMarriage and issue\nHe married Elizabeth (d. 25 December 1558), daughter of Sir William Capel, Lord Mayor of London in 1503, and by her had four sons and four daughters:\nJohn Paulet, 2nd Marquess of Winchester\nThomas\nChidiock Paulet (also spelled Chidiok, Chediok, Chidieok, or Chidiock), governor of Southampton under Mary and Elizabeth\nGiles\nAlice, married Richard Stawell, of Cotherston, Somerset\nMargaret, married Sir William Berkeley\nMargery, married Sir Richard Waller, of Oldstoke, Hampshire\nEleanor (died 26 September 1558), married Sir Richard Pecksall (died 1571) of Beaurepaire, Hampshire, hereditary Master of the Buckhounds.\n\nCareer as a national statesman\nDuring his long career Paulet held numerous offices, which included:\nHigh Sheriff of Hampshire 1511–12, 1518–19 and 1522–23\nJoint Master of the King's Wards 1526–34 and sole Master of the King's Wards 1534–40\nMember of Parliament for Hampshire 1529–36\nComptroller of the Household 1532–37\nKeeper of Pamber Forest 1535/6\nTreasurer of the Household 1537–38/9\nMaster of the King's Woods 1541\nMaster of the Court of Wards 1540–42\nMaster of the Court of Wards and Liveries 1542–54\nPrivy Counsellor 1542\nLord Chamberlain of the Household 1543–45\nLord Steward of the Household 1545-1549/50\nChief Justice in Eyre, South of Trent 1545–49/50\nLord President of the Council 1546–49\nJoint Governor of King Edward VI\nLord Keeper of the Great Seal 1547\nKeeper and Captain of St Andrew's Castle, Hamble 1547–71/2\nKeeper of Alice Holt and Woolmer Forests 1548–71/2\nLord High Treasurer 1549/50–71/2\nLord High Steward for the trial of the Duke of Somerset 1551\nLord Lieutenant of Hampshire 1552, 1553 and 1559\nLieutenant of the forces in London 1558\nSpeaker of the House of Lords 1558 and 1566\nLord Lieutenant of Hampshire and Middlesex 1569\nJoint Lord Lieutenant of London 1569Paulet's political career began in 1529, when he was elected knight of the shire for Hampshire. In 1532, he accompanied King Henry VIII to Calais, France, and the following spring, he accompanied the Duke of Norfolk to join King Francis I of France in a proposed audience with the Pope, to discuss Henry's divorce from Catherine of Aragon. In 1536, he was granted the keepership of Pamber Forest, and on 9 March 1539 was created Baron St John. He became steward of the bishopric of Winchester, and became a close associate of Cardinal Thomas Wolsey and a friend of Thomas Cromwell. He was also Comptroller of the Royal Household, and held many other high positions.\nIn 1535 and 1536, he served as one of the judges for the trials of John Fisher, Sir Thomas More, and the alleged accomplices of Anne Boleyn; in 1535, he became Lord Chamberlain. He partially led the royal forces against the Pilgrimage of Grace, a rebellion that broke out in the autumn of 1536, and in 1538, he became Treasurer of the Household. In 1540, he became the master of Henry's Court of Wards and Liveries, a Knight of the Garter in 1543, and Governor of Portsmouth and Lord Steward of the Household in 1545. In 1546, he became Lord President of the Council, and in 1547, he was an executor of the will of King Henry VIII.He continued his political manoeuvres in 1549 by supporting the Earl of Warwick against the Duke of Somerset—in reward, on 19 January 1550 he was given the Earldom of Wiltshire and Somerset's position of Lord Treasurer. In the following month Warwick took over the post of Lord President of the Council. When Warwick was created Duke of Northumberland on 11 October 1551, Paulet received the Marquessate of Winchester. Six weeks later, he served as Lord High Steward in the Duke of Somerset's trial.\nIt was said that Northumberland and Winchester \"ruled the court\" of the minor King Edward VI. Mary I affirmed him in all of his positions. After her death, he remained Lord Treasurer and retained many of his other positions, and even at an advanced age (in 1559, he was over seventy years old), he showed no signs of declining—he was Speaker of the House of Lords in 1559 and 1566. He remained in good standing with the English monarchs—Queen Elizabeth once joked, \"for, by my troth, if my lord treasurer were but a young man, I could find it in my heart to have him for a husband before any man in England.\" Late in life, he opposed any military support of Continental Protestantism, as he feared it would cause a breach with strongly Catholic Spain.\nPaulet enjoyed a remarkably long career during the Reformation. Starting out as a Catholic, he was quickly persuaded to see things Henry's way once the breach with Rome had been decided on. He was rewarded with former Church properties following the dissolution of the monasteries. Under Edward VI he became an evangelical Protestant and persecuted Roman Catholics and Henrician Conservatives alike. On the accession of the Catholic Mary he announced his reconversion and commenced persecuting his former Protestant co-religionists, even denouncing Bishop Bonner for \"laxity in prosecuting the heretics.\" His wife also found favour with Mary. On Tuesday 21 August 1554, when Mary went into Westminster Abbey her train was carried by Elizabeth, Marchioness of Winchester and Anne of Cleves.On Elizabeth's succession, he once again shifted his sails and became an advocate of middle-road Anglicanism. All in all, he professed five changes in religious course. Once, when asked how he managed to survive so many storms, not only unhurt, but rising all the while, Paulet answered: \"By being a willow, not an oak\".\n\nDeath\nPaulet was still in office when he died on 10 March 1572, a very old man, at Basing House, which he held to rebuild and fortify. His tomb is on the south side of the chancel of Basing church.\nPassage 9:\nJohn Paulet, 5th Marquess of Winchester\nJohn Paulet, 5th Marquess of Winchester (c. 1598 – 5 March 1675), styled Lord John Paulet until 1621 and Lord St John from 1621 to 1628, was the third but eldest surviving son of William Paulet and his successor as 5th Marquess of Winchester.\n\nLife\nHe kept terms at Exeter College, Oxford, but as a Roman Catholic could not matriculate. He sat for St Ives from 1620 to 1622. Staying away to recover his family fortune for most of the 1630s, he returned and presented himself to the court and the king in 1639. The fifth Marquess and the Queen became firm friends thereafter, and therefore his chief seat, Basing House, was the great resort of Queen Henrietta Maria's friends in southwest England.On the outbreak of the English Civil War, he fortified and garrisoned Basing House and held it for Charles I during 1643 and 1644. The siege of Basing House, notwithstanding an attempt of his youngest brother, Lord Edward Paulet, to deliver it up to the enemy, lasted from August 1643 to 16 October 1645, when, during the general decline of the Royal cause, it was taken by storm, after a determined defence, by Oliver Cromwell. The brutality with which the house was sacked was most unusual, as atrocities against civilians during the Civil War were rare and generally discouraged by both sides: the explanation may be the presence of a number of Catholic priests among the defenders. Paulet was subsequently renowned as a great loyalist.The Marquess was made prisoner with such of his garrison as survived the fight; ten pieces of ordnance and much ammunition were also taken by the victors, as Oliver Cromwell himself, who directed the assault, wrote to the Speaker.He was committed to the Tower of London on a charge of high treason in 1645, where he remained a long time; an order was made for allowing him 5l. a week out of his property on 15 Jan 1646. Lady Winchester, who had escaped from Basing two days before its fall, was sent to join her husband in the Tower on 31 Jan, and a weekly sum of 10l., afterwards increased to 15l., was ordered to be paid her for the support of herself and her children, with the stipulation that the latter were to be educated as Protestants. An ordinance for the sale of Winchester's land was passed on 30 Oct, and by the act of 16 July 1651, a portion was sold by the trustees for the sale of forfeited estates. On 7 September 1647 Winchester was allowed to drink the waters at Epsom, and stayed there by permission of parliament for nearly six months. The House of Lords on 30 June 1648 urged the commons to release him on bail in consideration of his bad health. In the propositions sent to the king at the Isle of Wight on 13 October, it was expressly stipulated that Winchester's name be excepted from pardon. Ultimately the commons resolved on 14 March 1649 not to proceed against him for high treason, but they ordered him to be detained in prison and excepted from any composition for his estate. In January 1656 he was a prisoner in execution in the upper bench for debts amounting to 2,000l., and he petitioned Cromwell for relief. The sale of his lands was discontinued by order of parliament on 15 March 1660, and after the Restoration Winchester received them back. It was proposed on 3 August 1660 to recompense him for his losses to the amount of 19,000l. and damages, subsequently reduced to 10,000l., and this was agreed to on 2 July 1661. In the event he was allowed to go unrecompensed at the Restoration of the Monarchy, but regained his lands.\n\nMarriages and issue\nHe married as his first wife:\nJane Savage, daughter of Thomas Savage, 1st Viscount Savage of Rocksavage, on 18 December 1622, and by her had a son:Charles Paulet, 1st Duke of Bolton, born c. 1630\nJane died in childbirth in 1631, prompting an epitaph by John MiltonHe married as his second wife:\n\nHonora de Burgh, born c. 1605, daughter of Richard Burke, 4th Earl of Clanricarde and Frances Walsingham, in around 1645 and by her, had a daughter:Anne, died c. September 1694, married John Belasyse, 1st Baron BelasyseHe married as his third wife:\n\nIsabel Howard, daughter of William Howard, 1st Viscount Stafford and Mary Stafford, sister of the 5th Baron Stafford, in 1669.\n\nDeath\nHe retired to Englefield House in Berkshire, which was a wedding gift from his second marriage to Lady Honora de Burgh in the early 1630s. He died on 5 March 1674 and was buried at Englefield, Berkshire. Paulet was succeeded, by his eldest son, Charles Paulet, as 6th Marquess of Winchester, later created 1st Duke of Bolton. Charles converted to the Church of England, a great blow to the Roman Catholic community of Hampshire, who had for many years looked to the Paulet family to shield them from the worst rigours of the Penal Laws.\n\nFootnotes\nAttribution This article incorporates text from this source, which is in the public domain: \"The Genealogy of the Existing British Peerage\" by Edmund Lodge (1859)\n This article incorporates text from this source, which is in the public domain: Goodwin, Gordon (1895). \"Paulet, John\" .  In Lee, Sidney (ed.). Dictionary of National Biography. Vol. 44. London: Smith, Elder & Co. pp. 90–92.\n\nSources\nExternal links\nJohn Paulet, Marquess of Winchester A family tree\nRoyal Berkshire History: John Paulet\nPortraits of John Paulet, 5th Marquess of Winchester at the National Portrait Gallery, London \nJohn Paulet\nHistory of Basing House\nPassage 10:\nJohn Paulet, 2nd Marquess of Winchester\nJohn Paulet, 2nd Marquess of Winchester (c. 1510 – 4 November 1576), styled The Honourable John Paulet between 1539 and 1550, Lord St John between 1550 and 1551 and Earl of Wiltshire between 1551 and 1555, was an English peer. He was the eldest son of William Paulet, 1st Marquess of Winchester and Elizabeth Capel.\n\nCareer\nJohn Paulet was knighted by Henry VIII at Boulogne on 30 September 1544. After the death of Edward VI he was (with his father) one of the signatories to the settlement of the Crown on Lady Jane Grey of 16 June 1553, although he later changed his allegiance to Queen Mary. He was styled Lord St John from 1550 to 1572. He was summoned to Parliament on 3 October 1554 in one of his father's baronies as Lord St John. He was one of the Peers at the trial of the Duke of Norfolk on 16 January 1572. He succeeded his father as Marquess of Winchester on 10 March 1572.The offices he held during his career included:\nHigh Sheriff of Hampshire 1533–34\nHigh Sheriff of Somerset and Dorset 1543–44\nSteward of Canford castle 1549/50\nConstable of Corfe Castle 1549/50\nLord Lieutenant of Dorset 1557\nGovernor of the Isle of Wight 1558\nKeeper of St Andrew's Castle, Hamble 1572–1576\n\nMarriages and issue\nPaulet was married three times:\nHe married as his first wife, by 20 October 1528, Elizabeth, daughter of Robert Willoughby, 2nd Baron Willoughby de Broke by his second wife, Dorothy, daughter of Thomas Grey, 1st Marquess of Dorset, and by her had four sons and two daughters:William Paulet, 3rd Marquess of Winchester (c. 1532 – 24 November 1598)\nGeorge Paulet\nRichard Paulet\nThomas Paulet\nElizabeth Paulet, married firstly Sir William Courtenay of Powderham and secondly Sir Henry Ughtred\nMary Paulet (died 10 October 1592), married Henry Cromwell, 2nd Baron CromwellHe married secondly, between 10 March and 24 April 1554, Elizabeth Seymour, daughter of Sir John Seymour and Margery Wentworth, and widow of Gregory Cromwell, 1st Baron Cromwell.\nHe married thirdly, before 30 September 1568, Winifred, widow of Sir Richard Sackville, and daughter of John Brydges, a former Lord Mayor of London. He succeeded his father as Marquess of Winchester in 1572.\n\nDeath\nJohn Paulet died at Chelsea on 4 November 1576 and was buried in St. Mary's Church, Basing, Hampshire. His widow, Winifred, died at Chelsea in 1586 and was buried in Westminster Abbey.", "answers": ["1510"], "length": 4621, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "4b28d517ce1c1e3cfec9282ca7b212c1cb87c254781d7c86"}
{"input": "Who is Edward Watson, Viscount Sondes's paternal grandfather?", "context": "Passage 1:\nEdward Watson, Viscount Sondes\nEdward Watson, Viscount Sondes (3 July 1686 – 20 March 1722) of Lees Court, Sheldwich, Kent, and Park Place, London, was a British Whig politician who sat in the House of Commons between 1708 and 1722.Watson was the eldest son of Lewis Watson, 1st Earl of Rockingham and Catherine Sondes, daughter of George Sondes, 1st Earl of Feversham. He matriculated at Merton College, Oxford on 1 June 1703, aged 16 and travelled abroad to Germany in 1707.Watson arrived back from Germany in 1708, in time to be elected as a Whig Member of Parliament for Canterbury at the 1708 British general election. He proposed a motion on 25 January 1709 for an address to the Queen that she should consider remarrying. He also supported the naturalization of the Palatines. He was appointed to a committee to draft a bill to limit the time allowed for public mourning, since this  was felt to be having an  adverse effect on Canterbury's silk trade. He also voted for the impeachment of Dr Sacheverell and possibly in consequence he lost his seat at the 1710 British general election. He was returned unopposed as MP for New Romney at a by-election on 20 April 1713. Following his father's elevation as Earl of Rockingham in 1714, he was styled Viscount Sondes. In 1718, he went over to the Opposition and in 1719 he was appointed a Gentleman of the Bedchamber to the Prince of Wales.He married, on 21 March 1709, Catherine Tufton, eldest daughter of Thomas Tufton, 6th Earl of Thanet in 1709, and had three sons and a daughter:\nLewis Watson, 2nd Earl of Rockingham, no issue\nThomas Watson, 3rd Earl of Rockingham, no issue\nEdward Watson, no issue\nCatherine Watson, married Edward Southwell and had issue.Watson died of consumption at Kensington Gravel Pits 20 March and was buried 31 March 1722 at Rockingham, predeceasing his father by 2 years. In 1729 his widow and her four sisters became co-heiresses to the Barony of Clifford. She died 13 February and was buried 20 February 1734 at Rockingham. The abeyance was terminated in 1734 for the third sister Margaret, wife of Lord Lovel, but following her death without surviving issue in 1775 the barony was restored in favour of Viscount Sondes' grandson, Edward Southwell, 20th Baron Clifford.\nPassage 2:\nKaya Alp\nKaya Alp (Ottoman Turkish: قایا الپ, lit. 'Brave Rock') was, according to Ottoman tradition, the son of Kızıl Buğa or Basuk and the father of Suleyman Shah. He was the grandfather of Ertuğrul Ghazi, the father of the founder of the Ottoman Empire, Osman I. He was also famously known for being the successing name of Ertokus Bey’s son Kaya Alp. He was a descendant of the ancestor of his tribe, Kayı son of Gun son of Oghuz Khagan, the legendary progenitor of the Oghuz Turks.\nPassage 3:\nEdward Watson (footballer)\nEdward Watson (27 October 1901 – 1986) was an English professional footballer who played as a full-back for Sunderland.\nPassage 4:\nLewis Watson, 1st Earl of Rockingham\nLewis Watson, 1st Earl of Rockingham (29 December 1655 – 19 March 1724) was an English peer and politician. He was the eldest son of Edward Watson, 2nd Baron Rockingham (1630 – 1689) and Anne Wentworth, daughter of Thomas Wentworth, 1st Earl of Strafford.In 1681–1685, Watson was Whig Member of Parliament for Canterbury and for Higham Ferrers briefly in 1689, before having to leave the Commons on inheriting his father's barony that year.Lord Rockingham was Master of the Buckhounds in 1703–1705, Custos Rotulorum and Lord Lieutenant of Kent in 1705–1724, Vice-Admiral of Kent in 1705 and Deputy Warden of the Cinque Ports in 1705–1708. In 1714, he was created Earl of Rockingham.In July 1677, he married Catherine Sondes (d. 1696), a daughter of George Sondes, 1st Earl of Feversham. They had five surviving children:\nEdward, styled Viscount Sondes (c. 1687 – Kensington, 20 March 1722), married on 21 March 1708 Lady Catherine Tufton (24 April 1693 – 13 February 1733), daughter of Thomas Tufton, 6th Earl of Thanet and Lady Catherine Cavendish, parents of the 2nd and 3rd Earls of Rockingham and Catherine Watson (d. April 1765), who married Edward Southwell and had Edward Southwell, 20th Baron de Clifford.\nHon. George (24 May 1689 – 1735)\nLady Margaret (1695–1751), married John Monson, 1st Baron Monson.\nLady Mary (d. 1737), married Wray Saunderson.\nLady Arabella, married Sir Robert Furnese, 2nd Baronet.\n\t\t\nHis wife died on 21 March 1696 and was buried at Rockingham. He died on 19 march 1724 and was buried 1 April at Rockingham. He was succeeded by his  grandson, Lewis.\nPassage 5:\nThomas Watson, 3rd Earl of Rockingham\nThomas Watson, 3rd Earl of Rockingham (30 December 1715 – 26 February 1746), styled Hon. Thomas Watson until 1745, was an English nobleman and politician. He represented Canterbury in the House of Commons and was appointed Lord Lieutenant of Kent after succeeding to the earldom, but died shortly thereafter.The second son of Edward Watson, Viscount Sondes, Watson entered Eton College in 1725 and Lincoln's Inn in 1732. In the 1741 British general election, he stood for Canterbury as an opposition Whig. Watson and the Tory Thomas Best ousted the incumbent Sir Thomas Hales, a Whig supporter of Walpole's administration. He continued in opposition to successive governments during his tenure in the House of Commons, which terminated in 1745 when he became Earl of Rockingham on the death of his elder brother Lewis.\nDespite his politics, he was appointed Lord Lieutenant of Kent in succession to his brother, but did not long survive the appointment: he died of smallpox at Rockingham Castle 26 February and was buried 11 March 1746 at Rockingham.On his death, which brought to an end the male line of the Watsons of Rockingham Castle, the Earldom of Rockingham, the Viscountcy of Sondes of Lees Court, and the Barony of Throwley became extinct. He was succeeded as Baron Rockingham by Thomas Watson-Wentworth, 1st Earl of Malton, his first cousin once removed. Rockingham left his estate to his first cousin Lewis Monson, who thereafter adopted the surname of Watson.\nPassage 6:\nEdward Watson (dancer)\nEdward Watson MBE (born 21 May 1976) is a British ballet dancer. He is a principal dancer and coach with the Royal Ballet in London.\n\nEarly years\nEdward Watson was born in Bromley, Kent and was brought up in Dartford with his twin sister, Liz.  He first attended dance classes at the age of 3, and was later accepted as a student at the Royal Ballet School, eventually joining the full-time school at White Lodge, Richmond Park.  Whilst at the school, he trained with Anatoly Grigoriev, a former dancer of the Kirov Ballet and was one of six male students who graduated into the Upper School at the age of 16.  At the Upper School his teachers included German Zammel and Julie Lincoln.  Whilst training at the Upper School, Watson danced a number of roles:\n\nCheckmate by Ninette de Valois, 1993 (Role: Black Castle)\nSimple Symphony by Matthew Hart, 1993\nMonotones No. 2 by Frederick Ashton, 1994\nNapoli by August Bournonville, 2004 (Role: Pas de Six)\n\nCareer\nWatson graduated into The Royal Ballet in 1994 and was promoted to Principal in 2005. His repertory with the Company includes major roles in works by Frederick Ashton and Kenneth MacMillan. His many role creations for Wayne McGregor include in Symbiont(s), Qualia, Chroma, Infra, Limen, Carbon Life, Raven Girl, Tetractys, Woolf Works, Obsidian Tear and Multiverse, and for Christopher Wheeldon Lewis Carroll/The White Rabbit in Alice’s Adventures in Wonderland), Leontes in The Winter’s Tale and John Singer Sargent in Strapless. Watson has worked with numerous other choreographers, including Siobhan Davies, David Dawson, Javier de Frutos, Alastair Marriott, Cathy Marston, Ashley Page and Arthur Pita.In August 2020, it was announced that Watson will retire following a performance of McGregor's The Dante Project. He will remain with the company as a coach. His official title is répétiteur to the principal dancers.\n\nAwards\nAt the National Dance Awards in 2008, Watson won 'Best Male Dancer'. He also won the Olivier Award in 2012 for Outstanding Achievement in Dance for his performance as Gregor Samsa in Arthur Pita's interpretation of Franz Kafka's Metamorphorsis at the Linbury Studio. In 2015 he won Prix Benois de la Danse for his performance as Leontes in Christopher Wheeldon The Winter's Tale at the Royal Ballet.\n\nHonours\nIn the 2015 Birthday Honours, Watson was appointed a Member of the Order of the British Empire (MBE) for services to dance.\nPassage 7:\nLewis Watson, 2nd Earl of Rockingham\nLewis Watson, 2nd Earl of Rockingham (c. 1714 – 4 December 1745) was a British peer, styled Viscount Sondes from 1722 to 1724.He was born the eldest son of Edward Watson, Viscount Sondes and Lady Catherine Tufton, the daughter of Thomas Tufton, 6th Earl of Thanet and Lady Catherine Cavendish.His father having predeceased his own father, Lewis inherited the earldom from his grandfather, Lewis Watson, 1st Earl of Rockingham, in 1724. He was Lord Lieutenant of Kent from 1737 to his death in 1745.He married his first cousin Catherine, daughter of Sir Robert Furnese. As part of the marriage settlement, he purchased a London house in Grosvenor Square and had it grandly decorated with marble tables, Persian carpets, mahogany panelling, silk damask hangings, and an organ.Watson died childless on 4 December and was buried on 14 December 1745 at Rockingham. He was succeeded by his brother, Thomas. His widow subsequently married, on 13 June 1751, as his third wife, Francis, Earl of Guildford, who died on 4 August 1790. She died on 17 December 1766 and was buried at Wroxton.\nPassage 8:\nEdward Southwell Jr.\nEdward Southwell Jr. (16 June 1705 – 16 March 1755) of King's Weston, Gloucestershire,  was an Anglo-Irish Whig politician who sat in the Parliament of Ireland from 1727 to 1755 and in the British House of Commons  from 1739 to 1754.\nSouthwell was the son of Edward Southwell (1671–1730) and Elizabeth Cromwell, 8th Baroness Cromwell and the grandson of Sir Robert Southwell. He was educated at Westminster School from 1715 to 1716 and matriculated at Queen's College, Oxford in 1721. He travelled abroad from 1723.Southwell sat in the Irish House of Commons for Downpatrick from 1727 until his death. He succeeded his father as Principal Secretary of State (Ireland) in 1730, and on 6 May 1732 he was appointed to the Privy Council of Ireland.Southwell married on 21 August 1729, to Lady Katherine Watson (died April 1765), daughter of Edward Watson, Viscount Sondes and Lady Katherine (née Tufton), and lived in Kings Weston House near Bristol. Their son, Edward, later became Baron de Clifford.\nEdward Southwell Jr. sat in the House of Commons of Great Britain from 1739 to 1754 as MP for Bristol.\n\nPersonal papers\nPapers relating to Edward Southwell are held by Bristol Archives (Ref. 44785 and 45317/2/5/1) (online catalogue page 1, online catalogue page 2). A travel journal, dating from 1725 to 1726, is held in the British Library Manuscripts Collections. Other records relating to Edward Southwell are held at Bristol Reference Library.\nPassage 9:\nAbd al-Muttalib\nShayba ibn Hāshim (Arabic: شَيْبَة إبْن هَاشِم; c. 497–578), better known as ʿAbd al-Muṭṭalib, (Arabic: عَبْد ٱلْمُطَّلِب, lit. 'Servant of Muttalib') was the fourth chief of the Quraysh tribal confederation. He was the grandfather of the Islamic prophet Muhammad.\n\nEarly life\nHis father was Hashim ibn 'Abd Manaf,: 81  the progenitor of the distinguished Banu Hashim, a clan of the Quraysh tribe of Mecca. They claimed descent from Ismā'īl and Ibrāhīm. His mother was Salma bint Amr, from the Banu Najjar, a clan of the Khazraj tribe in Yathrib (later called Madinah). Hashim died while doing business in Gaza, before Abd al-Muttalib was born.: 81 His real name was \"Shaiba\" meaning 'the ancient one' or 'white-haired' because of the streak of white through his jet-black hair, and is sometimes also called Shaybah al-Ḥamd (\"The white streak of praise\").: 81–82  After his father's death he was raised in Yathrib with his mother and her family until about the age of eight, when his uncle Muttalib ibn Abd Manaf went to see him and asked his mother Salmah to entrust Shaybah to his care. Salmah was unwilling to let her son go and Shaiba refused to leave his mother without her consent. Muṭṭalib then pointed out that the possibilities Yathrib had to offer were incomparable to Mecca. Salmah was impressed with his arguments, so she agreed to let him go. Upon first arriving in Mecca, the people assumed the unknown child was Muttalib's servant and started calling him 'Abd al-Muttalib (\"servant of Muttalib\").: 85–86\n\nChieftain of Hashim clan\nWhen Muṭṭalib died, Shaiba succeeded him as the chief of the Hāshim clan. Following his uncle Al-Muṭṭalib, he took over the duties of providing the pilgrims with food and water, and carried on the practices of his forefathers with his people. He attained such eminence as none of his forefathers enjoyed; his people loved him and his reputation was great among them.: 61 \n'Umar ibn Al-Khaṭṭāb's grandfather Nufayl ibn Abdul Uzza arbitrated in a dispute between 'Abdul-Muṭṭalib and Ḥarb ibn Umayyah, Abu Sufyan's father, over the custodianship of the Kaaba. Nufayl gave his verdict in favour of 'Abdul-Muṭṭalib. Addressing Ḥarb ibn Umayyah, he said:\nWhy do you pick a quarrel with a person who is taller than you in stature; more imposing than you in appearance; more refined than you in intellect; whose progeny outnumbers yours and whose generosity outshines yours in lustre? Do not, however, construe this into any disparagement of your good qualities which I highly appreciate. You are as gentle as a lamb, you are renowned throughout Arabia for the stentorian tones of your voice, and you are an asset to your tribe.\n\nDiscovery of Zam Zam Well\n'Abdul-Muṭṭalib said that while sleeping in the sacred enclosure, he had dreamed he was ordered to dig at the worship place of the Quraysh between the two deities Isāf and Nā'ila. There he would find the Zamzam Well, which the Jurhum tribe had filled in when they left Mecca. The Quraysh tried to stop him digging in that spot, but his son Al-Ḥārith stood guard until they gave up their protests. After three days of digging, 'Abdul-Muṭṭalib found traces of an ancient religious well and exclaimed, \"Allahuakbar!\" Some of the Quraysh disputed his claim to sole rights over water, then one of them suggested that they go to a female shaman who lived afar. It was said that she could summon jinns and that she could help them decide who was the owner of the well. So, 11 people from the 11 tribes went on the expedition. They had to cross the desert to meet the priestess but then they got lost. There was a lack of food and water and people started to lose hope of ever getting out. One of them suggested that they dig their own graves and if they died, the last person standing would bury the others. So all began digging their own graves and just as Abdul-Muṭṭalib started digging, water spewed out from the hole he dug and everyone became overjoyed. It was then and there decided that Abdul-Muttalib was the owner of the Zam Zam well. Thereafter he supplied pilgrims to the Kaaba with Zam Zam water, which soon eclipsed all the other wells in Mecca because it was considered sacred.: 86–89 : 62–65\n\nThe Year of the Elephant\nAccording to Muslim tradition, the Ethiopian governor of Yemen, Abrahah al-Ashram, envied the Kaaba's reverence among the Arabs and, being a Christian, he built a cathedral on Sana'a and ordered pilgrimage be made there.: 21  The order was ignored and someone desecrated (some saying in the form of defecation: 696 note 35 ) the cathedral. Abrahah decided to avenge this act by demolishing the Kaaba and he advanced with an army towards Mecca.: 22–23 There were thirteen elephants in Abrahah's army: 99 : 26  and the year came to be known as 'Ām al-Fīl (the Year of the Elephant), beginning a trend for reckoning the years in Arabia which was used until 'Umar ibn Al-Khaṭṭāb replaced it with the Islamic Calendar in 638 CE (17 AH), with the first year of the Islamic Calendar being 622 CE.\nWhen news of the advance of Abrahah's army came, the Arab tribes of Quraysh, Kinānah, Khuzā'ah and Hudhayl united in defence of the Kaaba. A man from the Ḥimyar tribe was sent by Abrahah to advise them that he only wished to demolish the Kaaba and if they resisted, they would be crushed. \"Abdul-Muṭṭalib told the Meccans to seek refuge in the nearest high hills while he, with some leading members of Quraysh, remained within the precincts of the Kaaba. Abrahah sent a dispatch inviting 'Abdul-Muṭṭalib to meet him and discuss matters. When 'Abdul-Muṭṭalib left the meeting he was heard saying, \"The Owner of this House is its Defender, and I am sure He will save it from the attack of the adversaries and will not dishonour the servants of His House.\": 24–26 It is recorded that when Abrahah's forces neared the Kaaba, Allah commanded small birds (abābīl) to destroy Abrahah's army, raining down pebbles on it from their beaks. Abrahah was seriously wounded and retreated towards Yemen but died on the way.: 26–27  This event is referred to in the following Qur'anic chapter:\n\nHave you not seen how your Lord dealt with the owners of the Elephant?\nDid He not make their treacherous plan go astray?\n\nAnd He sent against them birds in flocks, striking them with stones of baked clay, so He rendered them like straw eaten up.\nMost Islamic sources place the event around the year that Muhammad was born, 570 CE, though other scholars place it one or two decades earlier. A tradition attributed to Ibn Shihab al-Zuhri in the musannaf of ʽAbd al-Razzaq al-Sanʽani places it before the birth of Muhammad's father.\n\nSacrificing his son Abdullah\nAl-Harith was 'Abdul-Muṭṭalib's only son at the time he dug the Zamzam Well.: 64  When the Quraysh tried to help him in the digging, he vowed that if he were to have ten sons to protect him, he would sacrifice one of them to Allah at the Kaaba. Later, after nine more sons had been born to him, he told them he must keep the vow. The divination arrows fell upon his favourite son Abdullah. The Quraysh protested 'Abdul-Muṭṭalib's intention to sacrifice his son and demanded that he sacrifice something else instead. 'Abdul-Muṭṭalib agreed to consult a \"sorceress with a familiar spirit\". She told him to cast lots between Abdullah and ten camels. If Abdullah were chosen, he had to add ten more camels, and keep on doing the same until his Lord accepted the camels in Abdullah's place. When the number of camels reached 100, the lot fell on the camels. 'Abdul-Muṭṭalib confirmed this by repeating the test three times. Then the camels were sacrificed, and Abdullah was spared.: 66–68\n\nFamily\nWives\nAbd al-Muttalib had six known wives.\n\nSumra bint Jundab of the Hawazin tribe.\nLubnā bint Hājar of the Khuza'a tribe.\nFatima bint Amr of the Makhzum clan of the Quraysh tribe.\nHalah bint Wuhayb of the Zuhrah clan of the Quraysh tribe.\nNatīla bint Janab of the Namir tribe.\nMumanna'a bint Amr of the Khuza'a tribe.\n\nChildren\nAccording to Ibn Hisham, ʿAbd al-Muṭṭalib had ten sons and six daughters.: 707–708 note 97  However, Ibn Sa'd lists twelve sons.: 99–101 By Sumra bint Jundab:\n\nAl-Ḥārith.: 708  He was the firstborn and he died before his father.: 99 \nQuthum.: 100  He is not listed by Ibn Hisham.By Fatima bint Amr:\n\nAl-Zubayr.: 707  He was a poet and a chief; his father made a will in his favour.: 99  He died before Islam, leaving two sons and daughters.: 101 : 34–35 \nAbu Talib, born as Abd Manaf,: 99 : 707  father of the future Caliph Ali. He later became chief of the Hashim clan.\nAbdullah, the father of Muhammad.: 99 : 707 \nUmm Hakim al-Bayda,: 100 : 707  the maternal grandmother of the third Caliph Uthman.: 32 \nBarra,: 100 : 707  the mother of Abu Salama.: 33 \nArwa.: 100 : 707 \nAtika,: 100 : 707  a wife of Abu Umayya ibn al-Mughira.: 31 \nUmayma,: 100 : 707  the mother of Zaynab bint Jahsh and Abd Allah ibn Jahsh.: 33 By Lubnā bint Hājar:\n\nAbd al-'Uzzā, better known as Abū Lahab.: 100 : 708 By Halah bint Wuhayb:\n\nḤamza,: 707  the first big leader of Islam. He killed many leaders of the kufar and was considered as the strongest man of the quraysh. He was martyred at Uhud.: 100 \nṢafīyya.: 100 : 707 \nAl-Muqawwim.: 707  He married Qilaba bint Amr ibn Ju'ana ibn Sa'd al-Sahmia, and had children named Abd Allah, Bakr, Hind, Arwa, and Umm Amr (Qutayla or Amra).\nHajl.: 707  He married Umm Murra bint Abi Qays ibn Abd Wud, and had two sons, named Abd Allah, Ubayd Allah, and three daughters named Murra, Rabi'a, and Fakhita.By Natīlah bint Khubāb:\n\nal-'Abbas,: 100 : 707  ancestor of the Abbasid caliphs.\nḌirār,: 707  who died before Islam.: 100 \nJahl, died before Islam\nImran, died before IslamBy Mumanna'a bint 'Amr:\n\nMus'ab, who, according to Ibn Saad, was the one known as al-Ghaydāq.: 100  He is not listed by Ibn Hisham.\nAl-Ghaydaq, died before Islam.\nAbd al-Ka'ba, died before Islam.: 100 \nAl-Mughira,: 100  who had the byname al-Ghaydaq.\n\nThe family tree and some of his important descendants\nDeath\nAbdul Muttalib's son 'Abdullāh died four months before Muḥammad's birth, after which Abdul Muttalib took care of his daughter-in-law Āminah. One day Muhammad's mother, Amina, wanted to go to Yathrib, where her husband, Abdullah, died. So, Muhammad, Amina, Abd al-Muttalib and their caretaker, Umm Ayman started their journey to Medina, which is around 500 kilometres away from Makkah. They stayed there for three weeks, then, started their journey back to Mecca. But, when they reached halfway, at Al-Abwa', Amina became very sick and died six years after her husband's death. She was buried over there. From then, Muhammad became an orphan. Abd al-Muttalib became very sad for Muhammad because he loved him so much. Abd al-Muttalib took care of Muhammad. But when Muhammad was eight years old, the very old Abd al-Muttalib became very sick and died at age 81-82 in 578-579 CE.\nShaybah ibn Hāshim's grave can be found in the Jannat al-Mu'allā cemetery in Makkah, Saudi Arabia.\n\nSee also\nFamily tree of Muhammad\nFamily tree of Shaiba ibn Hashim\nSahaba\nPassage 10:\nKevin Watson\nKevin Edward Watson (born 3 January 1974) is a former professional footballer who played as a midfielder. After his retirement from playing, he turned non-league coach.\n\nPlaying career\nWatson started his career as a trainee with his local side Tottenham Hotspur. Watson scored his only Spurs goal on his debut in a League Cup tie against Brentford. As a youngster his first-team opportunities were limited and he went out on loan to several sides to build his experience, namely Brentford, Bristol City and Barnet.\nIn 1996, he was signed by Swindon Town manager Steve McMahon. Newly promoted to second tier of the English Football League – Watson helped them avoid relegation.\nWatson moved on to Rotherham United in July 1999, astute Millers' manager Ronnie Moore signing him on a free transfer. In his first season Watson helped Rotherham to promotion from League Two, narrowly missing out on the title.\nAnother promotion (and another title near miss) followed in the next season and Rotherham were promoted again into the Football League Championship. A key part of their rapid acceleration, Watson helped guide the team to survival by the narrowest of goal-difference margins.\nAfter over 100 games at Rotherham his knack for promotions was spotted by then Reading manager Alan Pardew who at first, took Watson on loan before signing him permanently in March 2002 for £150,000 – where he promptly repeated the trick, guiding the Royals to promotion from League One, in his now customary second place.\nHis first full season at Reading saw the Royals into a play-off place, where narrow defeat to Wolves cost them a place in the Premiership. Perhaps tellingly Watson was on the bench for those key play-off games.\nWhen former teammate Phil Parkinson was installed as manager at Colchester United one of his first acts was to bring Watson to Layer Road.\nMarshalling the midfield Watson played a key role as in his (and Parkinson's) first full season the U's stormed to promotion from League One, finishing in second place.\nNow with over 100 games for the U's under his belt he has been a key part of \"The Best Col U ever\" as the underdog U's finished their first season in the Football League Championship in 10th place.\nHe was released by Colchester at the end of the 2007–08 season, and then signed on a free transfer for Luton Town. However, Watson failed to make an impact at Luton as a result of a knee injury that kept him out of action for much of his contract duration, and he was released at the end of 2008.\nIn 2015 whilst assistant manager of Stevenage, Watson alongside Stevenage manager Teddy Sheringham registered as players.\n\nCoaching career\nAfter his release from a playing contract at Luton, Watson was given the position of first-team coach by Luton manager Mick Harford on 14 January 2009. After Harford's departure on 1 October 2009, Watson continued in his coaching capacity under new boss Richard Money until May 2010, when he left the club.Following the departure of Aidy Boothroyd from Colchester United to take the manager's job at Coventry City on 20 May 2010, Watson expressed an interest in taking up the vacant managerial position at his former club.In May 2015 he was appointed manager of Maldon & Tiptree However, Watson left the club after only eleven days to take up the position of assistant manager to Teddy Sheringham at Stevenage. He left his position in February after Teddy Sheringham's departure. From July 2015 to November 2015, he worked at the academy of Colchester United.\nIn August 2016, Watson was appointed assistant manager at Eastleigh. In January 2017 he was appointed to the same position at Whitehawk but left the club after just two games, following a change of manager. In March 2017 he was appointed manager of Bishop's Stortford. He left the club at the end of the 2017–18 season.On 28 June 2018, Watson was appointed assistant manager of Hungerford Town. He left the club on 2 October to join Ebbsfleet United as a first-team coach. However, only eight days later, he was appointed caretaker after manager Gary Hill's departure. After a three-game unbeaten run the club confirmed, that Watson had been appointed to the manager's job on a permanent basis.The club was beaten in the heaviest defeat in 27 years, 7–0 by Barrow on 4 January and also crashed out of the FA Trophy to lower league opponents Royston. Other heavy losses such as a 4–0 drubbing at Chesterfield away ultimately saw the club relegated out of the National League and Watson sacked.On 7 January 2021, Watson was announced as the new manager of National League South side Billericay Town.On 5 October 2021, Watson was sacked by Billericay after Watson led them to one win and one draw from eight matches to start the season.\n\nPersonal life\nWatson has been involved in greyhound racing, pairing up with Tottenham Hotspur teammate Stuart Nethercott to buy Elegant Brandy, who won the 1995 Grand National.\n\nHonours\nRotherham United\nFootball League Division Three runner-up: 1999–2000\nFootball League Division Two runner-up: 2000–01Reading\n\nFootball League Division Two runner-up: 2001–02Colchester United\n\nFootball League One runner-up: 2005–06", "answers": ["Edward Watson"], "length": 4625, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "81cd47ec621f2228d8bb4ec351ffd6b23d23107e5b287ffb"}
{"input": "What is the date of death of Humphrey De Bohun, 7Th Earl Of Hereford's father?", "context": "Passage 1:\nHenry de Bohun\nSir Henry de Bohun (died 23 June 1314) was an English knight, of Anglo-Norman origins, the grandson of Humphrey de Bohun, 2nd Earl of Hereford. He was killed on the first day of the Battle of Bannockburn by Robert the Bruce.\nRiding in the vanguard of heavy cavalry, de Bohun caught sight of the Scottish king who was mounted on a small palfrey (ane gay palfray Li till and joly) armed only with a battle-axe.De Bohun lowered his lance and charged, but Bruce stood his ground, riding on towards the English knight. The two men sped towards each other (Sprent thai samyn intill a ling). At the last moment Bruce manoeuvred his mount nimbly to one side, stood up in his stirrups and hit de Bohun so hard with his axe that it cut through both Sir Henry's helmet and skull and into his brain (That ner the heid till the harnys clave). Despite the great risk the King had taken, he merely expressed regret that he had broken the shaft of his favourite axe.An iconic description and picture of the death of Henry de Bohun is contained in Scotland's Story by H. E. Marshall.\nPassage 2:\nEleanor de Bohun\nEleanor de Bohun (c. 1366 – 3 October 1399) was the elder daughter and co-heiress (with her sister, Mary de Bohun), of  Humphrey de Bohun, 7th Earl of Hereford (1341–1373) and Joan Fitzalan, a daughter of Richard FitzAlan, 10th Earl of Arundel and his second wife Eleanor of Lancaster.\n\nMarriage\nIn 1376, Eleanor married Thomas of Woodstock, 1st Duke of Gloucester. Thomas was the youngest son of Edward III of England and Philippa of Hainault. Following their marriage, the couple went to reside in Pleshey Castle, Essex. According to Jean Froissart, Eleanor and her husband had the tutelage of her younger sister, Mary, who was being instructed in religious doctrine in the hope that she would enter a convent, thus leaving her share of the considerable Bohun inheritance to Eleanor and Thomas.\n\nIssue\nTogether Eleanor and Thomas had five children:\n\nHumphrey, 2nd Earl of Buckingham (c. 1381/1382 – 2 September 1399)\nAnne of Gloucester (c. 1383 – 1438) married (1st) Thomas Stafford, 3rd Earl of Stafford; (2nd) Edmund Stafford, 5th Earl of Stafford; and (3rd) William Bourchier, Count of Eu. Her son by 3rd marriage, John Bourchier, 1st Baron Berners, was grandfather of Richard Neville, 2nd Baron Latimer of Snape.\nJoan (1384 – 16 August 1400) married Gilbert Talbot, 5th Baron Talbot (1383–1419). Died in childbirth.\nIsabel (12 March 1385/1386 – April 1402), became a Minoress, later abbess, in a religious house near Aldgate\nPhilippa (c. 1388) Died young\n\nOrder of the Garter\nEleanor de Bohun was made a Lady of the Garter in 1384. She became a nun sometime after 1397 at Barking Abbey. Prior to her death, Eleanor divided her holdings among her children.  She died on 3 October 1399 and was buried in Westminster Abbey. Her executors included the chaplain in Pleshy, Essex.\n\nIn fiction\nEleanor appears briefly in Anya Seton's historical romance Katherine, based upon the life of Eleanor's sister-in-law Katherine Swynford, the third wife of John of Gaunt. She also appears in Act 1, Scene 2 of Shakespeare's Richard II, where she unsuccessfully urges John of Gaunt to avenge her murdered husband.\n\nNotes\nPassage 3:\nJohn de Ferrers, 1st Baron Ferrers of Chartley\nJohn de Ferrers, 1st Baron Ferrers of Chartley (20 June 1271 Cardiff – 1312) was the son of Robert de Ferrers, 6th Earl of Derby and Alianore de Bohun, daughter of Humphrey de Bohun and Eleanor de Braose, and granddaughter of Humphrey de Bohun, 2nd Earl of Hereford. He was both Seneschal of Gascony and Lieutenant of Aquitaine in 1312, the year of his death.Ferrers joined the baronial opposition to King Edward in 1297, but was summoned as a baron in 1299.He married Hawise de Muscegros, a daughter of Robert de Muscegros.Their eldest son John (died by 1324) inherited the title Baron Ferrers of Chartley upon his father's death from poisoning in Gascony in 1312.\nPassage 4:\nHumphrey de Bohun, 7th Earl of Hereford\nHumphrey de Bohun, 7th Earl of Hereford, 6th Earl of Essex, 2nd Earl of Northampton, KG (March 25, 1342 – January 16, 1373) was the son of William de Bohun, 1st Earl of Northampton, and Elizabeth de Badlesmere, and grandson of Humphrey de Bohun, 3rd Earl of Hereford, by Elizabeth of Rhuddlan, daughter of King Edward I. He became heir to the Earldom of Hereford after the death of his childless uncle Humphrey de Bohun, 6th Earl of Hereford.\nFollowing King Peter I's visit to England, Humphrey participated in the sack of Alexandria in 1365.His wife and the mother of his daughters was Joan Fitzalan, daughter of Richard Fitzalan, 10th Earl of Arundel, and Eleanor of Lancaster, whom he married after 9 September 1359.\nOn his death, his estates were inherited by his two surviving daughters and his titles went into abeyance: \n\nEleanor de Bohun (1366 - 3 October 1399); married Thomas of Woodstock, 1st Duke of Gloucester, youngest son of Edward III; mother of Anne of Gloucester.\nMary de Bohun, who married Henry Bolingbroke, the future King Henry IV of England.\nElizabeth de Bohun, died young.\nPassage 5:\nJohn de Bohun, 5th Earl of Hereford\nJohn de Bohun, 5th Earl of Hereford (23 November 1306 – 20 January 1336) was born in St Clement's, Oxford to Humphrey de Bohun, 4th Earl of Hereford and Elizabeth of Rhuddlan, a daughter of Edward I of England.\nAfter his father's death at the Battle of Boroughbridge, the family lands were forfeited. It was not until after the fall of the Despensers that John was permitted to succeed to his inherited position as Earl of Hereford and Essex, hereditary Constable of England, and Lord of Brecknock.\n\nMarriages\nHe married firstly, in 1325, to Alice FitzAlan (died 1326), daughter of Edmund FitzAlan, 2nd Earl of Arundel, and secondly to Margaret Basset (died 1355). After the marriage, it was discovered that the couple were related to the fourth degree of consanguinity and they were forced to live apart. An appeal to Pope John XXII resulted on 19 February 1331 in a papal commission to the bishops of Lichfield and London to hold an enquiry into the case. However, Roger Northburgh, the Bishop of Coventry and Lichfield, failed to act and the case was still pending when the Pope issued a further demand for an enquiry in 1334.\n\nDeath\nHe did not play much of a public role, despite his high titles and offices, most likely because he had some sort of incapacity. His younger brothers were often deputed to fulfil his duties as Constable. He died at Kirkby Thore, Westmorland and was interred in Stratford Langthorne Abbey, London.\n\n\n== Notes ==\nPassage 6:\nEleanor de Braose\nEleanor de Braose (c. 1228–1251) was a Cambro-Norman noblewoman and a wealthy co-heiress of her father, who was the powerful Marcher lord William de Braose, and of her mother, Eva Marshal, a co-heiress of the Earls of Pembroke. Her husband was Humphrey de Bohun, heir of the 2nd Earl of Hereford, by whom she had children, including Humphrey de Bohun, 3rd Earl of Hereford and Gilbert de Bohun.\n\nFamily\nEleanor was born in about 1228. She was the youngest of four daughters and a co-heiress of the powerful Marcher lord William de Braose, and Eva Marshal, both of whom held considerable lordships and domains in the Welsh Marches and Ireland. Eva was one of the daughters of William Marshal, 1st Earl of Pembroke by Isabel de Clare, 4th Countess of Pembroke, daughter of Richard de Clare, 2nd Earl of Pembroke, \"Strongbow\". Eleanor's three sisters were Isabella de Braose, Maud de Braose, Baroness Mortimer, and Eva de Braose, wife of William de Cantelou.While Eleanor was a young girl, her father - known to the Welsh as Gwilym Ddu (Black William) - was hanged on the orders of Llewelyn the Great, Prince of Wales for alleged adultery with Llewelyn's wife, Joan, Lady of Wales. Following the execution, her mother held de Braose lands and castles in her own right.\n\nMarriage and issue\nOn an unknown date after August 1241, Eleanor became the first wife of Humphrey de Bohun, the son of Humphrey de Bohun, 2nd Earl of Hereford and Maud de Lusignan. The marriage took place after the death of Humphrey's mother, Maud.Humphrey and Eleanor had the following children:\n\nHumphrey de Bohun, 3rd Earl of Hereford (c.1249- 31 December 1298), married Maud de Fiennes, daughter of Enguerrand II de Fiennes and Isabelle de Conde, by whom he had issue, including Humphrey de Bohun, 4th Earl of Hereford.\nGilbert de Bohun.(b.1251 - 1297) married Margarite had issue Gilbert (b.1302 d. 1381)His brother granted him Eleanor's lands in Ireland.\nEleanor de Bohun (died 20 February 1314, buried Walden Abbey). She married Robert de Ferrers, 6th Earl of Derby on 26 June 1269. They had at least two sons and one daughter.\nMargery de Bohun (fl.1265 – 1280) married Theobald de Verdun and had a son also Theobald de Verdun, both of whom were hereditary Constables of Ireland.Eleanor died in 1251, and was buried at Llanthony Secunda Priory. She passed on her considerable possessions in the Welsh Marches to her eldest son Humphrey. Her husband survived her, married Joan de Quincy, and died in 1265.\n\nNotes\nPassage 7:\nWilliam de Bohun, 1st Earl of Northampton\nWilliam de Bohun, 1st Earl of Northampton, KG (c. 1312 – 16 September 1360) was an English nobleman and military commander.\n\nLineage\nHe was the fifth son of Humphrey de Bohun, 4th Earl of Hereford and Elizabeth of Rhuddlan. He had a twin brother, Edward. His maternal grandparents were Edward I of England and his first wife, Queen consort Eleanor of Castile.\n\nLife\nWilliam de Bohun assisted at the arrest of Roger Mortimer in 1330, allowing Edward III to take power. After this, he was a trusted friend and commander of the king and he participated in the renewed wars with Scotland.In 1332, he received many new properties: Hinton and Spaine in Berkshire; Great Haseley, Ascott, Deddington, Pyrton and Kirtlington in Oxfordshire; Wincomb in Buckinghamshire; Longbenington in Lincolnshire; Kneesol in Nottinghamshire; Newnsham in Gloucestershire, Wix in Essex, and Bosham in Sussex.\nIn 1335, he married Elizabeth de Badlesmere (1313 – 8 June 1356). Her parents Bartholomew de Badlesmere, 1st Baron Badlesmere, and Margaret de Clare had both turned against Edward II the decade before. Elizabeth and William were granted some of the property of Elizabeth's first husband, who had also been Mortimer's son and heir.\nWilliam was created Earl of Northampton in 1337, one of the six earls created by Edward III to renew the ranks of the higher nobility. Since de Bohun was a younger son, and did not have an income suitable to his rank, he was given an annuity until suitable estates could be found.\nIn 1349 he became a Knight of the Garter. He served as High Sheriff of Rutland from 1349 until his death in 1360.\n\nCampaigns in Flanders, Brittany, Scotland, Sluys and Crecy\nIn 1339 he accompanied the King to Flanders.  He served variously in Brittany and in Scotland, and was present at the great English victories at Sluys and Crécy, the latter as a commander. His most stunning feat was leading an English force to victory against a much bigger French force at the Battle of Morlaix in 1342. Some of the details are in dispute, but it is clear that he made good use of pit traps, which stopped the French cavalry.\n\nDiplomat\nIn addition to being a warrior, William was also a renowned diplomat.  He negotiated two treaties with France, one in 1343 and one in 1350. He was also charged with negotiating in Scotland for the freedom of King David Bruce, King of Scots, who was held prisoner by the English.\n\nSenior naval command\nFrom the 8 March 1352 to 5 March 1355 he was appointed Admiral of the Northern Seas, Fleet.\n\nIssue\n1. Humphrey de Bohun, 7th Earl of Hereford (1341–1373)\n2. Elizabeth de Bohun (c. 1350–1385); married Richard FitzAlan, 4th Earl of Arundel\n\nIn historical fiction\nIn Bernard Cornwell's series The Grail Quest, the Earl of Northampton plays a minor role as protagonist Thomas of Hookton's lord. The Earl of Northampton also appears in Dan Jones' debut novel The Essex Dogs.\n\nExternal links\nInquisition Post Mortem William de Bohun's IPM #168 and his wife Elizabeth de Bohun #169 follows Inquisition Post Mortem.\n\nAncestry\nPassage 8:\nHumphrey de Bohun, 3rd Earl of Hereford\nHumphrey (VI) de Bohun (c. 1249 – 31 December 1298), 3rd Earl of Hereford and 2nd Earl of Essex, was an English nobleman known primarily for his opposition to King Edward I over the Confirmatio Cartarum. He was also an active participant in the Welsh Wars and maintained for several years a private feud with the earl of Gloucester. His father, Humphrey (V) de Bohun, fought on the side of the rebellious barons in the Barons' War. When Humphrey (V) predeceased his father, Humphrey (VI) became heir to his grandfather, Humphrey (IV). At Humphrey (IV)'s death in 1275, Humphrey (VI) inherited the earldoms of Hereford and Essex. He also inherited major possessions in the Welsh Marches from his mother, Eleanor de Braose.\nBohun spent most of his early career reconquering marcher lands captured by Llywelyn ap Gruffudd during the Welsh war in England. This was finally accomplished through Edward I's war in Wales in 1277. Hereford also fought in Wales in 1282–83 and 1294–95. At the same time he also had private feuds with other marcher lords, and his conflict with Gilbert de Clare, Earl of Gloucester, eventually ended with the personal intervention of King Edward himself. Hereford's final years were marked by the opposition he and Roger Bigod, Earl of Norfolk, mounted against the military and fiscal policy of Edward I. The conflict escalated to a point where civil war threatened, but was resolved when the war effort turned towards Scotland. The king signed the Confirmatio Cartarum—a confirmation of Magna Carta—and Bohun and Bigod agreed to serve on the Falkirk Campaign. Bohun died in 1298, and was succeeded by his son, Humphrey de Bohun, 4th Earl of Hereford.\n\nFamily background and inheritance\nHumphrey (VI) de Bohun was part of a line of Anglo-Norman aristocrats going back to the Norman Conquest, most of whom carried the same name. His grandfather was Humphrey (IV) de Bohun, who had been part of the baronial opposition of Simon de Montfort, but later gone over to the royal side. He was taken prisoner at the Battle of Lewes in May 1264, but was restored to favour after the royalist victory at the Battle of Evesham the next year. Humphrey (IV)'s son, Humphrey (V) de Bohun, remained loyal to the baronial side throughout the Barons' War, and was captured at Evesham on 4 August 1265. In October of that year, Humphrey (V) died in captivity at Beeston Castle in Cheshire from injuries he had sustained in the battle.Humphrey (V) had been excluded from succession as a result of his rebellion, but when Humphrey (IV) died in 1275, Humphrey (VI) inherited the earldoms of Hereford and Essex. Humphrey (VI) had already served as deputy Constable of England under Humphrey (IV). Humphrey (IV) had reserved the honour of Pleshey for his younger son Henry, but the remainder of his lands went to Humphrey (VI). The inheritance Humphrey (VI) received—in addition to land in Essex and Wiltshire from Humphrey (IV)—also consisted of significant holdings in the Welsh Marches from his mother. His mother Eleanor was a daughter and coheir of William de Braose and his wife Eva Marshal, who in turn was the daughter and co-heir of William Marshal, regent to Henry III.Since Humphrey (VI) was only sixteen years old at the time of his father's death, the Braose lands were taken into the king's custody until 1270. Part of this inheritance, the marcher lordship of Brecon, was in the meanwhile given to the custody of Gilbert de Clare, Earl of Hertford. Humphrey technically regained his lordship from Clare in 1270, but by this time these lands had effectively been taken over by the Welsh prince Llywelyn ap Gruffudd, who had taken advantage of the previous decade's political chaos in England to extend his territory into the Marches.He granted his brother Sir Gilbert de Bohun all of their mother's lands in Ireland and some land in England and Wales.\n\nWelsh Wars\nOver the next years, much of Hereford's focus was on reconquering his lost lands in the Marches, primarily through private warfare against Llywelyn. Henry III died in 1272, while his son—now Edward I—was crusading; Edward did not return until 1274. Llywelyn refused to pay homage to the new king, partly because of the military actions of Bohun and other marcher lords, which Llywelyn saw as violations of the Treaty of Montgomery. On 12 November 1276, Hereford was present at a royal assembly where judgment was passed on Llewelyn, and in 1277, Edward I declared war on the Welsh prince. Rebellion in his own Brecon lands delayed Hereford's participation in the early days of the Welsh war. He managed, however, to both suppress the rebellion and conquer lands further west. He then joined up with the royal army and served for a while in Anglesey, before returning to Brecon, where he received the surrender of certain Welsh lords. After the campaign was over, on 2 January 1278, he received protection from King Edward to go on pilgrimage to Santiago de Compostela in Spain.In 1282, war with Wales broke out again; this time it would not be simply a punitive campaign, but a full-scale war of conquest. Initially, the king wanted to fight the war with paid forces, but the nobility insisted on the use of the feudal summons. To men like Hereford, this was preferable, because as part of a feudal army the participants would have both a stake in the war and a justifiable claim on conquered land. In the end, although the earls won, none of them were paid for the war effort. Hereford jealously guarded his authority as hereditary Constable of England, and protested vigorously when the Gilbert de Clare, Earl of Gloucester was appointed commander of the forces in South Wales. In the post-war settlement, however, neither Hereford nor Gloucester received any significant rewards of land, the way several other magnates did. Hereford fought again in Wales, in the suppression of the rebellion of 1294–95, when he again had to pacify the territory of Brecon before joining the king in the north.\n\nPrivate war in the Marches\nParallel with the Welsh Wars, Hereford was also struggling to assert his claims to lands in the Marches against other marcher lords. In 1284 Edward I granted the hundred of Iscennen in Carmarthenshire to John Giffard. Hereford believed the land belonged to him by right of conquest, and started a campaign to win the lands back, but the king took Giffard's side. Problems also arose with the earl of Gloucester. As Gloucester's former ward, Hereford had to buy back his own right of marriage, but Gloucester claimed he had not received the full sum. There was also remaining resentment on Hereford's part for his subordination to Gloucester in the 1282–83 campaign. The conflict came to a head when Gloucester's started construction of a castle at Morlais, which Hereford claimed was his land. In 1286, the Crown ordered Gloucester to cease, but to no avail.It had long been established Marches custom to solve conflicts through private warfare. Hereford's problem, however, was his relative weakness in the Marches, and now he was facing open conflict with two different enemies. He, therefore, decided to take the issue to the king instead, in a break with tradition. King Edward again ordered Gloucester to stop, but the earl ignored the order and initiated raids on Hereford's lands. Hostilities continued and Hereford responded, until both earls were arrested and brought before the king. The real offence was not the private warfare in itself, but the fact that the earls had not respected the king's injunction to cease. In the parliament of January 1292, Gloucester was fined 10,000 marks and Hereford 1,000. Gloucester's liberty of Glamorgan was declared forfeit, and confiscated by the crown, as was Hereford's of Brecon.In the end, the fines were never paid, and the lands were soon restored. Edward had nevertheless demonstrated an important point. After the conquest of Wales, the strategic position of the Marches lordships was less vital to the English crown, and the liberty awarded to the marcher lords could be curtailed. For Edward this was, therefore, a good opportunity to assert the royal prerogative, and to demonstrate that it extended also into the Marches of Wales.\n\nOpposition to Edward I\nIn 1294 the French king declared the English duchy of Aquitaine forfeit, and war broke out between the two countries. Edward I embarked on a wide-scale and costly project of building alliances with other princes on the Continent, and preparing an invasion. When the king, at the parliament of March 1297 in Salisbury, demanded military service from his earls, Roger Bigod, Earl of Norfolk, refused in his capacity of marshal of England. The argument was that the king's subjects were not obliged to serve abroad if not in the company of the king, but Edward insisted on taking his army to Flanders while sending his earls to Gascony.\n  \nAt the time of the Salisbury parliament, Hereford was accompanying two of the king's daughters to Brabant, and could not be present. On his return, however, as Constable of England, he joined Bigod in July in refusing to perform feudal service. The two earls were joined in their opposition by the earls of Arundel and Warwick. The main reason for the magnates' defiance was the heavy burden of taxation caused by Edward's continuous warfare in Wales, France and Scotland. In this they were also joined by Robert Winchelsey, the Archbishop of Canterbury, who was in the midst of an ongoing dispute with the king over clerical taxation. At one point Bohun and Bigod turned up in person at the Exchequer to protest a tax they claimed did not have the consent of the community of the realm. For Hereford there was also a personal element in the opposition to the king, after the humiliation and the affront to his liberties he had suffered over the dispute in the Marches. At a meeting just outside London, Bohun gave an impassioned speech objecting to the king's abuse of power and demanding the restoration of ancient liberties. The grievances were summarised in a document known as the Remonstrances.Neither party showed any inclination to back down, and the nation seemed on the brink of another civil war. Just as the conflict was coming to a head, however, external events intervened to settle it. In September 1297, the English suffered a heavy defeat to the Scots at the Battle of Stirling Bridge. The Scottish victory exposed the north of England to Scottish raids led by William Wallace. The war with Scotland received wider support from the English magnates, now that their own homeland was threatened, than did the war in France to protect the king's continental possessions. Edward abandoned his campaign in France and negotiated a truce with the French king. He agreed to confirm Magna Carta in the so-called Confirmatio Cartarum (Confirmation of the Charters). The earls consequently consented to serve with the king in Scotland, and Hereford was in the army that won a decisive victory over the Scots in the Battle of Falkirk in 1298. Hereford, not satisfied that the king had upheld the charter, withdrew after the battle, forcing Edward to abandon the campaign.\n\nDeath and family\nIn 1275 Bohun married Maud de Fiennes, daughter of Enguerrand de Fiennes, chevalier, seigneur of Fiennes, by his 2nd wife, Isabel (kinswoman of Queen Eleanor of Provence). She predeceased him, and was buried at Walden Priory in Essex. Hereford himself died at Pleshey Castle on 31 December 1298, and was buried at Walden alongside his wife. They had one son Humphrey de Bohun, 4th Earl of Hereford, born around 1276. The son was given possession of his father's lands and titles on 16 February 1299. The young Humphrey also inherited his father's title of Constable of England.A common theme in Humphrey de Bohun's actions was his fierce protection of what he regarded as his feudal privileges. His career was marked by turbulence and political strife, particularly in the Marches of Wales, but eventually he left a legacy of consolidated possessions there. In 1297, at the height of the conflict between Edward I and rebellious barons, the king had actively tried to undermine Hereford's authority in the Marches, but failed due to the good relations the earl enjoyed with the local men.\n\nNotes\nPassage 9:\nHumphrey de Bohun, 2nd Earl of Hereford\nHumphrey IV de Bohun, 2nd Earl of Hereford, 1st Earl of Essex (1204 – 24 September 1275) was an Anglo-Norman nobleman and soldier who served as hereditary Constable of England.\n\nOrigins\nHe was the eldest son and heir of Henry de Bohun, 1st Earl of Hereford (1176–1220) by his wife Maud de Mandeville (alias Maud FitzGeoffrey), daughter and heiress of Geoffrey Fitz Peter, 1st Earl of Essex.\n\nCareer\nHe was one of the nine godfathers of Prince Edward, the future King Edward I. He served as Sheriff of Kent for 1239–40. In 1258, after returning from a pilgrimage to the Holy Land, Humphrey fell away, like his father, from the royal cause to that of the barons. He served as a nominee of the opposition on the \"committee of twenty-four\" which was appointed in the Oxford Parliament of that year, to create the Provisions of Oxford to reform the administration. The alliance of Simon de Montfort with Llywelyn ap Gruffudd of North Wales brought Bohun back to royal allegiance. He headed the first secession of the Welsh Marchers from the party of the opposition (1263), and was among the captives whom the Montfortians took at the Battle of Lewes in 1264.He was amongst the victors at the Battle of Evesham in 1265, which extinguished the power of de Montfort, at which, however, his eldest son, Humphrey V de Bohun, was mortally wounded. Humphrey was selected as one of the twelve arbitrators to draw up the Dictum of Kenilworth (1266), by which the disinherited rebels were allowed to make their peace.\n\nMarriages and issue\nHe married twice:\n\nFirstly, in about 1236, to Maud de Lusignan (c. 1210 – 14 August 1241), daughter of Raoul I of Lusignan, Comte d'Eu, second husband of Alix d'Eu, 8th Comtesse d'Eu. She died in 1241 and was buried at Llanthony, Gloucester, together with her husband. By Maud he had issue including:\nHumphrey V de Bohun (died 1265), eldest son and heir apparent, who predeceased his father, having shared with him in the victory at the Battle of Evesham (1264), which he did not long survive. The earldom, therefore, passed to his son Humphrey VI de Bohun, 3rd Earl of Hereford, 2nd Earl of Essex (c. 1249 – 1298).\nHenry de Bohun\nGeoffrey de Bohun\nRalph de Bohun, Cleric;\nMaud de Bohun, who married firstly Anselm Marshal, 6th Earl of Pembroke; secondly Roger de Quincy, 2nd Earl of Winchester;\nAlice de Bohun, who married Roger V de Toeni;\nEleanor de Bohun, who married Sir John de Verdun, Baron of Westmeath\nSecondly, he married Maud de Avenbury (died 8 October 1273), by whom he had two further sons:\nJohn de Bohun\nSir Miles de Bohun\n\nDeath and burial\nHe died in 1275 in Warwickshire and was buried at Llanthony Secunda in Gloucester. He was succeeded by his grandson Humphrey VI de Bohun (c. 1249 – 1298).\n\nNotes\nPassage 10:\nHenry de Bohun, 1st Earl of Hereford\nHenry de Bohun, 1st Earl of Hereford (1176 – 1 June 1220) of Pleshy Castle in Essex, was an Anglo-Norman nobleman who became Hereditary Constable of England from 1199.\n\nOrigins\nHe was the son and heir of Humphrey III de Bohun (pre-1144-1181) \nof Trowbridge Castle in Wiltshire and of Caldicot Castle in south-east Wales, 5th feudal baron of Trowbridge, who served King Henry II as Lord High Constable of England. His mother was Margaret of Huntingdon, widow of Conan IV, Duke of Brittany (d.1171) and a daughter of Henry of Scotland, 3rd Earl of Northumberland, 3rd Earl of Huntingdon, son of King David I of Scotland by his wife Maud, 2nd Countess of Huntingdon. Henry's half-sister was Constance, Duchess of Brittany.\n\nEarldom\nHis paternal grandmother was Margaret of Hereford, a daughter of Miles FitzWalter of Gloucester, 1st Earl of Hereford, Lord of Brecknock (died 1143), Sheriff of Gloucester and Constable of England. After the male line of Miles of Gloucester failed, in 1199 King John created Henry de Bohun Earl of Hereford and Constable of England. His lands lay chiefly on the Welsh Marches, and from this date the Bohuns took a foremost place among the Marcher barons.Henry de Bohun was one of the twenty-five barons elected by their peers to enforce the terms of Magna Carta in 1215. He was subsequently excommunicated by the Pope. In the civil war that followed Magna Carta, he was a supporter of King Louis VIII of France and was captured at the Battle of Lincoln in 1217.\n\nMarriage and issue\nHe married Maud de Mandeville (alias Maud FitzGeoffrey), daughter and heiress of Geoffrey Fitz Peter, 1st Earl of Essex, of Pleshy Castle in Essex, by whom he had issue including:\n\nHumphrey IV de Bohun, 2nd Earl of Hereford, 1st Earl of Essex (1204-1275), eldest son and heir, created Earl of Essex in 1239, who married Maud de Lusignan, by whom he had at least three children.\nHenry de Bohun, who died young.\nRalph de Bohun.\n\nDeath\nHe died in June 1220 while on crusade to the Holy Land.", "answers": ["16 September 1360"], "length": 5001, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "048e82b64b5651b74d452db7151c2110a718128dfd12a774"}
{"input": "Where does the director of film Wine Of Morning work at?", "context": "Passage 1:\nDana Blankstein\nDana Blankstein-Cohen (born March 3, 1981) is the executive director of the Sam Spiegel Film and Television School. She was appointed by the board of directors  in November 2019.  Previously she was the CEO of the Israeli Academy of Film and Television. She is a film director, and an Israeli culture entrepreneur.\n\nBiography\nDana Blankstein was born in Switzerland in 1981 to theatre director Dedi Baron and Professor Alexander Blankstein. She moved to Israel in 1983 and grew up in Tel Aviv.\nBlankstein graduated from the Sam Spiegel Film and Television School, Jerusalem in 2008 with high honors. During her studies she worked as a personal assistant to directors Savi Gabizon on his film Nina's Tragedies and to Renen Schorr on his film The Loners.  She also directed and shot 'the making of' film on Gavison's film Lost and Found. Her debut film Camping competed at the Berlin International Film Festival, 2007.\n\nFilm and academic career\nAfter her studies, Dana founded and directed the film and television department at the Kfar Saba municipality. The department encouraged and promoted productions filmed in the city of Kfar Saba, as well as the established cultural projects, and educational community activities.\nBlankstein directed the mini-series \"Tel Aviviot\" (2012). From 2016-2019 was the director of the Israeli Academy of Film and Television.\nIn November 2019 Dana Blankstein Cohen was appointed the new director of the Sam Spiegel Film and Television School where she also oversees the Sam Spiegel International Film Lab. In 2022, she spearheaded the launch of the new Series Lab and the film preparatory program for Arabic speakers in east Jerusalem.\n\nFilmography\nTel Aviviot (mini-series; director, 2012)\nGrowing Pains (graduation film, Sam Spiegel; director and screenwriter, 2008)\nCamping (debut film, Sam Spiegel; director and screenwriter, 2006)\nPassage 2:\nIan Barry (director)\nIan Barry is an Australian director of film and TV.\n\nSelect credits\nWaiting for Lucas (1973) (short)\nStone (1974) (editor only)\nThe Chain Reaction (1980)\nWhose Baby? (1986) (mini-series)\nMinnamurra (1989)\nBodysurfer (1989) (mini-series)\nRing of Scorpio (1990) (mini-series)\nCrimebroker (1993)\nInferno (1998) (TV movie)\nMiss Lettie and Me (2002) (TV movie)\nNot Quite Hollywood: The Wild, Untold Story of Ozploitation! (2008) (documentary)\nThe Doctor Blake Mysteries (2013)\nPassage 3:\nJason Moore (director)\nJason Moore (born October 22, 1970) is an American director of film, theatre and television.\n\nLife and career\nJason Moore was born in Fayetteville, Arkansas, and studied at Northwestern University. Moore's Broadway career began as a resident director of Les Misérables at the Imperial Theatre in during its original run. He is the son of Fayetteville District Judge Rudy Moore.In March 2003, Moore directed the musical Avenue Q, which opened Off-Broadway at the Vineyard Theatre and then moved to Broadway at the John Golden Theatre in July 2003.  He was nominated for a 2004 Tony Award for his direction. Moore also directed productions of the musical in Las Vegas and London and the show's national tour. Moore directed the 2005 Broadway revival of Steel Magnolias and Shrek the Musical, starring Brian d'Arcy James and Sutton Foster which opened on Broadway in 2008. He directed the concert of Jerry Springer — The Opera at Carnegie Hall in January 2008.Moore, Jeff Whitty, Jake Shears, and John \"JJ\" Garden worked together on a new musical based on Armistead Maupin's Tales of the City. The musical premiered at the American Conservatory Theater, San Francisco, California in May 2011 and ran through July 2011.For television, Moore has directed episodes of Dawson's Creek, One Tree Hill, Everwood, and Brothers & Sisters. As a writer, Moore adapted the play The Floatplane Notebooks with Paul Fitzgerald from the novel by Clyde Edgerton. A staged reading of the play was presented at the New Play Festival at the Charlotte, North Carolina Repertory Theatre in 1996, with a fully staged production in 1998.In 2012, Moore made his film directorial debut with Pitch Perfect, starring Anna Kendrick and Brittany Snow. He also served as an executive producer on the sequel. He directed the film Sisters, starring Tina Fey and Amy Poehler, which was released on December 18, 2015. Moore's next project will be directing a live action Archie movie.\n\nFilmography\nFilms\n\nPitch Perfect (2012)\nSisters (2015)\nShotgun Wedding (2022)Television\n\nSoundtrack writer\n\nPitch Perfect 2 (2015) (Also executive producer)\nThe Voice (2015) (1 episode)\nPassage 4:\nWine of Morning\nWine of Morning is a 1955 American film directed by Katherine Stenholm and starring Al Carter, Joan DeVolk, and Katherine Helmond. It has Barabbas as the subject, who was pardoned according to the Biblical report in place of Jesus Christ by Pontius Pilate.\nThe film is based on the novel Wine of Morning by Bob Jones Jr.\n\nPlot\nOn a stormy voyage Barabbas writes a letter to his friend Stephen and remembers his time in Galilean Nazareth.\nThere he is friends with the carpenter Josef and his son Jesus . One day he meets Irene, the future bride of his friend Stephanus, and falls in love with her. On Stephanus' and Irene's wedding in Cana, the wine goes out surprisingly. Mary asks Jesus for help, who then turns water into wine.\nA little later, Joel decides to leave Nazareth and go to Capernaum. There he visits his friends Sarah and Jonathan and their son Dismas. Jonathan has been paralyzed, so Joel and Dismas supports Jonathan's business. He meets the rabid tax collector Levi and notes bitterly the oppression of the Jews by the Romans. While Joel would like to fight, Jonathan is waiting for salvation from God.\nAfter Dismas one day watched with enthusiasm a demon exorcism by Jesus, he and Joel also bring Jonathan to Jesus. They let Jonathan down through the ceiling of the house to Jesus; Jonathan is healed.\nIn the meantime, Joel meets the stranger Omah, whose family was cruelly killed by the Romans. Omah takes Joel to Jesus; but to Joel's disappointment, Jesus does not choose him as a disciple. Omah recruits Joel for an underground movement fighting for the liberation of Israel.\nIn Jerusalem, Joel meets Prince Manean and is educated by him for six months. While performing a mission, he saves the Egyptian dancer Myra from the advances of Manean's servant Toron. Joel and Myra fall in love.\nManean plans an action against the Romans for the approaching Passover celebration . He gives Joel the name Barabbas, as he proposes to rob the necessary money from the expected pilgrims. A little later, Dismas also joins the group around Manean.\nWhen Toron betrays Barabbas to the Romans, Barabbas escapes and kills Toron. Myra suggests Manean use Toron's funeral to escape together. Soon Barabbas is wanted by the Romans as a robber. A little later, Barabbas is caught in a robbery; Myra is coming.\nAt the same time, Pontius Pilate is presented Jesus as a prisoner accused of blasphemy. Pilate can not blame him, but the people demand Jesus' crucifixion. Pilate leaves the people the choice of whether to pardon Jesus or Barabbas on the occasion of the Passover feast; the people choose Barabbas. Finally, Pilate gives in to the people and condemns Jesus; Barabbas' accomplices Gestas and Dismas are crucified together with Jesus. Shaken, Barabbas follows Jesus' crucifixion on Golgotha . Jesus is mocked by those present; only Dismas holds to him.\nAfter Barabbas' ship is shipwrecked during the storm, Barabbas is taken to Irene, who now has a son named Joel. Irene tells Barabbas of Jesus' resurrection and ascension. She and Stephen were later baptized; Stephen was stoned by the Romans for his faith. Shortly before leaving, Barabbas Irene admits that he once loved her.\nIn Antioch, Barabbas meets Manean, who has converted to Christianity; Barabbas also becomes a Christian. On his journey with Paul Barabbas is arrested and imprisoned. On the occasion of the death of King Herod Agrippa Barabbas has the prospect of being pardoned.\n\nCast\nAl Carter as Joel / Barabbas\nJoan DeVolk as Myra\nKatherine Helmond as Irene\nGeorge Hennix as Omah\nBob Jones Jr. as Pontius Pilate\nBob Jones III as Dismas\nRobert Pratt as Jonathan\nDavid Yearick as Prince Manaen\nHarvey Maddrix as Toron\nHarold Root as Manaen's Servant\nClaire Baker as Captain\nHarry Brown as Mucius, Roman soldier\nHoward Burns as Joseph\nJack Buttram as Stephanus\nGeorge Capps as Levi\nVincent Cervera as Apostle Paul\nBob Davis as Caesarean prison guard\nElizabeth Edwards as Maria\nVelma Eubanks as Rebecca\nDwight Gustafson as Magistrate\nR. K. Johnson as Stephanus' father\nFannie Mae Jones as Sarah\nBill Kinkaid as Barnabas\nGeorge Law as Peter\nBruce Lemmen as Caiaphas\nJohn Ludwig as Priest\nMelba Jo McKenzie as Claudia's maid\nFritz Mollenkott as Priest\nElmer Rumminger as Priest\nJames Ryerson as Fremder\nGlenn Schunk as Irenes Vater\nBilly Shelton as Enos\nClifford Wallace as Zebedee\nZeb Wolfe as Priest\nThomas Woodward as Herod Agrippa\nBob Kendall as Longinus, Roman soldier\nDan Dunkelberger as Vinicius, Roman soldier\nJon Formo as Vestus, Roman soldier\nRoy Lichtenwalter as Sextus, Roman soldier\nBarry Thomas as: Marcus, Roman soldier\nGlenn Zachary as Octavus, Roman soldier\n\nProduction\nThe film was created with the participation of students and staff of Bob Jones University. The basis was the novel Wine of Morning of the University President, Bob Jones, Jr., 1950, who had long been planning to write a novel about Barabbas, but did not find the time to write until a pleurisy forced him into the hospital bed for two months. Six months later, the novel was completed. The novel was finally filmed by Unusual Films; Bob Jones Jr. took over the role of Pontius Pilatus.\nWine of Morning was featured at the International Congress of Motion's Picture and Television School Directors at the Cannes International Film Festival. It was the first film to win the four major awards from the National Evangelical Film Foundation.\nWine of Morning was Katherine Helmond's film debut.\n\nExternal links\nWine of Morning in the Internet Movie Database (English)\nWine of Morning on www.unusualfilms.com\nPassage 5:\nBrian Kennedy (gallery director)\nBrian Patrick Kennedy (born 5 November 1961) is an Irish-born art museum director who has worked in Ireland and Australia, and now lives and works in the United States.  He was the director of the Peabody Essex Museum in Salem for 17 months, resigning December 31, 2020. He was the director of the Toledo Museum of Art in Ohio from 2010 to 2019. He was the director of the Hood Museum of Art from 2005 to 2010, and the National Gallery of Australia (Canberra) from 1997 to 2004.\n\nCareer\nBrian Kennedy currently lives and works in the United States after leaving Australia in 2005 to direct the Hood Museum of Art at Dartmouth College. In October 2010 he became the ninth Director of the Toledo Museum of Art. On 1 July 2019, he succeeded Dan Monroe as the executive director and CEO of the Peabody Essex Museum.\n\nEarly life and career in Ireland\nKennedy was born in Dublin and attended Clonkeen College. He received B.A. (1982), M.A. (1985) and PhD (1989) degrees from University College-Dublin, where he studied both art history and history.\nHe worked in the Irish Department of Education (1982), the European Commission, Brussels (1983), and in Ireland at the Chester Beatty Library (1983–85), Government Publications Office (1985–86), and Department of Finance (1986–89). He married Mary Fiona Carlin in 1988.He was Assistant Director at the National Gallery of Ireland in Dublin from 1989 to 1997. He was Chair of the Irish Association of Art Historians from 1996 to 1997, and of the Council of Australian Art Museum Directors from 2001 to 2003. In September 1997 he became Director of the National Gallery of Australia.\n\nNational Gallery of Australia (NGA)\nKennedy expanded the traveling exhibitions and loans program throughout Australia, arranged for several major shows of Australian art abroad, increased the number of exhibitions at the museum itself and oversaw the development of an extensive multi-media site.  Although he oversaw several years of the museum's highest ever annual visitation, he discontinued the emphasis of his predecessor, Betty Churcher, on showing \"blockbuster\" exhibitions.\nDuring his directorship, the NGA gained government support for improving the building and significant private donations and corporate sponsorship. However, the initial design for the building proved controversial generating a public dispute with the original architect on moral rights grounds.  As a result, the project was not delivered during Dr Kennedy's tenure, with a significantly altered design completed some years later.  Private funding supported two acquisitions of British art, including David Hockney's A Bigger Grand Canyon in 1999, and Lucian Freud's After Cézanne in 2001. Kennedy built on the established collections at the museum by acquiring the Holmgren-Spertus collection of Indonesian textiles; the Kenneth Tyler collection of editioned prints, screens, multiples and unique proofs; and the Australian Print Workshop Archive. He was also notable for campaigning for the construction of a new \"front\" entrance to the Gallery, facing King Edward Terrace, which was completed in 2010 (see reference to the building project above).\nKennedy's cancellation of the \"Sensation exhibition\" (scheduled at the NGA from 2 June 2000 to 13 August 2000) was controversial, and seen by some as censorship. He claimed that the decision was due to the exhibition being \"too close to the market\" implying that a national cultural institution cannot exhibit the private collection of a speculative art investor. However, there were other exhibitions at the NGA during his tenure, which could have raised similar concerns. The exhibition featured the privately owned Young British Artists works belonging to Charles Saatchi and attracted large attendances in London and Brooklyn. Its most controversial work was Chris Ofili's The Holy Virgin Mary, a painting which used elephant dung and was accused of being blasphemous. The then-mayor of New York, Rudolph Giuliani, campaigned against the exhibition, claiming it was \"Catholic-bashing\" and an \"aggressive, vicious, disgusting attack on religion.\" In November 1999, Kennedy cancelled the exhibition and stated that the events in New York had \"obscured discussion of the artistic merit of the works of art\". He has said that it \"was the toughest decision of my professional life, so far.\"Kennedy was also repeatedly questioned on his management of a range of issues during the Australian Government's Senate Estimates process - particularly on the NGA's occupational health and safety record and concerns about the NGA's twenty-year-old air-conditioning system. The air-conditioning was finally renovated in 2003. Kennedy announced in 2002 that he would not seek extension of his contract beyond 2004, accepting a seven-year term as had his two predecessors.He became a joint Irish-Australian citizen in 2003.\n\nToledo Museum of Art\nThe Toledo Museum of Art is known for its exceptional collections of European and American paintings and sculpture, glass, antiquities, artist books, Japanese prints and netsuke. The museum offers free admission and is recognized for its historical leadership in the field of art education.  During his tenure, Kennedy has focused the museum's art education efforts on visual literacy, which he defines as \"learning to read, understand and write visual language.\"   Initiatives have included baby and toddler tours, specialized training for all staff, docents, volunteers and the launch of a website, www.vislit.org. In November 2014, the museum hosted the International Visual Literacy Association (IVLA) conference, the first Museum to do so. Kennedy has been a frequent speaker on the topic, including 2010 and 2013 TEDx talks on visual and sensory literacy.\nKennedy has expressed an interest in expanding the museum's collection of contemporary art and art by indigenous peoples. Works by Frank Stella, Sean Scully, Jaume Plensa, Ravinder Reddy and Mary Sibande have been acquired.  In addition, the museum has made major acquisitions of Old Master paintings by Frans Hals and Luca Giordano.During his tenure the Toledo Museum of Art has announced the return of several objects from its collection due to claims the objects were stolen and/or illegally exported prior being sold to the museum.  In 2011 a Meissen sweetmeat stand was returned to Germany followed by an Etruscan Kalpis or water jug to Italy (2013), an Indian sculpture of Ganesha (2014) and an astrological compendium to Germany in 2015.\n\nHood Museum of Art\nKennedy became Director of the Hood Museum of Art in July 2005. During his tenure, he implemented a series of large and small-scale exhibitions and oversaw the production of more than 20 publications to bring greater public attention to the museum's remarkable collections of the arts of America, Europe, Africa, Papua New Guinea and the Polar regions. At 70,000 objects, the Hood has one of the largest collections on any American college of university campus. The exhibition, Black Womanhood: Images, Icons, and Ideologies of the African Body, toured several US venues. Kennedy increased campus curricular use of works of art, with thousands of objects pulled from storage for classes annually. Numerous acquisitions were made with the museum's generous endowments, and he curated several exhibitions: including Wenda Gu: Forest of Stone Steles: Retranslation and Rewriting Tang Dynasty Poetry, Sean Scully: The Art of the Stripe, and Frank Stella: Irregular Polygons.\n\nPublications\nKennedy has written or edited a number of books on art, including:\n\nAlfred Chester Beatty and Ireland 1950-1968: A study in cultural politics, Glendale Press (1988), ISBN 978-0-907606-49-9\nDreams and responsibilities: The state and arts in independent Ireland, Arts Council of Ireland (1990), ISBN 978-0-906627-32-7\nJack B Yeats: Jack Butler Yeats, 1871-1957 (Lives of Irish Artists), Unipub (October 1991), ISBN 978-0-948524-24-0\nThe Anatomy Lesson: Art and Medicine (with Davis Coakley), National Gallery of Ireland (January 1992), ISBN 978-0-903162-65-4\nIreland: Art into History (with Raymond Gillespie), Roberts Rinehart Publishers (1994), ISBN 978-1-57098-005-3\nIrish Painting, Roberts Rinehart Publishers (November 1997),  ISBN 978-1-86059-059-7\nSean Scully: The Art of the Stripe, Hood Museum of Art (October 2008), ISBN 978-0-944722-34-3\nFrank Stella: Irregular Polygons, 1965-1966, Hood Museum of Art (October 2010), ISBN 978-0-944722-39-8\n\nHonors and achievements\nKennedy was awarded the Australian Centenary Medal in 2001 for service to Australian Society and its art. He is a trustee and treasurer of the Association of Art Museum Directors, a peer reviewer for the American Association of Museums and a member of the International Association of Art Critics. In 2013 he was appointed inaugural eminent professor at the University of Toledo and received an honorary doctorate from Lourdes University. Most recently, Kennedy received the 2014 Northwest Region, Ohio Art Education Association award for distinguished educator for art education.\n\n\n== Notes ==\nPassage 6:\nMichael Govan\nMichael Govan (born 1963) is the director of the Los Angeles County Museum of Art. Prior to his current position, Govan worked as the director of the Dia Art Foundation in New York City.\n\nEarly life and education\nGovan was born in 1963 in North Adams, Massachusetts, and was raised in the Washington D.C. area, attending Sidwell Friends School.He majored in art history and fine arts at Williams College, where he met Thomas Krens, who was then director of the Williams College Museum of Art. Govan became closely involved with the museum, serving as acting curator as an undergraduate. After receiving his B.A. from Williams in 1985, Govan began an MFA in fine arts from the University of California, San Diego.\n\nCareer\nAs a twenty-five year old graduate student, Govan was recruited by his former mentor at Williams, Thomas Krens, who in 1988 had been appointed director of the Solomon R. Guggenheim Foundation. Govan served as deputy director of the Solomon R. Guggenheim Museum under Krens from 1988 to 1994, a period that culminated in the construction and opening of the Frank Gehry designed Guggenheim branch in Bilbao, Spain. Govan supervised the reinstallation of the museum's permanent collection galleries after its extensive renovation.\n\nDia Art Foundation\nFrom 1994 to 2006, Govan was president and director of Dia Art Foundation in New York City. There, he spearheaded the conversion of a Nabisco box factory into the 300,000 square foot Dia:Beacon in New York's Hudson Valley, which houses Dia's collection of art from the 1960s to the present. Built in a former Nabisco box factory, the critically acclaimed museum has been credited with catalyzing a cultural and economic revival within the formerly factory-based city of Beacon. Dia's collection nearly doubled in size during Govan's tenure, but he also came under criticism for \"needlessly and permanently\" closing Dia's West 22nd Street building. During his time at Dia, Govan also worked closely with artists James Turrell and Michael Heizer, becoming an ardent supporter of Roden Crater and City, the artists' respective site-specific land art projects under construction in the American southwest. Govan successfully lobbied Washington to have the 704,000 acres in central Nevada surrounding City declared a national monument in 2015.\n\nLACMA\nIn February 2006, a search committee composed of eleven LACMA trustees, led by the late Nancy M. Daly, recruited Govan to run the Los Angeles County Museum of Art. Govan has stated that he was drawn to the role not only because of LACMA's geographical distance from its European and east coast peers, but also because of the museum's relative youth, having been established in 1961. \"I felt that because of this newness I had the opportunity to reconsider the museum,\" Govan has written, \"[and] Los Angeles is a good place to do that.\"Govan has been widely regarded for transforming LACMA into both a local and international landmark. Since Govan's arrival, LACMA has acquired by donation or purchase over 27,000 works for the permanent collection, and the museum's gallery space has almost doubled thanks to the addition of two new buildings designed by Renzo Piano, the Broad Contemporary Art Museum (BCAM) and the Lynda and Stewart Resnick Pavilion. LACMA's annual attendance has grown from 600,000 to nearly 1.6 million in 2016.\n\nArtist collaborations\nSince his arrival, Govan has commissioned exhibition scenography and gallery designs in collaboration with artists. In 2006, for example, Govan invited LA artist John Baldessari to design an upcoming exhibition about the Belgian surrealist René Magritte, resulting in a theatrical show that reflected the twisted perspective of the latter's topsy-turvy world. Baldessari has also designed LACMA's logo. Since then, Govan has also commissioned Cuban-American artist Jorge Pardo to design LACMA's Art of the Ancient Americas gallery, described in the Los Angeles Times as a \"gritty cavern deep inside the earth ... crossed with a high-style urban lounge.\"Govan has also commissioned several large-scale public artworks for LACMA's campus from contemporary California artists. These include Chris Burden's Urban Light (2008), a series of 202 vintage street lamps from different neighborhoods in Los Angeles, arranged in front of the entrance pavilion, Barbara Kruger's Untitled (Shafted) (2008), Robert Irwin's Primal Palm Garden (2010), and Michael Heizer's Levitated Mass, a 340-ton boulder transported 100 miles from the Jurupa Valley to LACMA, a widely publicized journey that culminated with a large celebration on Wilshire Boulevard. Thanks in part to the popularity of these public artworks, LACMA was ranked the fourth most instagrammed museum in the world in 2016.In his first three full years, the museum raised $251 million—about $100 million more than it collected during the three years before he arrived. In 2010, it was announced that Govan will steer LACMA for at least six more years. In a letter dated February 24, 2013, Govan, along with the LACMA board's co-chairmen Terry Semel and Andrew Gordon, proposed a merger with the financially troubled Museum of Contemporary Art, Los Angeles and a plan to raise $100 million for the combined museum.\n\nZumthor Project\nGovan's latest project is an ambitious building project, the replacement of four of the campus's aging buildings with a single new state of the art gallery building designed by architect Peter Zumthor. As of January 2017, he has raised about $300 million in commitments. Construction is expected to begin in 2018, and the new building will open in 2023, to coincide with the opening of the new D Line metro stop on Wilshire Boulevard. The project also envisages dissolving all existing curatorial departments and departmental collections. Some commentators have been highly critical of Govan's plans. Joseph Giovannini, recalling Govan's technically unrealizable onetime plan to hang Jeff Koons' Train sculpture from the facade of the Ahmanson Gallery, has accused Govan of \"driving the institution over a cliff into an equivalent mid-air wreck of its own\". Describing the collection merging proposal as the creation of a \"giant raffle bowl of some 130,000 objects\", Giovannini also points out that the Zumthor building will contain 33% less gallery space than the galleries it will replace, and that the linear footage of wall space available for displays will decrease by about 7,500 ft, or 1.5 miles. Faced with losing a building named in its honor, and anticipating that its acquisitions could no longer be displayed, the Ahmanson Foundation withdrew its support.\nOn the merging of the separate curatorial divisions to create a non-departmental art museum, Christopher Knight has pointed out that \"no other museum of LACMA's size and complexity does it\" that way, and characterized the museum's 2019 \"To Rome and Back\" exhibition, the first to take place under the new scheme, as \"bland and ineffectual\" and an \"unsuccessful sample of what's to come\".\n\nPersonal life\nGovan is married and has two daughters, one from a previous marriage. He and his family used to live in a $6 million mansion in Hancock Park that was provided by LACMA - a benefit worth $155,000 a year, according to most recent tax filings - until LACMA decided that it would sell the property to make up for the museum's of almost $900 million in debt [2]. That home is now worth nearly $8 million and Govan now lives in a trailer park in Malibu's Point Dume region.\nLos Angeles CA 90020\nUnited States. He has had a private pilot's license since 1995 and keeps a 1979 Beechcraft Bonanza at Santa Monica Airport.\nPassage 7:\nKatherine Stenholm\nKatherine Corne Stenholm (June 19, 1917 – November 3, 2015) was an American film director and the founding director of Unusual Films, the production company of Bob Jones University.\n\nBiography\nKatherine Corne was born and reared in Hendersonville, North Carolina.  As a high school student during the Depression, she supplemented her family's income by writing movie reviews for a local newspaper.  Rejecting a college scholarship to Wellesley, Corne attended the fledgling Bob Jones College in Cleveland, Tennessee, after an evangelist convinced her that a Christian young person should attend a Christian college.  At BJC, she majored in speech and became a private student of Bob Jones Jr., eventually helping him direct Shakespearean plays.  After earning her undergraduate degree, she served on the BJC speech faculty while attending graduate school at Northwestern University for twelve summers. During this period she married Gilbert R. Stenholm (1915–89), who became an influential administrator at the institution; they had one son.In 1950, after the college moved to Greenville and became Bob Jones University, Bob Jones Sr. and Jr. asked Stenholm to head a newly conceived campus film production company, Unusual Films.  Stenholm then attended summer film school at the University of Southern California, making important professional contacts and serving an internship with Stanley Kramer.   Stenholm was a quick learner and soon \"became one of only a handful of women in the United States to direct feature films.\"  Through her career she produced seventy-two films of various types including sermon films, religious documentaries, promotional films, and multi-image presentations.  She directed five feature-length religious films, all costume dramas: \n\nWine of Morning\nRed Runs the River\nFlame in the Wind\nSheffey\nBeyond the Night.The National Evangelical Film Foundation named Stenholm Director of the Year in 1953, 1955, and 1963; and her favorite film, Sheffey, received a Silver Medallion award from the International Film and Television Festival of New York.In 1958, at the height of the Cold War, the University Film Producers Association selected Wine of Morning as its submission to the International Congress of Motion Picture and Television School Directors at the Cannes Film Festival, and Stenholm was the keynote speaker on the occasion. A U.S. State Department official who briefed Stenholm told her there had been a round of applause when the Department discovered that BJU had been chosen to represent the United States because \"Bob Jones University is one school about which there is no worry!\" The selection committee thought Wine of Morning would demonstrate the excellence of American cinema training and the film's frank religious message would \"provide a revealing contrast to the entries from Russia and the other Communist-dominated countries.\"In 1986, Stenholm suffered a stroke in the Soviet Union while taking scenic footage in preparation for another feature-length film.  She retired as director of Unusual Films but continued to teach at BJU until 2001. Stenholm died in November 2015 at the age of 98.\nPassage 8:\nJesse E. Hobson\nJesse Edward Hobson (May 2, 1911 – November 5, 1970) was the director of SRI International from 1947 to 1955. Prior to SRI, he was the director of the Armour Research Foundation.\n\nEarly life and education\nHobson was born in Marshall, Indiana. He received bachelor's and master's degrees in electrical engineering from Purdue University and a PhD in electrical engineering from the California Institute of Technology. Hobson was also selected as a nationally outstanding engineer.Hobson married Jessie Eugertha Bell on March 26, 1939, and they had five children.\n\nCareer\nAwards and memberships\nHobson was named an IEEE Fellow in 1948.\nPassage 9:\nOlav Aaraas\nOlav Aaraas (born 10 July 1950) is a Norwegian historian and museum director.\nHe was born in Fredrikstad. From 1982 to 1993 he was the director of Sogn Folk Museum, from 1993 to 2010 he was the director of Maihaugen and from 2001 he has been the director of the Norwegian Museum of Cultural History. In 2010 he was decorated with the Royal Norwegian Order of St. Olav.\nPassage 10:\nPeter Levin\nPeter Levin is an American director of film, television and theatre.\n\nCareer\nSince 1967, Levin has amassed a large number of credits directing episodic television and television films. Some of his television series credits include Love Is a Many Splendored Thing, James at 15, The Paper Chase, Family, Starsky & Hutch, Lou Grant, Fame, Cagney & Lacey, Law & Order and Judging Amy.Some of his television film credits include Rape and Marriage: The Rideout Case (1980), A Reason to Live (1985), Popeye Doyle (1986), A Killer Among Us (1990), Queen Sized (2008) and among other films. He directed \"Heart in Hiding\", written by his wife Audrey Davis Levin, for which she received an Emmy for Best Day Time Special in the 1970s.\nPrior to becoming a director, Levin worked as an actor in several Broadway productions. He costarred with Susan Strasberg in \"[The Diary of Ann Frank]\" but had to leave the production when he was drafted into the Army. He trained at the Carnegie Mellon University. Eventually becoming a theatre director, he directed productions at the Long Wharf Theatre and the Pacific Resident Theatre Company. He also co-founded the off-off-Broadway Theatre [the Hardware Poets Playhouse] with his wife Audrey Davis Levin and was also an associate artist of The Interact Theatre Company.", "answers": ["Bob Jones University"], "length": 5162, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "be97290f663a83ba27007dd262ca2a6072c9156f775a24ad"}
{"input": "What is the place of birth of Ratna Malla's father?", "context": "Passage 1:\nJayayakshya Malla\nJayayakshya Malla (often named  Yaksha Malla for short) (Nepali: यक्ष मल्ल) was the son of Jayajyotir Malla and the last Malla king of the united Kathmandu Valley from around 1428 until his death in 1482. The valley was divided among his sons after his death.\n\nConstruction works\nHe encircled Khowpa Bhaktapur city with moats and defense walls pierced with defense gates and ordered the construction of The Palace of Fifty-five Windows (Bhaktapur's Royal Palace). The palace would later be remodelled by Bhupatindra Malla in the seventeenth centuryHe constructed the Pashupatinath Temple, a replica of the temple by the Bagmati River in Yein Kathmandu and the Siddha Pokhari, a large rectangular water tank located near the main city gate of Khowpa Bhaktapur. He is also credited as the founder of Yaksheswar Temple now standing in the palace complex.\n\nConquests and treaties\nEarly in his reign, he raided south into Mithila, into the State of Bihar and as far as Bengal.  He consolidated control over the trade route to Tibet and captured the Tibetan stronghold of Shelkar Dzong. As a result of his conquests, the boundary of Nepal extended as far as Sikkim in the east, Kerung in the North, Gorkha in the west, and Bihar in the south.After his death in 1482, he was succeeded in Bhaktapur by his son, Raya Malla, and in Kantipur by his son Ratna Malla.\nPassage 2:\nTakayama Tomoteru\nTakayama Tomoteru (高山友照) (1531–1596) was a Japanese samurai of the Azuchi–Momoyama period, who served Matsunaga Hisahide.\nHe was the father of Takayama Ukon, and was a Kirishitan.\nPassage 3:\nAnacyndaraxes\nAnacyndaraxes (Greek: Ἀνακυνδαράξης) was the father of Sardanapalus, king of Assyria.\n\nNotes\n\n This article incorporates text from a publication now in the public domain: Smith, William, ed. (1870). \"Anacyndaraxes\". Dictionary of Greek and Roman Biography and Mythology. Vol. 1. p. 157-158.\nPassage 4:\nArthur Beauchamp\nArthur Beauchamp (1827 – 28 April 1910) was a Member of Parliament from New Zealand. He is remembered as the father of Harold Beauchamp, who rose to fame as chairman of the Bank of New Zealand and was the father of writer Katherine Mansfield.\n\nBiography\nBeauchamp came to Nelson from Australia on the Lalla Rookh, arriving on 23 February 1861.He lived much of his life in a number of locations around the top of the South Island, also Whanganui when Harold was 11 for seven years and then to the capital (Wellington). Then south to Christchurch and finally Picton and the Sounds. He had business failures and was bankrupted twice, in 1879 and 1884. He married Mary Stanley on the Victorian goldfields in 1854; Arthur and Mary lived in 18 locations over half a century, and are buried in Picton. Six of their ten children born between 1855 and 1893 died, including the first two sons born before Harold.Beauchamp represented the Picton electorate from 1866 to 1867, when he resigned.  He had the energy and sociability required for politics, but not the private income then required to be a parliamentarian. He supported the working man and the subdivision of big estates, opposed the confiscation of Māori land and was later recognised as a founding Liberal, the party that Harold supported and was a \"fixer\" for. Yska calls their life an extended chronicle of rootlessness, business failure and almost ceaseless family tragedy and Harold called his father a rolling stone by instinct. Arthur also served on the council of Marlborough Province and is best-remembered for a 10-hour speech to that body when an attempt was made to relocate the capital from Picton to Blenheim.In 1866 he attempted to sue the Speaker of the House, David Monro. At the time the extent of privilege held by Members of Parliament was unclear; a select committee ruled that the case could proceed, but with a stay until after the parliamentary session.\n\nSee also\nYska, Redmer (2017). A Strange Beautiful Excitement: Katherine Mansfield's Wellington 1888-1903. Dunedin: Otago University Press. pp. 91–99. ISBN 978-0-947522-54-4.\nPassage 5:\nAshesh Malla\nAshesh Malla (Nepali: अशेष मल्ल; born 1954 in Dhankuta, Nepal) is a playwright, theatre director, Co-founder and Artistic Director of Sarwanam Theatre Group. He is also the pioneer of street theatre in Nepal.\nPassage 6:\nObata Toramori\nObata Toramori (小畠虎盛, 1491 – July 14, 1561) was Japanese samurai warrior of the Sengoku Period. He is known as one of the \"Twenty-Four Generals of Takeda Shingen\" \nHe also recorded as having been wounded 41 times in 36 encounters. \nHe was the father of Obata Masamori.\n\nSee also\nIsao Obata\nPassage 7:\nInoue Masaru (bureaucrat)\nViscount Inoue Masaru (井上 勝, August 25, 1843 – August 2, 1910) was the first Director of Railways in Japan and is known as the \"father of the Japanese railways\".\n\nBiography\nHe was born into the Chōshū clan at Hagi, Yamaguchi, the son of Katsuyuki Inoue. He was briefly adopted into the Nomura family and became known as Nomura Yakichi, though he was later restored to the Inoue family.\nMasaru Inoue was brought up as the son of a samurai belonging to the Chōshū fief. At 15, he entered the Nagasaki Naval Academy established by the Tokugawa shogunate under the direction of a Dutch naval officer. In 1863, Inoue and four friends from the Chōshū clan stowed away on a vessel to the United Kingdom. He studied civil engineering and mining at University College London and returned to Japan in 1868. After working for the government as a technical officer supervising the mining industry, he was appointed Director of the Railway Board in 1871. Inoue played a leading role in Japan's railway planning and construction, including the construction of the Nakasendo Railway, the selection of the alternative route (Tokaido), and the proposals for future mainline railway networks.In 1891 Masaru Inoue founded Koiwai Farm with Yanosuke Iwasaki and Shin Onogi.  After retirement from the government, Inoue founded Kisha Seizo Kaisha, the first locomotive manufacturer in Japan, becoming its first president in 1896. In 1909 he was appointed President of the Imperial Railway Association. He died of an illness in London in 1910, during an official visit on behalf of the Ministry of Railways.\n\nHonors\nInoue and his friends later came to be known as the Chōshū Five. To commemorate their stay in London, two scholarships, known as the Inoue Masaru Scholarships, are available each session under the University College London 1863 Japan Scholarships scheme to enable University College students to study at a Japanese University. The value of the scholarships are £3000 each.\n\nHis tomb is in the triangular area of land where the Tōkaidō Main Line meets the Tōkaidō Shinkansen in Kita-Shinagawa.\n\nChōshū Five\nThese are the four other members of the \"Chōshū Five\":\n\nItō Shunsuke (later Itō Hirobumii)\nInoue Monta (later Inoue Kaoru)\nYamao Yōzō who later studied engineering at the Andersonian Institute, Glasgow, 1866-68 while working at the shipyards by day\nEndō Kinsuke\n\nSee also\nJapanese students in Britain\nStatue of Inoue Masaru\nPassage 8:\nCleomenes II\nCleomenes II (Greek: Κλεομένης; died 309 BC) was king of Sparta from 370 to 309 BC. He was the second son of Cleombrotus I, and grandfather of Areus I, who succeeded him. Although he reigned for more than 60 years, his life is completely unknown, apart from a victory at the Pythian Games in 336 BC. Several theories have been suggested by modern historians to explain such inactivity, but none has gained consensus.\n\nLife and reign\nCleomenes was the second son of king Cleombrotus I (r. 380–371), who belonged to the Agiad dynasty, one of the two royal families of Sparta (the other being the Eurypontids). Cleombrotus died fighting Thebes at the famous Battle of Leuctra in 371. His eldest son Agesipolis II succeeded him, but he died soon after in 370. Cleomenes' reign was instead exceptionally long, lasting 60 years and 10 months according to Diodorus of Sicily, a historian of the 1st century BC. In a second statement, Diodorus nevertheless tells that Cleomenes II reigned 34 years, but he confused him with his namesake Cleomenes I (r. 524–490).\n\nDespite the outstanding length of his reign, very little can be said about Cleomenes. He has been described by modern historians as a \"nonentity\". Perhaps that the apparent weakness of Cleomenes inspired the negative opinion of the hereditary kingship at Sparta expressed by Aristotle in his Politics (written between 336 and 322). However, Cleomenes may have focused on internal politics within Sparta, because military duties were apparently given to the Eurypontid Agesilaus II (r. 400–c.360), Archidamus III (r. 360–338), and Agis III (r. 338–331). As the Spartans notably kept their policies secret from foreign eyes, it would explain the silence of ancient sources on Cleomenes. Another explanation is that his duties were assumed by his elder son Acrotatus, described as a military leader by Diodorus, who mentions him in the aftermath of the Battle of Megalopolis in 331, and again in 315.Cleomenes' only known deed was his chariot race victory at the Pythian Games in Delphi in 336. In the following autumn, he gave the small sum of 510 drachmas for the reconstruction of the Temple of Apollo at Delphi, which had been destroyed by an earthquake in 373. Cleomenes might have made this gift as a pretext to go to Delphi and engage in informal diplomacy with other Greek states, possibly to discuss the consequences of the recent assassination of the Macedonian king Philip II.One short witticism of Cleomenes regarding cockfighting is preserved in the Moralia, written by the philosopher Plutarch in the early 2nd century AD:\nSomebody promised to give to Cleomenes cocks that would die fighting, but he retorted, \"No, don't, but give me those that kill fighting.\"\nAs Acrotatus died before Cleomenes, the latter's grandson Areus I succeeded him while still very young, so Cleomenes' second son Cleonymus acted as regent until Areus' majority. Some modern scholars also give Cleomenes a daughter named Archidamia, who played an important role during Pyrrhus' invasion of the Peloponnese, but the age difference makes it unlikely.\nPassage 9:\nRatna Malla\nRatna Malla was a Malla king and the first independent king of Kantipur. He was one of the six sons of Yakshya Malla.\n\nReign\nOn the death of his father in 1482, he and his brothers attempted to rule collegially. However, Ratna Malla decided to become an independent ruler and created the Kingdom of Kantipur, with its capital in Kathmandu, in 1484. He was the first Nepalese king to invite Kashmiri Muslim traders to Kathmandu. His elder brother, Raya Malla, was the King of Bhaktapur.Ratna Malla also ruled over Patan for some time and suppressed the rebellion of Thakuri feudatories, and Bhotia with the help of  Kingdom of Palpa. It was during Ratna Malla's rule that the priests from Mithila, and South India started to become prominent in court affairs which was usually the place of Hindu and Buddhist priests. He circulated copper coins using the local copper mines in present-day Chitlang.He ruled for 38 years and was succeeded by his son Surya Malla in 1520 after his death.\nPassage 10:\nJohn Templeton (botanist)\nJohn Templeton (1766–1825) was a pioneering Irish naturalist, sometimes referred to as the \"Father of Irish Botany\". He was a leading figure in Belfast's late eighteenth century enlightenment, initially supported the  United Irishmen, and figured prominently in the town's scientific and literary societies.\n\nFamily\nTempleton was born in Belfast in 1766, the son of James Templeton, a prosperous wholesale merchant, and his wife Mary Eleanor, daughter of Benjamin Legg, a sugar refiner. The family resided in a 17th century country house to the south of the town, which been named Orange Grove in honour of William of Orange who had stopped at the house en route to his victory over James II at the Battle of the Boyne in 1690.Until the age of 16 Templeton attended a progressive, co-educational, school favoured by the town's liberal, largely Presbyterian, merchant class. Schoolmaster David Manson sought to exclude \"drudgery and fear\" by combining classroom instruction with play and experiential learning. Templeton counted among his schoolfellows brother and sister Henry Joy and Mary Ann McCracken, and maintained a warm friendship with them throughout his life.In 1799, Templeton married Katherine Johnson of Seymour Hill. Her family had been touched by the United Irish rebellion the previous year: her brother-in-law, Henry Munro, commander of the United army at the Battle of Ballynahinch, had been hanged. The couple had five children: Ellen, born on 30 September 1800, Robert, born on 12 December 1802, Catherine, born on 19 July 1806, Mary, born on 9 December 1809 and Matilda on 2 November 1813.\nThe union between the two already prosperous merchant families provided more than ample means enabling Templeton to devote himself passionately to the study of natural history.\n\nUnited Irishman\nLike many of his liberal Presbyterian peers in Belfast, Templeton was sympathetic to the programme and aims of the Society United Irishmen:  Catholic Emancipation and democratic reform of the Irish Parliament. But it was several years before he was persuaded to take the United Irish \"test\" or pledge. In March 1797 his friend, Mary Ann McCracken, wrote to her brother: [A] certain Botanical friend of ours whose steady and inflexible mind is invulnerable to any other weapon but reason, and only to be moved by conviction has at last turned his attention from the vegetable kingdom to the human species and after pondering the matter for some months, is at last determined to become what he ought to have been months ago.\nShe hoped his sisters would \"soon follow him.\" Having committed himself to the patriotic union of Catholic, Protestant and Dissenter, Templeton changed the name of the family home from loyalist Orange Grove to Irish \"Cranmore\" (crann mór, 'big tree').\nTempleton was disenchanted by the Rebellion of 1798, and mindful of events in France , repelled by the violence. He nonetheless withdrew from the Belfast Literary Society, of which he had been a founding member in 1801, rather than accept the continued presence of Dr. James MacDonnell. MacDonnell's offence had been to subscribe forty guineas in 1803 for the capture (leading to execution) of the unreformed rebel Thomas Russell who had been their mutual friend. (While unable to \"forget the amiable Russell\", time, he conceded,  \"softened a little my feelings\": in 1825, Templeton and MacDonnell met and shook hands).\n\nGarden\nThe garden at Cranmore spread over 13-acre garden was planted with exotic and native species acquired on botanical excursions, from fellow botanists, nurseries, botanical gardens and abroad: \"Received yesterday a large chest of East Indian plants which I examined today.\" \"Box from Mr. Taylor\".Other plants arrived, often as seeds from North America, Australia, India, China and other parts of the British Empire  Cranmore also served as a small animal farm.for experimental animal husbandry and a kitchen garden.\n\nBotanist\nJohn Templeton's interest in botany began with this experimental garden laid out according to a suggestion in Rousseau's 'Nouvelle Heloise' and following Rousseau's 'Letters on the Elements of Botany Here he cultivated many tender exotics out of doors (a list provided by Nelson and began botanical studies which lasted throughout his life and corresponded with the most eminent botanists in England Sir William Hooker, William Turner, James Sowerby and, especially Sir Joseph Banks, who had travelled on Captain James Cook's voyages, and in charge of Kew Gardens. Banks tried (unsuccessfully) to tempt him to New Holland (Australia) as a botanist on the Flinders's Expedition with the offer of a large tract of land and a substantial salary. An associate of the Linnean Society, Templeton visited London and saw the botanical work being achieved there. This led to his promotion of the Belfast Botanic Gardens as early as 1809, and to work on a Catalogue of Native Irish Plants, in manuscript form and now in the Royal Irish Academy, which was used as an accurate foundation for later work by succeeding Irish botanists. He also assembled text and executed many beautiful watercolour drawings for a Flora Hibernica, sadly never finished, and kept a detailed journal during the years 1806–1825 (both now in the Ulster Museum, Belfast).[1] Of the 12000 algal specimens in the Ulster Museum Herbarium about 148 are in the Templeton collection and were mostly collected by him, some were collected by others and passed to Templeton. The specimens in the Templeton collection in the Ulster Museum (BEL) have been catalogued. Those noted in 1967 were numbered: F1 – F48. Others were in The Queen's University Belfast. All of Templeton's specimens have now been numbered in the Ulster Museum as follows: F190 – F264; F290 – F314 and F333 – F334.\nTempleton was the first finder of Rosa hibernicaThis rose, although collected by Templeton in 1795, remained undescribed until 1803 when he published a short diagnosis in the Transactions of the Dublin Society.\n\nEarly additions to the flora of Ireland include Sisymbrium Ligusticum seoticum (1793), Adoxa moschatellina (1820), Orobanche rubra and many other plants. His work on lichens was the basis of this secton of Flora Hiberica by James Townsend Mackay who wrote of him The foregoing account of the Lichens of Ireland would have been still more incomplete, but for the extensive collection of my lamented friend, the late Mr. John Templeton, of Cranmore, near Belfast, which his relict, Mrs. Templeton, most liberally placed at my disposal. I believe that thirty years ago his acquirements in the Natural History of organised beings rivalled that of any individual in Europe : these were by no means limited to diagnostic marks, but extended to all the laws and modifications of the living force. The frequent quotation of his authority in every preceding department of this Flora, is but a brief testimony of his diversified knowledge\n\nBotanical Manuscripts\nThe MSS. left by Templeton consist of seven volumes. One of these is a small 8vo. half bound ; it is in the Library of the Royal Irish Academy, and contains 280 pp. of lists of Cryptogams, chiefly mosses, with their localities. In this book is inserted a letter from Miss F. M. More, sister of Alexander Goodman More, to Dr. Edward Perceval Wright, Secretary, Royal Irish Academy, dated March, 1897, in which she says—‘*‘ The Manuscript which accompanies this letter was drawn up between 1794 and 1810, by the eminent naturalist, John Templeton, in Belfast. It was lent by his son, Dr. R. Templeton, to my brother, Alex. G. More, when he was preparing the second edition of the ‘ Cybele Hibernica,’ on condition that it should be placed in the Library of the Royal Irish Academy afterwards.\" The other six volumes are quarto size, and contain 1,090 folios, with descriptions of many of the plants, and careful drawings in pen and pencil and colours of many species. They are now lent to the Belfast Museum. About ten years ago I [Lett]spent a week in examining these volumes, and as their contents have hitherto never been fully described, I would like to give an epitome of my investigation of them.\n\nVol. 1.—Phanerogams, 186 folios, with 15 coloured figures, and 6 small drawings in the text.\nVol. Il.—Fresh-water Algae, 246 folios, 71 of which are coloured.\nVol.IIl.—Marine Algae, 212 folios, of which 79 are coloured figures. At the end of this volume are 3 folios of Mosses, the pagination of which runs with the rest of this volume, but it is evident they had at some time been misplaced.\nVol. IV Fungi, 112 folios.\nVol. V.—Mosses, 117 folios, of which 20 are coloured, and also 73 small drawings in the text. *Vol. VI.—Mosses and Hepatics. 117 folios are Hepatics, 40 of which are in colours ; 96 folios are Mosses, of which 39 are full-page coloured figures; and in addition there are 3 small coloured drawings in the text.All these drawings were executed by Templeton himself, they are every one most accurately and beautifully drawn; and the colouring is true to nature and artistically finished; those of the mosses and hepatics being particularly good. Templeton is not mentioned in Tate’s ‘‘ Flora Belfastiensis,’ published in 1863, at Belfast. The earliest published reference to his MSS. is in the \"* Flora of Ulster,\" by Dickie, published in 1864, where there is this indefinite allusion—‘* To the friends of the late Mr. Templeton I am indebted for permission to take notes of species recorded in his manuscript.\" The MS. was most likely the small volume now in the Royal Irish Academy Library. In the introduction to the \"*‘ Flora of the North-east of Ireland\"’ (1888), there is a brief biographical sketch of Templeton, but no mention of any MS. However, in a ‘‘ Supplement\" to the Flora (1894), there is this note— ‘* Templeton, John, four volumes of his ‘ Flora Hibernica’ at present deposited with the Belfast Natural History and Philosophical Society, contain much original matter, which could not be worked out in time for the present paper.\" This fixes the approximate date of the MSS. being loaned to the Belfast Museum. They were not known to the authors of the ‘‘ Cybele Hibernica’\"’ in 1866, while in the second edition (1898) the small volume of the MSS. in R.1.A. Library is described in the Index of Authors under its full title—Catalogue of the Native Plants of Ireland, by John Templeton, A.L.S.\n\nNotable plant finds\nAntrim:Northern beech fern Glenaan River, Cushendall 1809: intermediate wintergreen Sixmilewater 1794: heath pearlwort :Muck Island Islandmagee  1804: dwarf willow Slievenanee Mountain 1809: thin-leaf brookweed beside River Lagan in its tidal reaches – gone now 1797: Dovedale moss Cave Hill 1797: Arctic root Slemish Mountain pre 1825: Cornish moneywort  formerly cultivated at Cranmore, Malone Road, Belfast1 pre-1825 J. persisted to 1947: rock whitebeam  basalt cliffs of the Little Deerpark, Glenarm 15 July 1808: yellow meadow rue Portmore Lough 1800:  Moschatel Mountcollyer Deerpark 2 May 1820 , Bearberry  Fair Head pre 1825, Sea Bindweed Bushfoot dunes pre 1825,  Flixweed , 'Among the ruins of Carrickfergus I found Sisymbrium Sophia in plenty' 2 Sept. 1812 – Journal of J. Templeton J4187, Needle Spike-rush Broadwater pre 1825, Dwarf Spurge Lambeg gravel pit 1804, Large-flowered Hemp-nettle, Glenarm  pre 1825\nDown:\nField Gentian Slieve Donard 1796: Lesser Twayblade Newtonards Park pre 1825:  Rough poppy 15 July 1797: Six-stamened Waterwort Castlewellan Lake 1808: Great Sundew going to the mountains from Kilkeel 19 August 1808: Hairy Rock-cress Dundrum Castle 1797: Intermediate Wintergree Moneygreer Bog 1797 Cowslip Holywood Warren pre 1825 long gone since: Water-violet Crossgar 7th July 1810 Scots Lovage Bangor Bay 1809, Mountain Everlasting  Newtownards 1793, Frogbit boghole near Portaferry, Parsley fern, Slieve Binnian, Mourne Mountains  19 August 1808, Bog-rosemary Wolf Island Bog 1794,  Marsh Pea Lough Neagh\nFermanagh: Marsh Helleborine\n\nNatural History of Ireland\nJohn Templeton had wide-ranging scientific interests including chemistry as it applied to agriculture and horticulture, meteorology and phenology following Robert Marsham. He published very little aside from monthly reports on natural history and meteorology in the 'Belfast Magazine' commenced in 1808. John Templeton studied birds extensively, collected shells, marine organisms (especially \"Zoophytes\") and insects, notably garden pest species. He planned a 'Hibernian Fauna' to accompany 'Hibernian Flora'. This was not published, even in part, but A catalogue of the species annulose animals and of rayed ones found in Ireland as selected from the papers of the late J Templeton Esq. of Cranmore with localities, descriptions, and illustrations Mag. Nat. Hist. 9: 233- 240; 301 305; 417–421; 466 -472[2], 1836.  Catalogue of Irish Crustacea, Myriapoda and Arachnoida, selected from the papers of the late John Templeton Esq. Mag. Nat. Hist. 9: 9–14 [3].and 1837 Irish Vertebrate animals selected from the papers of the late. John Templeton Esq Mag. Nat. Hist . 1: (n. s.): 403–413 403 -413 were (collated and edited By Robert Templeton). Much of his work was used by later authors, especially by William Thompson whose 'The Natural History of Ireland' is its essential continuation.\n\nDublin\nTempleton was a regular visitor to the elegant Georgian city of Dublin (by 1816 the journey was completed in one day in a wellington coach with 4 passengers) and he was a Member of the Royal Dublin Society.By his death in 1825 the Society had established a Botanic at Glasnevin \"with the following sections:\n1 The Linnaean garden, which contains two divisions, - Herbaceous plants, and shrub-fruit; and forest-tree plants.\n2. Garden arranged on the system of Jussieu. 3. Garden of Indigenous plants (to Ireland), disposed according to the system of Linnaeus. 4. Kitchen Garden, where six apprentices are constantly employed, who receive a complete knowledge of systematic botany. 5. Medicinal plants. 6. Plants eaten, or rejected, by cattle. 7. Plants used in rural economy. 8. Plants used in dyeing. 9. Rock plants. 10. Aquatic and marsh plants. - For which an artificial marsh has been formed. 11. Cryptogamics. 12. Flower garden, besides extensive hot-houses, and a conservatory for exotics\".\nOther associations were with Leinster House housing the RDS Museum and Library.\n\"Second Room. Here the animal kingdom is displayed, arranged in six classes. 1. Mammalia. 2. Aves. 3. Amphibia. 4. Pisces. 5. Insectae. 6. Vermes. Here is a great variety of shells, butterflies and beetles, and of the most beautiful species\" and the Leske collection.\nThe library at Leinster House held 12,000 books and was particularly rich in  works on botany; \"amongst which is a very valuable work in four large folio volumes, \"Gramitia Austriaca\" [Austriacorum Icones et descriptions graminum]; by Nicholas Thomas Host\".Templeton was also associated with theFarming Society funded 1800, the \nKirwanian Society  founded 1812, Marsh's Library, Trinity College Botanic Garden. Four acres supplied with both exotic and indigenous plants,the Trinity Library (80,000 volumes) and Trinity Museum.Also the Museum of the College of Surgeons.\n\nDeath and legacy\nNever of strong constitution, he was not expected to survive, he was in failing health from 1815 and died in 1825 aged only 60, \"leaving a sorrowing wife, youthful family and many friends and townsmen who greatly mourned his death\". The Australian leguminous genus Templetonia is named for him.\nIn 1810 Templeton had supported the veteran United Irishman, William Drennan, in the foundation of the Belfast Academical Institution. With the staff and scholars of the Institution's early Collegiate Department, he then helped form the Belfast Natural History and Philosophical Society (the origin of both the Botanical Gardens and what is now the Ulster Museum).\nAlthough always ready to communicate his own findings, Templeton did not publish much. Robert Lloyd Praeger (1865-1953), editor of the Irish Naturalist and President of the Royal Irish Academy, described him nonetheless as \"the most eminent naturalist Ireland has produced\".Templeton's son, Robert Templeton (1802-1892), educated at the Belfast Academical Institution (which was eventually to acquire Cranmore House), became an entomologist renowned for his work on Sri Lankan arthropods. Robert's fellow pupil James Emerson Tennent went on to write Ceylon, Physical, Historical and Topographical\n\nContacts\nThomas Martyn From 1794 supplied Martyn with many remarks on cultivation for Martyn's edition of Miller's Gardener's Dictionary.\nGeorge Shaw\nJames Edward Smith Contributions to English Botany and Flora Britannica\nJames Lee\nSamuel Goodenough\nAylmer Bourke Lambert\nJames Sowerby\nWilliam Curtis\nJoseph Banks\nRobert Brown.\nLewis Weston Dillwyn's Contributions to British Confervæ (1802–07)\nDawson Turner Contributions to British Fuci (1802), and Muscologia Hibernica (1804).\nJohn Walker\nFrancis Rawdon-Hastings, 1st Marquess of Hastings\nJohn Foster, 1st Baron Oriel\nJonathan Stokes\nWalter Wade\n\nOther\nJohn Templeton maintained a natural history cabinet containing specimens from Calobar, New Holland and The Carolinas as well as is Ireland cabinets. His library included Rees's Cyclopædia and works by Carl Linnaeus, Edward Donovan and William Swainson s:Zoological Illustrationsand he used a John Dollond microscope and lenses. He made a tour of Scotland with Henry MacKinnon. His diaries record the Comet of 1807 and the Great Comet of 1811.\n\nGallery\n|\n\nSee also\nLate Enlightenment\nJames Townsend Mackay", "answers": ["Nepal"], "length": 4625, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "6275ba9fee197d2fab82671a4497b4764a461c2c7c33ac0b"}
{"input": "Who is Archibald Acheson, 4Th Earl Of Gosford's paternal grandfather?", "context": "Passage 1:\nArchibald Primrose, 4th Earl of Rosebery\nArchibald John Primrose, 4th Earl of Rosebery,  (14 October 1783 – 4 March 1868), styled Viscount Primrose until 1814, was a British politician.\n\nHe was the eldest son of Neil Primrose, 3rd Earl of Rosebery and his second wife, Mary Vincent. Primrose was educated at Pembroke College, Cambridge, gaining his MA in 1804. He was Member of Parliament for Helston from 1805 to 1806 and Cashel from 1806 to 1807.\nHe succeeded to the earldom in 1814, and was created Baron Rosebery, of Rosebery in the County of Edinburgh, in the Peerage of the United Kingdom, in 1828. He was appointed a Privy Counsellor in 1831 and a Knight of the Thistle in 1840. He was a Fellow of the Royal Society.\nHe was the grandfather of Archibald Primrose, 5th Earl of Rosebery, who succeeded him to the title of Lord Primprose and briefly served as Prime Minister of the United Kingdom from 1894 to 1895.\n\nFamily\nLord Rosebery married firstly Harriett Bouverie, daughter of Hon. Bartholomew Bouverie  in 1808. They had four children:\n\nArchibald John Primrose, Lord Dalmeny (1809–1851)\nLady Harriet Primrose (born 1810)\nLady Mary Anne Primrose (1812–1826)\nHon. Bouverie Francis Primrose  (1813–1898)Lord and Lady Rosebery were divorced in 1815. He married secondly Anne Margaret Anson, daughter of Thomas Anson, 1st Viscount Anson in 1819. They had two children:\nLady Anne Primrose (22 Aug 1820 – 17 Sept 1862).\nLady Louisa Primrose (4 May 1822 – 23 Mar 1870).\nPassage 2:\nJohn Manners, 4th Earl of Rutland\nJohn Manners, 4th Earl of Rutland (c. 1559 – 24 February 1588) was the son of Henry Manners, 2nd Earl of Rutland, and Lady Margaret Neville, daughter of Ralph Neville, 4th Earl of Westmorland.\n\nMarriage and children\nHe married Elizabeth Charlton, a daughter of Francis Charlton of Apley Castle, by whom he had ten children:\n\nLady Bridget Manners (21 Feb 1572 – 10 July 1604) married  Robert Tyrwhitt of Kettleby 1594\nRoger Manners, 5th Earl of Rutland (6 October 1576 – 26 June 1612) married Elizabeth Sidney.\nFrancis Manners, 6th Earl of Rutland (1578 – 17 December 1632) married twice, first to Frances Knyvet, and secondly to Cecily Tufton.\nGeorge Manners, 7th Earl of Rutland (1580 – 29 March 1641) married Frances Cary.\nSir Oliver Manners (c. 1582 – 1613)\nLady Frances Manners (22 October 1588 – 1643) married William Willoughby, 3rd Baron Willoughby of Parham\nLady Mary Manners\nLady Elizabeth Manners (died 16 March 1653)\nEdward Manners died young\nLady Anne Manners; married Sir George Wharton\nPassage 3:\nGilbert Talbot, 3rd Baron Talbot\nGilbert Talbot, 3rd Baron Talbot (c. 1332–1387) was an English nobleman and soldier.\n\nFamily\nTalbot was the son and heir of Richard Talbot, 2nd Baron Talbot and his wife Elizabeth de Comyn. The Talbot family had been locally prominent in Herefordshire since the reign of Henry II of England, and had blood connections to both the Welsh and Scottish nobility. His father died in 1356, resulting in his succession as the third Baron Talbot.\n\nMilitary career\nTalbot served in several English military campaigns. He fought in the Hundred Years War under the Black Prince, and was with the royal fleet under Admiral Michael de la Pole, 1st Earl of Suffolk. During the Peasants' Revolt, he was one of the commissioners tasked with raising forces to fight the rebels. He served under the Earl of Cambridge in Portugal and Spain in 1381–1382, and was present at the capture of Higuera la Real. During this Iberian service, he was chosen as the ambassador of the English forces to the king of Portugal to demand their wages. He returned to England, where he was called to Newcastle in 1385 for service against the Scots. He returned to Spain in 1386 with John of Gaunt when the latter was pressing his claim to the throne of Castile. He died of the plague while in Spain in 1387.\n\nMarriages and children\nTalbot was married twice. Prior to 1361, he married Petronilla, daughter of James Butler, 1st Earl of Ormond by his wife Eleanor de Bohun. They had two children:\nRichard Talbot, 4th Baron Talbot, his son and heir. He is an ancestor to Lady Maud Parr, mother of Queen Catherine Parr who was the sixth and final wife of Henry VIII.\nElizabeth Talbot, who married Henry Grey, 5th Baron Grey de WiltonHe married secondly Joan, daughter of Ralph de Stafford, 1st Earl of Stafford by his wife Margaret de Audley, 2nd Baroness Audley.\n\nDeath and legacy\nTalbot died on 24 April 1387 and was succeeded by his son Richard. He seems to have been a spendthrift, and left significant debts at his death. A year earlier, he had been pardoned for outlawry after failing to answer the Earl of Arundel concerning a debt of £3000. The economic problems he left behind were still affecting the Talbot family in the time of his grandson, the fifth baron.\nPassage 4:\nLawrie McKinna\nLawrie McKinna (born 8 July 1961) is a Scottish-Australian former football player, coach, and former Mayor of Gosford City Council.\nIn 2012, McKinna stood for election as an independent for City of Gosford. Lawrie was successful in gaining a seat, and was elected by his fellow councillors Mayor of Gosford City on 24 September.He was removed from his position as Mayor, as a result of the amalgamation of Gosford and Wyong Councils on 12 May 2016.\nLawrie unsuccessfully stood as a candidate in the September 2013 Federal election for the seat of Robertson. His campaign was backed by John Singleton to the tune of $380,000. While receiving 8.7% of the vote, Singleton and McKinna controversially decided the outcome of the seat by directing preference votes to the conservative Liberal Party candidate Ms Lucy Wicks. Ms Wicks formally thanked Lawrie & his backer John Singleton in her maiden speech in parliament.\n\nEarly life\nMcKinna was born in Galston in southwest Scotland.\n\nPlaying career\nClub\nMcKinna began his career as a striker with local junior side Darvel and made his debut for Scottish Football League side Kilmarnock in 1982. He made 87 league appearances for Kilmarnock, scoring 17 times before moving to Australia in 1986 where he went on to play for several more clubs in the NSL and various state leagues.\n\nManagement career\nMcKinna's coaching career began in 1992 with New South Wales side Blacktown City as assistant manager. In 1995 Hills United hired him as a player/manager (http://www.hillsbrumbies.com.au/). In 1997, he became assistant to David Mitchell with National Soccer League clubs Sydney Olympic, then following Mitchell to Sydney United in 1998 and Parramatta Power in 1999.\nHe left Parramatta Power in 2002 to take over as manager of Northern Spirit. His first season as a NSL coach was extremely promising and successful as he beat many accomplished coaches, and was awarded with the NSL coach of the year award after taking the Northern Spirit to their first finals campaign for three years.\n\nCentral Coast Mariners\nIn 2005, he was named as manager of the new A-League club the Central Coast Mariners, earning the inaugural A-League coach of the year award after leading the Mariners to the grand final and winning the preseason cup. In May 2006 he signed a new five-year contract with the Mariners.McKinna was popular in the community for his insistence that all the players at the club engaged in community activities. This became a hallmark of his tenure at the fledgling club.\nIn the 2006/2007 season, McKinna gave an interview during which his team were struggling for on field success. Notably saying how it was frustrating for him when the press report losses in matches but don't mention the long-term injury's to the sides key players like Nik Mrdja, Andre Gumprecht and Noel Spencer. In the interview he also talked about his footballing coaching licenses and mentions that he would be preparing to take his '\"Asian 'B' license\" course soon.On 9 February 2010, it was announced that McKinna will take over as the Football and Commercial Operations Manager for the Mariners from the 2010/2011 season, with Graham Arnold replacing him as head coach.\n\nChengdu Blades\nChinese Super League club Chengdu Blades have shown interest in McKinna taking over the reins as manager of the first team on 18 March 2011. A day later, he was appointed as the head coach of Chengdu Blades  a club known to have the lowest operating budget in the CSL.\nOn 15 August, it was confirmed by McKinna via his Twitter account, that he had resigned from his position as manager at the Blades. He cited off-field, back room issues as a major reason for his decision, which contributed to the Blades poor 2011 CSL season, in which at the time of McKinna's departure had seen them only win only twice, conceding 30+ goals, whilst only scoring 13, and the club at the bottom of the ladder after just 20 matches.\n\nChongqing Lifan\nOn 2 December 2011 it was announced that McKinna had signed a one-year contract with China League One side Chongqing Lifan. On 15 April 2012 he announced he was leaving the club after a disagreement with the board.\n\nCentral Coast Mariners\nOn 4 May 2012 it was announced that Lawrie would become the new Director of Football for the Central Coast Mariners. A position that he took on again temporarily for two months in 2014.\n\nNewcastle Jets\nIn June 2016, McKinna was appointed chief executive of the Newcastle Jets.\n\nAfter football\nMcKinna was elected a councillor of Gosford City Council in September 2012 and nominated as Mayor at the first council meeting.In the 2013 Australian election, McKinna ran as a conservative independent for the seat of Robertson.\n\nManagerial statistics\nAs of February 2010\n\nHonours\nPlayer\nClub\nAPIA Leichhardt:\n\nNSL Cup: 1988\n\nManager\nClub\nCentral Coast Mariners:\n\nA-League Championship:Finalists: 2006, 2008\nA-League Premiership: 2007–08\nA-League Challenge Cup: 2005Finalists: 2006\n\nIndividual\nNSL Coach of the Year: 2002–03\nA-League Coach of the Year: 2005–2006\nPassage 5:\nRalph Neville, 3rd Earl of Westmorland\nRalph Neville, 3rd Earl of Westmorland (c. 1456 – 6 February 1499) was an English peer. He was the grandfather of Ralph Neville, 4th Earl of Westmorland.\n\nOrigins\nHe was born in about 1456, the only child of John Neville, Baron Neville (younger brother of Ralph Neville, 2nd Earl of Westmorland) by his wife Anne Holland, daughter of John Holland, 2nd Duke of Exeter (1395-1447).\n\nCareer\nNeville's father was slain fighting for the Lancastrians at the Battle of Towton on 29 March 1461, and attainted on 4 November of that year. On 6 October 1472 Ralph Neville obtained the reversal of his father's attainder and the restoration of the greater part of his estates, and thereby became Lord Neville (1459 creation).On 18 April 1475 Neville was created a Knight of the Bath together with the sons of King Edward IV. He was a justice of the peace in Durham. For his 'good services against the rebels', on 23 March 1484 King Richard III granted Neville manors in Somerset and Berkshire and the reversion of lands which had formerly belonged to Margaret, Countess of Richmond. In September 1484 he was a commissioner to keep the truce with Scotland. On 3 November 1484 his uncle, Ralph Neville, 2nd Earl of Westmorland, died, and Neville succeeded as 3rd Earl of Westmorland and Lord Neville (1295 creation).After the Yorkist defeat at Bosworth, Westmorland entered into bonds to the new king, Henry VII, of £400 and 400 marks, and on 5 December 1485, he gave custody (and the approval of the marriage of his eldest son and heir), Ralph Neville (d.1498), to the King.Westmorland held a command in the army sent into Scotland in 1497 after James IV supported the pretensions to the crown of Perkin Warbeck.\n\nDeath\nWestmorland's eldest son died in 1498. Westmorland died at Hornby Castle, Yorkshire, the seat of his son-in-law, Sir William Conyers, on 6 February 1499, allegedly of grief for his son's death, and was buried in the parish church there. His grandson, Ralph Neville, succeeded to the earldom as 4th Earl of Westmorland.\n\nMarriage and issue\nBefore 20 February 1473, Neville married Isabel Booth, the daughter of Sir Roger Booth, esquire (1396–1467) and Catherine Hatton, and the niece of Lawrence Booth, Archbishop of York, by whom he had a son and a daughter:\nRalph Neville, Lord Neville (d. 1498). As noted above, on 5 December 1485, his father had granted his custody (and the approval of the marriage of his eldest son) to the King. Accordingly, Lord Neville married firstly, in the presence of King Henry VII and his Queen, Elizabeth of York, Mary Paston (born 19 January 1470), the eldest daughter of Sir William Paston (b. 1436 – died before 7 September 1496) by Lady Anne Beaufort, daughter of Edmund Beaufort, 2nd Duke of Somerset. She died of measles at court, about Christmas 1489. There were no issue of the marriage.\nLady Anne Neville, who married firstly, William Conyers, 1st Baron Conyers, and secondly, Anthony Saltmarsh (1473–1550) of Langton by Wragby, Lincolnshire.Lord Neville married secondly, again in the royal presence, Edith Sandys (d. 22 August 1529), sister of William Sandys, 1st Baron Sandys, by whom he had three children:\n\nRalph Neville, 4th Earl of Westmorland\na son who died young\nCecilia Neville, who married John Weston, son of John Weston Jr. and Virginia Alice Edshaw, and was the mother of Dr Robert Weston, Lord Chancellor of IrelandAfter Lord Neville's death in 1498, his widow Edith married Thomas Darcy, 1st Baron Darcy of Darcy, who was beheaded on Tower Hill on 30 June 1537.\n\nFootnotes\nPassage 6:\nSir Archibald Acheson, 1st Baronet\nSir Archibald Acheson of Glencairn, Lord Glencairn, 1st Baronet (1583 – 9 September 1634), was a Scottish jurist.\n\nBiography\nAcheson was the son of Captain Patrick Acheson and Martha Drummond.On 31 March 1620, \"Archibald Acheson, a Scotchman\", was knighted at Theobalds by King James I, and in 1621 he was appointed Master in Chancery of Ireland. Sometime before 25 October 1626 he was appointed a Lord of Session of Scotland as 'Lord Glencairn'. On 21 October 1627, he was appointed by King Charles I as Royal Secretary of State of Scotland. On 1 January 1628, he was made a Baronet of Nova Scotia.Lord Glencairn died at Letterkenny, County Donegal, in the west of Ulster in September 1634.\n\nIreland\nIn 1610, at the start of the Plantation of Ulster, numerous land grants were made in the precinct of Fewes in County Armagh. One was of 2,000 acres to Sir James Douglas, Knt., of Spott, Haddingtonshire, subsequently sold the next year to Henry Acheson, who afterwards sold it to Sir Archibald Acheson. A further 1,000 acres originally granted to Henry was also sold on to Sir Archibald Acheson in 1628. Acheson does not ever appear to have resided in Ireland, however, and his position in the Court of Chancery there appears titular; his judicial duties were all in Scotland. He nevertheless became a \"denizen\" of Ireland on 12 February 1618, presumably in order to qualify for the lands he was receiving from his brother, Henry Acheson of Dromlech, County Armagh. Certainly Sir Archibald's second son, George, resided in Ireland.\n\nFamily\nAcheson wed Agnes Vernor at some point before 1610, fathering an eldest son, Sir Patrick Acheson, 2nd Baronet (c.1611-1638). Sir John Scot (1754) states that this son died after his first year of marriage, to an English heiress, without issue.\nAfter his first wife died, Sir Archibald remarried in 1622, Margaret, daughter of Sir John Hamilton and Johanna Everard, by whom he had a son, George (1629–1685).By his first wife he had a daughter, Jean, who married Sir Lewis Lauder of Over Gogar & Alderston, Knt., (c1599-c1640), Sheriff-Principal of Edinburgh and son of Sir Alexander Lauder of Haltoun, Knt. They had at least three known children. Jean was still living on 3 April 1663 as \"relict of Sir Lewes Lauder of Over Gogar\".Lord Glencairn may have had another daughter by one of his marriages, Isabella Acheson of Gosford, who married Hector Og Maclean (1583–1623). Sources list her as the daughter of \"Sir Archibald Acheson\", but because of her age, she may have been the daughter of Captain Patrick Acheson or one of his siblings. If she was the same age as Hector Og Maclean, she would have been born in 1583 and would have had her first child around 1600 at age 17. If she was the daughter of Sir Archibald Acheson she would be born no earlier than 1610 the year Archibald married. This would make her at least 20 years younger than Hector Og Maclean, and would make her the same age as her own children. This is the error in the standard genealogy.His eldest son Patrick succeeded him to the baronetcy but having died without issue several years after his father, whereupon the title passed to his half-brother Sir George Acheson, 3rd Baronet, who relocated to Ireland and in 1657 was High Sheriff of Counties Armagh and Tyrone.\nPassage 7:\nArchibald Acheson, 3rd Earl of Gosford\nArchibald Acheson, 3rd Earl of Gosford KP (20 August 1806 – 15 June 1864), styled Viscount Acheson between 1807 and 1849, was a British peer and Member of Parliament.\n\nEarly life\nGosford was born on 20 August 1806. He was the only son of Archibald Acheson, 2nd Earl of Gosford of Gosford Castle, County Armagh and the former Mary Sparrow (1777–1841). He had four younger sisters, including Lady Mary Acheson (wife of James Hewitt, 4th Viscount Lifford) and Lady Millicent Acheson (wife of Dr. Henry Bence Jones).His paternal grandparents were Arthur Acheson, 1st Earl of Gosford and the former Millicent (née Pole) (a daughter of Lt.-Gen. Edward Pole). His mother was the only daughter and heiress of Robert Sparrow of Worlingham Hall and Mary (née Bernard) Sparrow (sister and heiress of Sir Robert Bernard, 5th Baronet and only daughter of Sir John Bernard, 4th Baronet).He was educated at Harrow School, and matriculated at Christ Church, Oxford in 1825, graduating B.A. in 1828.\n\nCareer\nHe was elected in 1830 as the Member of Parliament for County Armagh in the British House of Commons, a seat he held until 1847, when he was ennobled as 1st Baron Acheson, of Clancairney, County Armagh, in the Peerage of the United Kingdom. He was Lord of the bedchamber between 1831-1834. He succeeded to his father's Irish titles and estates in 1849, including the 2,800 acres (4.4 sq mi; 11 km2) Worlingham Hall estate which he sold at auction in August 1849. He was created a Knight of the Order of St. Patrick in 1855.He was appointed lord-lieutenant and custos rotulorum of co. Armagh from February 1864 to his death later that year.\n\nPersonal life\nOn 22 June 1832, he was married to Lady Theodosia Brabazon (1808-1876), daughter of John Brabazon, 10th Earl of Meath and the former Lady Melosina Adelaide Meade (fourth daughter of John Meade, 1st Earl of Clanwilliam). Together, they were the parents of seven children:\nLady Gertrude Emily Acheson (d. 1927), who married Francis Foljambe, half-brother of Cecil Foljambe, 1st Earl of Liverpool, and eldest son and heir of George Savile Foljambe and Harriet Emily Mary Milner (a daughter of Sir William Milner, 4th Baronet) in 1856.\nLady Mary Acheson (1835–1892), who married Hon. Leopold William Henry Fox-Powys, second son of Thomas Powys, 3rd Baron Lilford and the former Hon. Mary Elizabeth Fox (sister and heiress of Henry Fox, 4th Baron Holland and only daughter of Henry Fox, 3rd Baron Holland) in 1862.\nRuthanne Acheson\nLady Edith Acheson (1837–1906)\nArchibald Brabazon Sparrow Acheson, 4th Earl of Gosford (1841–1922), who married Lady Louisa Montagu, the second daughter of William Montagu, 7th Duke of Manchester and the former Countess Louisa von Alten. His wife was a Lady-in-Waiting to Queen Alexandra.\nMaj.-Gen. the Hon. Edward Archibald Brabazon Acheson (1844–1921), who married Clementina Le Marchant, a daughter of Gen. Sir John Gaspard Le Marchant, in 1869.\nLady Katherine French Acheson (1847–1898), who married Capt. Frederick William Duncombe, third son of Adm. Hon. Arthur Duncombe (fourth son of Charles Duncombe, 1st Baron Feversham), in 1868.Lord Gosford died on 15 June 1864 and was succeeded by his son, Archibald. His widow died on 13 February 1876.\nPassage 8:\nArchibald Acheson, 4th Earl of Gosford\nArchibald Brabazon Sparrow Acheson, 4th Earl of Gosford,  (19 August 1841 – 11 April 1922) was a British peer.\nThe son of Archibald Acheson, 3rd Earl of Gosford, he was born at Worlingham Hall, Suffolk, in 1841, and educated at Harrow School; and succeeded to the earldom upon the death of his father in 1864. \nHe was Lord of the Bedchamber to Edward VII, Prince of Wales between 1886 and 1901, and bore the Queen consort's Ivory rod At Edward VII's King's coronation. He became vice-admiral of Ulster, also received the Order of the Dannebrog, and the Order of the White Eagle (Russian Empire). Since there are two United Kingdom peerages (e.g. Baron Worlingham) subsumed in that Irish Earldom, he was entitled to an automatic seat in the House of Lords. He was Lord Lieutenant of Armagh from 1883 to 1920, and served as Vice-Chamberlain of the Household of Queen Alexandra from 1901.He was Honorary Colonel of the 3rd Battalion of the Royal Irish Fusiliers from 1899, and Vice-Admiral of Ulster. Gosford died in London in 1922, aged 80, and was cremated at Golders Green Crematorium.\n\nFamily\nHe married Lady Louisa Augusta Beatrice Montagu (named, in 1920, as a Dame Commander of the Order of the British Empire, DBE), daughter of William Montagu, 7th Duke of Manchester, at London on 10 August 1876, with whom he had the following children:\n\nArchibald Charles Montagu Brabazon Acheson, 5th Earl of Gosford (1877–1954)\nLady Alexandra Louise Elizabeth Acheson (1878 – 21 January 1958); married Lt.-Col. Hon. William Frederick Stanley, son of Frederick Stanley, 16th Earl of Derby.\nLady Mary Acheson (1881–????); married Hon. Robert Arthur Ward.\nLady Theodosia Louisa Augusta Acheson (1882 – 16 October 1977), married Alexander Cadogan.\nCaptain Patrick George Edward Cavendish Acheson (30 June 1883 – 30 August 1957)\nPassage 9:\nThomas Stewart, Master of Mar\nSir Thomas Stewart, Master of Mar was an illegitimate son of Alexander Stewart, the earl of Mar. He was the great-grandson of King Robert II of Scotland. He died before August 1432.Thomas married Elizabeth, the widow of John Stewart, 2nd Earl of Buchan, who was daughter of Archibald Douglas, 4th Earl of Douglas and Margaret Stewart, Lady of Galloway. They were required to obtain a marriage license, which was granted on 1 May 1427, due to their degrees of consanguinity and affinity.He had a son.\n\nCitations\nPassage 10:\nArchibald Acheson, 2nd Earl of Gosford\nArchibald Acheson, 2nd Earl of Gosford,  (1 August 1776 – 27 March 1849), styled The Honourable Archibald Acheson from 1790 to 1806 and Lord Acheson from 1806 to 1807, was a British politician who served as Lieutenant-Governor of Lower Canada and Governor General of British North America in the 19th century.\n\nEarly life\nAcheson was born on 1 August 1776 at Markethill, County Armagh, Ireland.  Gosford was the son of Arthur Acheson, 1st Earl of Gosford, and his wife Millicent (née Pole). He succeeded his father to his titles and estates in 1807.\n\nCareer\nAcheson sat in the Irish House of Commons for County Armagh from 1798 until the Act of Union in 1801, when Ireland became part of the United Kingdom. Subsequently, he was a Member of the British House of Commons representing Armagh to 1807, when he succeeded to his father's Irish titles as Earl of Gosford. He entered the British House of Lords in 1811 upon being elected an Irish Representative Peer.In 1831 he was appointed the first Lord Lieutenant of Armagh for life, having previously been a Governor of Armagh since 1805. The new position incorporated the post of Custos Rotulorum of County Armagh which he also already held. He was created Baron Worlingham in the Peerage of the United Kingdom in 1835 and thus became a member of the UK House of Lords in his own right. He commissioned Thomas Hopper (1776–1856) to design a new house, Gosford Castle on his Gosford estate. The house would not be completed until after his death.\nIn 1835, he became Governor General of British North America (also Lieutenant-Governor of Lower Canada), and commissioner in the Royal Commission for the Investigation of all Grievances Affecting His Majesty's Subjects of Lower Canada. He was instructed to appease the reformists, led by Louis-Joseph Papineau, without giving them any real power. Gosford attempted to distance himself from his predecessor, Lord Aylmer, who had exacerbated the hostility of French-Canadians to the British administration. Gosford officially established the Diocese of Montreal in 1836, though it had been unofficially created a few years before. In August of that year Gosford dissolved the Legislative Assembly when they refused to pass his budget.In November, Lord Gosford learned of the planned Lower Canada Rebellion and had many of Papineau's followers arrested, although Papineau himself escaped to the United States. The next month, he issued a reward for the capture of Papineau, and declared martial law in Lower Canada.\nLord Gosford resigned in November 1837 and returned to Britain the next year. His eventual successor, Lord Durham, implemented the Act of Union 1840, uniting Lower and Upper Canada, which Lord Gosford had unsuccessfully argued against.\n\nPersonal life\nHe married Mary Sparrow, the daughter and heiress of Robert Sparrow of Worlingham Hall, Suffolk, with whom he had a son and four daughters.\nArchibald Acheson, 3rd Earl of Gosford (20 August 1806 – 15 June 1864), he succeeded his father upon his death.\nLady Mary Acheson (27 June 1809 – 13 March 1850). On 9 July 1835 she married James Hewitt, 4th Viscount Lifford. They had four sons, and four daughters.\nLady Millicent French Acheson (circa 1812 – 29 August 1887). She married Henry Bence Jones on 28 May 1842. They had three sons, and four daughters. The youngest son, Archibald, married a daughter of Henry Lopes, 1st Baron Ludlow.Lord Gosford died in 1849.\n\nLegacy\nIt is believed the city of Gosford in New South Wales, Australia was named after him, the Governor of New South Wales having served with him in Canada.\n\nSee also\nList of Canadian Governors General", "answers": ["Archibald Acheson, 2nd Earl of Gosford"], "length": 4383, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "40de6f6c353c080845a8ea644039e9ba83cd481d3745395b"}
{"input": "When is Henrietta Maria Of Brandenburg-Schwedt's father's birthday?", "context": "Passage 1:\nFrederick Henry, Margrave of Brandenburg-Schwedt\nFrederick Henry, Margrave of Brandenburg-Schwedt (21 August 1709, in Schwedt – 12 December 1788, in Schwedt) was the last owner of the Prussian secundogeniture of Brandenburg-Schwedt.\n\nEarly life\nHis was the son of Margrave Philip William, son of Philip William, Margrave of Brandenburg-Schwedt and his wife Sophia Dorothea of Schleswig-Holstein-Sonderburg-Glücksburg. His mother was Princess Johanna Charlotte of Anhalt-Dessau, daughter of Prince John George II of Anhalt-Dessau and Princess Henriette Catherine of Nassau.\n\nLife\nAfter his father's death in 1711, his mother put Frederick Henry under the guardianship of his uncle Frederick I, and after Frederick I's death in 1713, under the guardianship of his cousin Frederick William I.  In 1711, Frederick Henry was made the chief of the Infantry Regiment No. 12.  However, he showed little interest in military affairs.  In 1733, King Frederick William I was so incensed with the disorder in Frederick Henry's regiment that he was jailed for several weeks.  Frederick the Great held little respect for Frederick Henry's abilities and did not employ him.  In 1741, Frederick Henry traded the Infantry Regiment No. 12 for the Infantry Regiment No. 42, but again, he cared little for his duties, and he left its business to the respective commanders.\nWhen his brother Frederick William died in 1771, Frederick Henry inherited the Lordship of Schwedt-Wildenbruch.  As \"Margrave of Brandenburg-Schwedt\", he was a patron of the arts, especially theater.  In 1755 he acquired the Prinzessinnenpalais in Berlin and in 1785, he contracted the actress Henriette Hendel-Schutz to perform in his Court Theater.\nHe married his first cousin Leopoldine Marie of Anhalt-Dessau, a daughter of Prince Leopold I of Anhalt-Dessau, nicknamed the old Dessauer.  After the birth of two daughters, he and his wife quarreled so often and so violently, that he banned her to Kolberg for the rest of her life.\nBetween 1760 and 1762, the mathematician Leonhard Euler sent numerous letters in French about mathematical and philosophical subjects to his daughter Frederike. These letters were published between 1769 and 1773 under the title \"Letters to a German Princess\" and were printed in Leipzig and St. Petersburg. The French edition alone enjoyed 12 printings. It was the Age of Enlightenment and Euler tried to explain physical issues and in particular their philosophical background in a generally understandable manner. Frederick Henry may have employed Euler as her teacher.When he died in 1788, the junior line of Brandenburg-Schwedt died out and the secundogeniture fell back to the Electorate.  His daughters and nieces received a pension.\n\nDaughters\nLouise of Brandenburg-Schwedt (10 August 1750 – 21 December 1811) married Prince (later Duke) Leopold III of Anhalt-Dessau (1740-1817)\nFriederike Charlotte of Brandenburg-Schwedt (18 August 1745 – 23 January 1808), the last Abbess of Herford Abbey\nPassage 2:\nPrincess Anna Elisabeth Louise of Brandenburg-Schwedt\nPrincess and Margravine Anna Elisabeth Louise of Brandenburg-Schwedt (German: Luise; 22 April 1738 – 10 February 1820) was a Prussian princess by marriage to her uncle Prince Augustus Ferdinand of Prussia. She was a daughter of Margrave Frederick William of Brandenburg-Schwedt and Princess Sophia Dorothea of Prussia.\n\nEarly life\nAnna Elisabeth Louise was one of five children born to Margrave Frederick William of Brandenburg-Schwedt and Sophia Dorothea of Prussia. Her siblings included Sophia Dorothea, Duchess of Württemberg, and Philippine, Landgravine of Hesse-Cassel.\nHer father was a son of Philip William, Margrave of Brandenburg-Schwedt and Princess Johanna Charlotte of Anhalt-Dessau.\nHer mother was a daughter of Frederick William I of Prussia and Sophia Dorothea of Hanover. Through her mother, Anna Elisabeth Louise was a niece of Frederick the Great.\n\nPrincess of Prussia\nOn 27 September 1755 in Charlottenburg Palace, Berlin, Anna Elisabeth Louise married her uncle Prince Augustus Ferdinand of Prussia, a younger brother of her mother, Sophia Dorothea. He was eight years older than she and was a younger son of Frederick William I of Prussia and Sophia Dorothea of Hanover (herself the only daughter of George I of Great Britain).\n\nThe biological father of her daughter Louise, who was born in 1770, may have been Count Friedrich Wilhelm Carl von Schmettau. Louise was described as nice, witty and kind. The Swedish Princess Hedwig Elizabeth Charlotte described her at the time of her visit in 1798: In the afternoon, we visited this Princess, who lives at Bellevue in the outskirts of Berlin. It is a little villa, very suitable for a private person but far from royal. The reception here was quite dissimilar from the one at my aunt. Princess Ferdinand is stiff and made it obvious that she wished to impress us. I was of course polite, but after I had noticed, that she took on a condescending tone and wished to embarrass me, I replied the same way and displayed the same haughtiness. The Princess is no longer young, has surely been beautiful, looks like an aristocratic Frenchwoman but not like a Princess, for she has nothing royal about her. I do not think she is that clever, but she can make a pleasant conversation and is quite confident, as one becomes through a long habit of socializing in the grand world. \nAnna Elisabeth Louise was one of the few members of the royal house to remain in Berlin during the French occupation in 1806. While most of the royal family left, reportedly because of the anti-Napoleonic criticism they had expressed, and the members of the royal court either followed them or left the capital for their country estates, Elisabeth Louise remained with her spouse and Princess Wilhelmina of Hesse-Kassel because of \"their great age\", as did Princess Augusta of Prussia, who was pregnant at the time.One visitor to her in 1813–14 commented that, \"I never saw such a formal, stiff, disagreeable old woman - vieille cour outree, and she frightened me to death. I was glad to get away...\".\n\nDeath\nAugustus Ferdinand died in Berlin on 2 May 1813. Elisabeth Louise died seven years later, on 22 February 1820. She is buried in Berlin Cathedral.\n\nIssue\nOn 27 September 1755 in Charlottenburg Palace, Berlin, Anna Elisabeth Louise married her uncle Prince Augustus Ferdinand of Prussia\nThe couple had seven children:\n\nFriederike Elisabeth Dorothea Henriette Amalie, Princess of Prussia (1761–1773)\nFriedrich Heinrich Emil Karl, Prince of Prussia (1769–1773)\nFriederike Dorothea Louise Philippine, Princess of Prussia (1770–1836), married to Prince Antoni Radziwiłł\nHeinrich Friedrich Carl Ludwig (1771–1790)\nFriedrich Ludwig Christian (1772–1806)\nFriedrich Paul Heinrich August, Prince of Prussia (1776)\nFriedrich Wilhelm Heinrich August, Prince of Prussia (1779–1843)\n\nAncestry\nPassage 3:\nFrederick William, Margrave of Brandenburg-Schwedt\nFrederick William of Brandenburg-Schwedt (17 November 1700 – 4 March 1771) was a German nobleman. In his lifetime, from 1711 to 1771, he held the titles Prince in Prussia and Margrave of Brandenburg, with the style Royal Highness. He was made a knight of the Order of the Black Eagle.\nIn the 19th century he was retrospectively known by the title Margrave of Brandenburg-Schwedt, in order to differentiate his branch of the Hohenzollern dynasty.  He was the second owner of the Prussian secundogeniture of Brandenburg-Schwedt. His parents were Philip William, Margrave of Brandenburg-Schwedt, and Princess Johanna Charlotte of Anhalt-Dessau. He was the nephew of King Frederick I of Prussia.\n\nLife\nFrederick William was known as a brutal man because of his short temper, severity, and coarse manners. He was born at Oranienbaum Castle (modern-day Oranienbaum-Wörlitz, Wittenberg), and was educated and raised by his uncle, King Frederick I, and then by his cousin, King Frederick William I. His character closely resembled that of his second royal guardian, who like himself, hated idleness and was a terror to all loungers. The clergy were especial objects of his ridicule and persecution. His cane was as much feared as that of his royal namesake.He made the fashionable Grand Tour, travelling to Geneva 1715, and in 1716 to Italy. He returned in 1719 to Prussia, where he received the Order of the Black Eagle from Frederick William I. On 15 June 1723 he was made a Prussian major-general. On 10 July 1737 he was appointed lieutenant-general.The existence of the Schwedt branch of the Hohenzollern dynasty, descended as they were from Frederick I's father and being 'princes of the blood', posed a theoretical threat to the Prussian kings.  Frederick William I tried to neutralise this threat by keeping his cousins close, bringing the Schwedt brothers into his own household, acting as their guardian, and later marrying Frederick William to his daughter. Following the margrave's reaching adulthood the king was so fearful of any covert political activity on his cousin's part that he sent spies to Schwedt to find out who met with Frederick William and his brother.Margrave Frederick William pursued a lavish programme of building in Schwedt, both in the palace and town, and he actively purchased land and estates to augment his inheritance; this aggrandisement resulted in the king eventually forbidding him from making any more such purchases. In contrast to his father's policy Frederick II sought to distance himself from his Schwedt cousins, humiliating them at every chance. He made them unwelcome at his court, undermined the margrave's authority in his own dominions by encouraging complaints and lawsuits by his tenants and neighbours and, most effectively, he marginalised the position of the Schwedt brothers within the Prussian army.  Margrave Frederick William was removed from command in the army, a denigration the king also extended to his own brothers.Frederick William was 19 years older than his wife Sophia Dorothea of Prussia, who was his first cousin once removed.  The marriage, in 1734, was at the express wish of King Frederick William, against the wishes of his daughter; the bride was given away by her brother the future Frederick II, as the king was unwell. The relationship of the couple was not happy. Sophia often fled to the protection of her brother King Frederick. The latter did not stop at friendly admonitions, but sent General Meir to Schwedt with unlimited authority to protect the margravine from insult. Eventually they lived in separate places: Sophia lived in the castle Montplaisir, and the Margrave lived in the castle of Schwedt. Apparently they were only reconciled when the margravine was in her terminal illness; she died in her husband's arms.On 4 March 1771, Frederick William died at Wildenbruch Castle, when the heavy cold he was suffering from worsened. The Margrave acknowledged one illegitimate son, the only one of his male offspring to survive infancy. Due to his lack of surviving legitimate male issue, his lands and title were inherited by his younger brother Frederick Henry (ruled 1771–1788).\n\nIssue\nIn 1734, the Margrave married Sophia Dorothea of Prussia and they had five children.\nSophia Dorothea (18 December 1736 – 9 March 1798); married Frederick II Eugene, Duke of Württemberg\nElisabeth Louise (22 April 1738 – 10 February 1820); married her uncle Prince Augustus Ferdinand of Prussia\nGeorge Philip (10 September 1741 – 28 April 1742)\nPhilippine (10 October 1745 – 1 May 1800); married Frederick II, Landgrave of Hesse-Kassel (or Hesse-Cassel)\nGeorge Frederick (3 May 1749 – 13 August 1751)He also fathered an illegitimate son named Georg Wilhelm von Jägersfeld (1725–1797).\n\nGenealogy\nFrederick William belonged to a junior branch of the House of Hohenzollern; the senior branch were the Counts of Hohenzollern-Sigmaringen. The junior line produced electors of Brandenburg and kings and emperors of Prussia and Germany. Frederick William was a descendant of Burkhard I, Count of Zollern. Through his daughter Sophia Dorothea he is an ancestor of Mary of Teck (Queen Mary), the wife of George V, and therefore an ancestor of the present British royal family.\n\nAncestry\nSee also\nFrederick William, Elector of Brandenburg\n\nNotes\nPassage 4:\nCharles Frederick Albert, Margrave of Brandenburg-Schwedt\nKarl Friedrich Albrecht, Margrave of Brandenburg-Schwedt (10 June 1705 – 22 June 1762), a grandson of Frederick William of Brandenburg (the Great Elector) and son of Margrave Albert Frederick of Brandenburg-Schwedt, was a Prussian military officer and the Herrenmeister (grand master) of the Order of Saint John (Bailiwick of Brandenburg).\n\nLife\nCharles of Brandenburg-Schwedt was born in Berlin. He joined the Prussian Army at an early age and distinguished himself during the First Silesian War at the capture of Głogów, at the Battle of Mollwitz and the Battle of Chotusitz. He took command in Upper Silesia in the spring of 1745, to the special satisfaction of his cousin, King Frederick II of Prussia.\nDuring the Seven Years' War Margrave Charles again held independent commands, as Frederick II had confidence in him, and he distinguished himself at the Battle of Hochkirch and the Battle of Torgau. In both battles, as at Mollwitz, he was wounded.\nThe General German Biography (ADB) describes him as a noble, philanthropic character and lover of the arts and sciences.\nFor 31 years he governed the knights, the Bailiwick of Brandenburg, and its fiefs as Grand Master of the Order of St. John, having been installed at Sonnenburg in 1731. He died in Breslau.\n\nIssue\nCharles Frederick Albert was never married, but had one daughter with his mistress, Dorothea Regina Wuthner (who was raised to the nobility on 14 January 1744 as \"Frau von Carlowitz\"):\n\nCaroline Regina von Carlowitz (Soldin, 12 December 1731 – Berlin, 16 September 1755), married in Berlin on 16 June 1747 to Count Albrecht Christian von Schönburg-Hinterglauchau (22 January 1720 – 9 March 1799), Charles's adjutant. They had three children:Countess Ernestine Caroline Wilhelmine Albertine of Schönburg-Hinterglauchau (6 June 1748 – 21 March 1810); married in Berlin on 2 November 1770 to Count Frederick Louis Finck von Finckenstein (18 February 1745 – 18 April 1818).\nCount Frederick William Charles Ernest of Schönburg-Hinterglauchau (9 January 1751 – 17 June 1751).\nCount Christian William Charles Frederick Ernest of Schönburg-Hinterglauchau (14 June 1752 – 9 March 1770).In 1744, Charles was engaged to marry Maria Amalia of Hesse-Cassel (1721–1744), but she died before they could wed.  Upon his death in 1762, lacking legitimate heirs, his estate reverted to the crown.  After the Treaty of Hubertusburg, Frederick II granted these fortunes to the two officers for whom he had particular gratitude: Hans Sigismund von Lestwitz received the estate of Friedland, and Joachim Bernhard von Prittwitz, who had led the king from the battlefield in the Kunersdorf, received the estate at Quillitz.  Theodore Fontane gave this circumstance a special mention, by quoting a proverb:   \"Lestwitz a sauvé l'etat, Prittwitz a sauvé le roi.\" (Lestwitz saved the state, Prittwitz the king.) The staff officers of the Lestwitz regiment received a golden medal.\n\nNotes\nPassage 5:\nCharles Philip of Brandenburg-Schwedt\nMargrave Charles Philip of Brandenburg-Schwedt (5 January 1673 in Sparnberg – 23 July 1695 in Casale Monferrato) was a Hohenzollern prince and a titular Margrave of Brandenburg-Schwedt. Near the end of his life he became Grand Master of the Order of Saint John (Bailiwick of Brandenburg).\n\nLife\nCharles Philip was the third surviving son of the \"Great Elector\", Frederick William of Brandenburg (1620–1688) from his second marriage with Sophia Dorothea (1636–1689), the daughter of Philip, Duke of Schleswig-Holstein-Sonderburg-Glücksburg.In 1693, Charles Philip proved himself at the Battle of Neerwinden and was promoted to Lieutenant General by his brother Frederick I. He participated in the War of the Palatine Succession at the head of an auxiliary contingent.  He joined the main force of Victor Amadeus II of Sardinia in Turin.In Turin, he met Countess Caterina di Salmour (1670-1719), widow of Giovanni Gabaleone, Count di Salmour and daughter of Geofredo Alberico Balbiani, Marchese di Colcavagno by his wife, Marta-Maria Benso di Cavour, heiress of Isolabella. On the afternoon of 29 May 1695 three officers of Brandenburg's army, Col. Ludwig von Blumenthal, Lt. Col. von Hackeborn and Col. von Stille learned that Charles Philipp had lodged in the recently ruined Palace of Venaria, near Turin, where he was about to marry the Countess di Salmour in secret. They hurried towards La Venaria. As they neared the château, the Margrave’s Master of the Horse met them on the road and confirmed the rumour. The Margrave had invited a small gathering to his secret wedding, including three women who were friends of the Countess, her brother Flaminio Balbiano, and some local Torino notables; on the German side were a Prince of Hesse-Cassel and a Captain Beaupré, currently serving in Brandenburg’s army. The local priest Fr. Galli was summoned, and before him and in the presence of Abbot Alexander del Marro and the Chevalier Parella, they declared their determination to marry. But the priest refused to co-operate on the grounds that they were not his parishioners. The Abbot and Captain Beaupré fought; Staff intervened and the Margrave then fell upon the Master of Horse with drawn sword, who fled.Riding on, the three colonels came upon the Margrave, heading for the Countess’ house in Turin in his carriage with his escort. They joined the cavalcade, and when they reached the destination the Prince’s advisors implored him not to carry on. Neither the Elector of Brandenburg, nor the Duke of Savoy recognized the marriage. To avoid diplomatic complications, Duke Victor Amadeus imprisoned Caterina in a convent. The Curia supported Charles Philip's claim that the marriage was legal, in the hope that he would convert to Catholicism.  While the issue was still being debated, Charles Philip died of a fever or (it was said) of a broken heart. He was buried in the Hohenzollern family crypt in Berlin Cathedral.\nTwo years later, Rome ruled that the marriage was valid.  The Elector still did not recognize it.\nIn 1707, Caterina married the Saxon general Count August Christoph von Wackerbarth.\nPassage 6:\nMargrave Albert Frederick of Brandenburg-Schwedt\nAlbert Frederick, Prince of Prussia, Margrave of Brandenburg-Schwedt (24 January 1672 – 21 June 1731), was a Lieutenant General in the army of the Electorate of Brandenburg-Prussia and Grand Master of the Order of Saint John. In his lifetime he held the courtesy title of Margrave of Brandenburg. His elder brother Philip William held the town and lands of Schwedt.\n\nLife\nAlbert Frederick was born in Berlin, a son of Elector Frederick William of Brandenburg and his second wife Sophia Dorothea.  His brother Philip William was from 1692 to 1711 Governor of Magdeburg.  Albrecht Frederick joined the Prussian army as a volunteer in 1689, at the beginning of the War of the Palatine Succession against France.  On 10 May 1692 he became head of a cavalry regiment and on 14 March 1693, he was promoted to major general. In 1694 he participated in the campaign in Italy and was on 9 March 1695, he was promoted to lieutenant general.  The Margrave became in 1696 Grand Master of the Order of Saint John (Bailiwick of Brandenburg) and, on 17 January 1701, one of the first knights of the Order of the Black Eagle.\nBeginning 14 February 1702 he fought against France as head of an infantry regiment in the War of Spanish Succession as the commander of the Prussian corps in the Netherlands.  In November of that year he had to leave this post because of illness.  In 1706, he was appointed governor in Pomerania. He died at Friedrichsfelde Palace, aged 59.\n\nMarriage and issue\nOn 31 October 1703 Albert Frederick married with Princess Maria Dorothea Ketteler of Courland (1684–1743), daughter of Frederick Casimir, Duke of Courland. They had the following children:\n\nFrederick of Brandenburg-Schwedt (1704–1707)\nCharles Frederick Albert, Margrave of Brandenburg-Schwedt (1705–1762)\nAnna Sophie Charlotte of Brandenburg-Schwedt (1706–1751); married in 1723 Wilhelm Heinrich, Duke of Saxe-Eisenach (1691–1741)\nLuise Wilhelmine of Brandenburg-Schwedt (1709–1726)\nFrederick of Brandenburg-Schwedt (1710–1741), died in the Battle of Mollwitz as a Prussian colonel\nSophie Friederike Albertine of Brandenburg-Schwedt (1712–1750); married in 1733 Victor Frederick, Prince of Anhalt-Bernburg (1700–1765)\nFrederick William (1715–1744).\nPassage 7:\nHenrietta Maria of Brandenburg-Schwedt\nHenriette Maria of Brandenburg-Schwedt (2 March 1702 probably in Berlin – 7 May 1782 in Köpenick), was a granddaughter of the \"Great Elector\" Frederick William of Brandenburg.  She was the daughter of Philip William, Margrave of Brandenburg-Schwedt (1669-1711), the eldest son of the elector's second marriage with Sophia Dorothea of Schleswig-Holstein-Sonderburg-Glücksburg.  Her mother was Johanna Charlotte (1682-1750), the daughter of Prince John George II, Prince of Anhalt-Dessau.\n\nLife\nShe married on 8 December 1716 in Berlin to Hereditary Prince Frederick Louis of Württemberg (1698-1731), the only son of Duke Eberhard Louis of Württemberg.  The marriage produced two children:\n\nEberhard Frederick (1718-1719)\nLouise Frederica (1721-1791), married Frederick II, Duke of Mecklenburg-Schwerin.Henrietta Maria died on 7 May 1782, aged 81, and was buried in the crypt below the church of Köpenick Palace, where she had spent her years of widowhood.  Her daughter arranged for a black marble plate in the crypt to commemorate her mother.  In the 1960s, the coffin was cremated, with permission of the Hohenzollern family, and the formerly open-ended crypt (as described by Fontane) was walled off.  Her urn was buried below the black marble plate.\nPassage 8:\nMargrave Frederick William of Brandenburg-Schwedt (1715–1744)\nFrederick William of Brandenburg-Schwedt (18 March 1715 – 12 September 1744 in Prague) was a Prussian Major General and commander of the Guards on Foot.  He was the son of Margrave Albert Frederick of Brandenburg-Schwedt and his wife Maria Dorothea of Courland (1684-1743). In his lifetime he held the courtesy title of Margrave of Brandenburg. His first cousin of the same name (Frederick William) was of the senior line and held the town and lands of Schwedt.\n\nLife\nIn May 1719, when he was only four years old, he was awarded the Order of the Black Eagle.\nFrom 1734, he participated as a volunteer in the campaigns of the Prussian army.  During the War of the Austrian Succession, he was wounded in the Battle of Mollwitz.  His elder brother Frederick fell during this battle.\nIn 1740, the Guard on Foot were formed from the Infantry Regiment Nr. 15, and Frederick William was the first colonel of the new unit.  On May 16, 1743, he was promoted to major general and made commander of the Guard.\nDuring the Siege of Prague in 1744, he commanded the trenches.  The king was present when he was killed by a cannonball.  His body was transferred to Berlin and he was buried in the Hohenzollern crypt in Berlin Cathedral.\n\nFootnotes\nPassage 9:\nPhilip William, Margrave of Brandenburg-Schwedt\nPhilip William, Prince in Prussia (German: Philipp Wilhelm von Brandenburg-Schwedt; May 19, 1669, castle of Königsberg – December 19, 1711, castle of Schwedt) was a Prussian Prince,  was the first owner of the Prussian secundogeniture of Brandenburg-Schwedt and was governor of Magdeburg from 1692 to 1711.\n\nBiography\nPhilip William was the eldest son of the Great Elector and his second wife, Princess Sophia Dorothea of Schleswig-Holstein-Sonderburg-Glücksburg. One of her major endeavours was to ensure the financial security of her sons, mostly by the purchase of land. Shortly after the birth of Philip William, he was invested with his mother's dominion of Schwedt, later, the Brandenburg-Prussian government added the lands of Wildenbruch. Both dominions were improved by Princess Dorothea's care and investments. Following the death of his mother, Philip, in an accord of dating to 3 March 1692, reached agreement with his half-brother, the Elector Friedrich III, about income and lands left to him by the Great Elector, including the lordship, without sovereignty, of Halberstadt. Philip received for himself and his descendants guaranteed appanages generating an income of 24,000 thalers each year. Added revenue came in to the amount of 22,000 thalers from the rule of Schwedt, plus military salaries of about 20,000 thalers, so that with a total income of 66,000 crowns he was enabled to hold court, in some style, himself.\nHe held, like all the male members of his house, the courtesy title, Margrave of Brandenburg. After the coronation of his elder brother, Frederick, he became Prince in Prussia, Margrave of Brandenburg with the style Royal Highness. The nomenclature \"Brandenburg-Schwedt\" came into use in the 19th century, posthumously, to distinguish the lords of Schwedt from the main line of the Hohenzollerns. Philip William was the ancestor of the Schwedt branch of the Royal House of Hohenzollern. On 25 January 1699 Philip Wilhelm married Princess Johanna Charlotte of Anhalt-Dessau (1682–1750), daughter of John George II, Prince of Anhalt-Dessau. As a widow she became Abbess of the Imperial Abbey of Herford.\nPhilipp Wilhelm served as a general in the campaigns against France and was promoted in 1697 to Inspector-General of the artillery. His half-brother, Prince Elector Friedrich III (later King Frederick I of Prussia), also gave him the proprietorship of several regiments. During his time as governor of Magdeburg, he was raised by the University of Halle (Saale) to the post of \"Rector magnificentissimus”.\nPhilip's Berlin residence, the Margrave Weilersche Palace, was later used by Kaiser Wilhelm I. He was buried in the Berlin Cathedral, where most of the senior members of the House of Hohenzollern are buried.\nSince Philip's eldest son, Frederick William, was a minor at his death, the King of Prussia (Frederick I and Frederick William I) took over guardianship. With the death of his granddaughter, Anna Elisabeth Luise, the collateral line of Brandenburg-Schwedt became extinct in 1820.\n\nIssue\nFrederick William, Margrave of Brandenburg-Schwedt (1700–1771); married in 1734 Princess Sophia Dorothea of Prussia (1719–1765).\nMargravine Friederike Dorothea Henriette of Brandenburg-Schwedt (1700–1701).\nMargravine Henrietta Maria of Brandenburg-Schwedt (1702–1782); married in 1716 Hereditary Prince Frederick Louis of Württemberg (1698–1731).\nGeorge William of Brandenburg-Schwedt (* / † 1704).\nFrederick Henry, Margrave of Brandenburg-Schwedt (1709–1788); married in 1739 Princess Leopoldine Marie of Anhalt-Dessau (1716–1782).\nMargravine Charlotte of Brandenburg-Schwedt (1710–1712).\n\nAncestry\nPassage 10:\nMarie Amalie of Brandenburg\nMaria Amalia of Brandenburg-Schwedt (26 November 1670 in Cölln – 17 November 1739 at Bertholdsburg Castle in Schleusingen) was a princess from the Brandenburg-Schwedt line of the House of Hohenzollern and by marriage a Duchess of Saxe-Zeitz.\n\nFamily\nShe was the daughter of the \"Great Elector\" Frederick William of Brandenburg from his second marriage with Sophia Dorothea of Schleswig-Holstein-Sonderburg-Glücksburg, daughter of Duke Philip of Schleswig-Holstein-Sonderburg-Glücksburg.\n\nLife\nIn 1709, while she was a duchess, she visited the William Fountain, a medicinal spring in Schleusingen.  She promoted the development of Schleusingen as a spa.\nShe died in 1739, at the age of 68, at the castle in Schleusingen that had earlier served as the seat of the Counts of Henneberg-Schleusingen.  She had received this castle as her widow seat.  Via her daughter, she was related to the Landgraviate family in Hesse and on that basis, she was buried in the royal crypt in the Martinskirche, Kassel.\n\nMarriage and issue\nHer first marriage was on 20 August 1687 in Potsdam with Prince Charles of Mecklenburg-Güstrow, the son of the Duke Gustav Adolph of Mecklenburg-Güstrow and Magdalene Sibylle of Holstein-Gottorp.  They had one child, who was born on 15 March 1688 and died later that day.  Her husband also died that day.\nShe married her second husband on 25 June 1689 in Potsdam.  He was Duke Maurice William of Saxe-Zeitz, the son of Duke Maurice of Saxe-Zeitz and Dorothea Maria of Saxe-Weimar.  She survived him by 21 years.  They had the following children:\n\nFrederick William (Moritzburg, 26 March 1690 – Moritzburg, 15 May 1690).\nDorothea Wilhelmine (Moritzburg, 20 March 1691 – Kassel, 17 March 1743), married on 27 September 1717 to Landgrave William VIII of Hesse-Kassel.\nKaroline Amalie (Moritzburg, 24 May 1693 – Moritzburg, 5 September 1694).\nSophie Charlotte (Moritzburg, 25 April 1695 – Moritzburg, 18 June 1696).\nFrederick Augustus (Moritzburg, 12 August 1700 – Halle, 17 February 1710)./\n\nExternal links\nPublications by or about Marie Amalie of Brandenburg at VD 17\nJohann Hübner's ...Three hundred and thirty three and Genealogical Tables, Table 171", "answers": ["May 19, 1669"], "length": 4570, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "96500ef697df70106798988a9065594622cdeff67156cd20"}
{"input": "What nationality is the performer of song You Can?", "context": "Passage 1:\nDáithí Sproule\nDáithí Sproule (born 23 May 1950) is a guitarist and singer of traditional Irish music. He is the grandson of Frank Carney and uncle of singer Claire Sproule.\n\nBiography\nBorn and raised in Derry, Northern Ireland, at the age of 18 he moved to Dublin in Ireland, where he attended university. Growing up, he listened to Bob Dylan, Bert Jansch, the Beatles, British folk songs and traditional Irish music. It was in Dublin that he entered the music scene which was prominent in Ireland at the time. As a teenager he had met the Ó Domhnaill family during trips to the Gaeltacht area of Rann na Feirste in Co. Donegal, and while in Dublin they formed a band, Skara Brae who would go on to have a great effect on Irish traditional music.\nDáithí is well known as a guitarist and was one of the first guitarists to use the DADGAD guitar tuning for Irish music after the originator Davy Graham. In 1992 he joined Irish supergroup Altan with whom he sings and plays guitar. Of his use of DADGAD tuning, Sproule says, it \"just seemed to instantly gel with Irish music. The nature of the tuning meant that you didn't really produce anything that was terribly, drastically, offensively wrong to people. I was always a singer, but when I started playing with instrumentalists in sessions and pubs, I was able to develop a style by just playing along with them quietly and tactfully.\" He was deemed \"a seminal figure in Irish music\" by The Rough Guide to Irish Music.\nSproule is also a member of various other bands and has recorded further solo albums; he also teaches DADGAD guitar and traditional songs at the Center for Irish Music in St. Paul, Minnesota.\n\nDiscography\nSolo albums\nThe Crow in the Sun (2007)\nLost River, Vol. 1 (New Folk, 2011)\nA Heart Made of Glass (1995)\n\nwith Altan\nOther bands\nBright and Early (with Paddy O'Brien and Nathan Gourley - 2015 - New Folk Records)\nFrom Uig to Duluth (with Laura MacKenzie and Andrea Stern - 2014)\nThe Pinery (with Laura MacKenzie – 2009 – New Folk Records)\nSeanchairde (with Tara Bingham and Dermy Diamond – 2008 – New Folk Records)\nFingal (with Randal Bays and James Keane – 2008 – New Folk Records)\nSnug in the Blanket (with Jamie Gans and Paddy O'Brien – 2004)\nOverland (with Randal Bays – 2004)\nTrian II (with Liz Carroll and Billy McComiskey – 1995)\nA Thousand Farewells (with Martin and Christine Dowling – 1995)\nTrian (with Liz Carroll and Billy McComiskey – 1992)\nStranger at the Gate (with Paddy O'Brien – 1988)\nThe Iron Man (with Tommy Peoples – 1984)\nCarousel (with Seamus and Manus McGuire – 1984)\nSpring in the Air (with James Kelly and Paddy O'Brien – 1981)\nIs it Yourself?  (with James Kelly and Paddy O'Brien – 1979)\nSkara Brae (Skara Brae – 1971)\n\nGuest appearances\nFour & Eight String Favorites (Bone Tone Records) 2021 - Eric Mohring & Friends\nMerrijig Creek - Fintan Vallely\nSpinning Yarns (Two Tap Records) 2015 - Norah Rendell\nHeigh Ho, The Green Holly (New Folk Records) 2015 - Laura MacKenzie\nMinnesota Lumberjack Songs (Two Tap Records) 2011 - Brian Miller\nSide by Side (Dawros Music) 2010 - Liz and Yvonne Kane\n40 Acre Notch (New Folk Records) 2008 – the HiBs\nThe Essential Chieftains (RCA) 2006 – The Chieftains\nBlue Waltz 2004 – Julee Glaub\nEvidence (New Folk Records) 2003 – Laura MacKenzie\nOver the Water (Heart Productions) 2002 – Ross Sutter\nLittle Sparrow (Sugarhill) 2001 – Dolly Parton\nLost in the Loop (Green Linnet) 2001 – Liz Carroll\nShine (Swallowtail) 2001 – Katie McMahon\nPersevere 2000 – The Proclaimers\nWater from the Well (RCA) 2000 – The Chieftains\nTis the Season (Compass) 1997  – Laura MacKenzie\nIrish Women Musicians of America (Shanachie) 1995 – Cherish the Ladies\nHeartsongs (Sony) 1994 – Dolly Parton\nMamma, Will you Buy Me a Banana? (Heart Productions) 1991 – Ross Sutter\nBlue Mesa (Red House) 1989 – Peter Ostroushko\nLiz Carroll (Green Linnet) 1988 – with Liz Carroll\nSean O'Driscoll (Shanachie/Meadowlark) 1987 – Sean O'Driscoll\nCapel Street (Capelhouse) 1986 – James Kelly\nThe Streets of My Old Neighborhood (Rounder) 1983 – Peter Ostroushko\nSluz Duz Music (Rounder) 1982 – Peter Ostroushko\n\nCompilations\nA Harvest Home: Center for Irish Music Live Recordings, Vol. 5 2013\nStrings Across the North Shore 2009\nYoung Irish Musicians Weekend Live!  2008 – with James Kelly and Paddy O'Brien\nNew Folk Records Sampler 2007 (New Folk Records) 2007\nMasters of the Irish Guitar (Shanachie) 2006\nThe Independence Suite (Celtic Crossings) 2005 – with Randal Bays\nSimply Folk Sampler 3 (Wisconsin Public Radio) 2005\nFestival International des Arts Traditionnels de Québec (Folklore) 2004 – with Trian\nThe Ice Palace – Irish Originals from Minnesota (IMDA) 2001\nThe Last Bar – Irish Music from Minnesota (IMDA) 2000\nAlternate Tunings Guitar Collection (String Letter) 2000 – with Trian\nAs They Pass Through (Kieran's) 2000\nBest of Thistle and Shamrock, Vol. 1 (Hearts of Space) 1999 – with Altan\nCeltic Colours International Festival – the Second Wave (Stephen McDonald) 1999 – with Altan\nA Winter's Tale (Universal) 1998 – with Altan\nGaelic Roots (Kells) 1997 – with James Kelly, Paddy O'Brien and Gerry O'Connor\nCeltic Music from Mountain Stage (Blue Plate) 1997 – with Altan\nHunger No More (Éire Arts) 1997\nPassage 2:\nCaspar Babypants\nCaspar Babypants is the stage name of children's music artist Chris Ballew, who is also the vocalist and bassist of The Presidents of the United States of America.\n\nHistory\nBallew's first brush with children's music came in 2002, when he recorded and donated an album of traditional children's songs to the nonprofit Program for Early Parent Support titled \"PEPS Sing A Long!\" Although that was a positive experience for him, he did not consider making music for families until he met his wife, collage artist Kate Endle. Her art inspired Ballew to consider making music that \"sounded like her art looked\" as he has said. Ballew began writing original songs and digging up nursery rhymes and folk songs in the public domain to interpret and make his own. The first album, Here I Am!, was recorded during the summer of 2008 and released in February 2009.\nBallew began to perform solo as Caspar Babypants in the Seattle area in January 2009. Fred Northup, a Seattle-based comedy improvisor, heard the album and offered to play as his live percussionist.  Northrup also suggested his frequent collaborator Ron Hippe as a keyboard player. \"Frederick Babyshirt\" and \"Ronald Babyshoes\" were the Caspar Babypants live band from May 2009 to April 2012. Both Northup and Hippe appear on some of his recordings but since April 2012 Caspar Babypants has exclusively performed solo. The reasons for the change were to include more improvisation in the show and to reduce the sound levels so that very young children and newborns could continue to attend without being overstimulated. \nBallew has made two albums of Beatles covers as Caspar Babypants. Baby Beatles! came out in September 2013 and Beatles Baby! came out in September 2015.\nBallew runs the Aurora Elephant Music record label, books shows, produces, records, and masters the albums himself. Distribution for the albums is handled by Burnside Distribution in Portland, Oregon.\nCaspar Babypants has released a total of 17 albums. The 17th album, BUG OUT!, was released on May 1, 2020. His album FLYING HIGH! was nominated for a Grammy Award for Best Children's Album. All 17 of the albums feature cover art by Ballew's wife, Kate Endle.\n\"FUN FAVORITES!\" and \"HAPPY HITS!\" are two vinyl-only collections of hit songs that Caspar Babypants has released in the last couple of years.\n\nDiscography\nAlbumsPEPS (2002)\nHere I Am! (Released 03/17/09) Special guests: Jen Wood, Fysah Thomas\nMore Please! (Released 12/15/09) Special guests: Fred Northup, Ron Hippe\nThis Is Fun! (Released 11/02/10)  Special guests: Fred Northup, Ron Hippe, Krist Novoselic, Charlie Hope\nSing Along! (Released 08/16/11) Special guests: Fred Northup, Ron Hippe, \"Weird Al\" Yankovic, Stone Gossard, Frances England, Rachel Loshak\nHot Dog! (Released 04/17/12) Special guests: Fred Northup, Ron Hippe, Rachel Flotard (Visqueen)\nI Found You! (Released 12/18/12) Special guests: Steve Turner (Mudhoney), Rachel Flotard (Visqueen), John Richards\nBaby Beatles! (Released 09/15/13)\nRise And Shine! (Released 09/16/14)\nNight Night! (Released 03/17/15)\nBeatles Baby! (Released 09/18/2015)\nAway We Go! (Released 08/12/2016)\nWinter Party! (Released 11/18/16)\nJump For Joy! (Released 08/18/17)\nSleep Tight! (Released 01/19/18)\nKeep It Real! (Released 08/17/18)\nBest Beatles! (Released 03/29/19)\nFlying High! (Released 08/16/19)\nBug Out! (released 05/1/20)\nHappy Heart! (Released 11/13/20)\nEasy Breezy! (Released 11/05/21)AppearancesMany Hands: Family Music for Haiti CD (released 2010) – Compilation of various artists\nSongs Stories And Friends: Let's Go Play – Charlie Hope (released 2011) – vocals on Alouette\nShake It Up, Shake It Off (released 2012) – Compilation of various artists\nKeep Hoping Machine Running – Songs Of Woody Guthrie (released 2012) – Compilation of various artists\nApple Apple – The Harmonica Pocket (released 2013) – vocals on Monkey Love\nSimpatico – Rennee and Friends (released 2015) – writer and vocals on I Am Not Afraid\nSundrops – The Harmonica Pocket (released 2015) – vocals on Digga Dog Kid\nPassage 3:\nPanda (Astro song)\nAstro is the first album of long duration (after the EP Le disc of Astrou) of Chilean indie band Astro, released in 2011. The first single from the album was \"Ciervos\" and followed \"Colombo\", \"Panda\" and \"Manglares\".\nThis album was chosen by National Public Radio among the 50 discs of 2012.\n\nTrack listing\nAll tracks written by Andrés Nusser, except where noted.\n\nCiervos (Deer)\nCoco (Coconut)\nColombo\nDruida de las nubes (Druid of the clouds)\nPanda\nMiu-Miu\nManglares (Mangroves)\nMira, está nevando en las pirámides (Look, it's snowing in the pyramids)\nVolteretas (Tumbles)\nPepa\nNueces de Bangladesh (Nuts of Bangladesh)\nMiu-Miu reaparece (Miu-Miu reappears)\n\nPersonnel\nAstro\n\nAndrés Nusser – vocals, guitar\nOctavio Caviares – drums\nLego Moustache – keyboards, percussion\nZeta Moustache – keyboards, bassProduction\n\nAndrés Nusser – producer, recording and mixing\nChalo González – mixing and mastering\nCristóbal Carvajal – recording\nIgnacio Soto – recording\nPassage 4:\nMadleen Kane\nMadleen Kane (born Madeleine Flerkell 4 March 1958 in Malmö, Sweden), is a Swedish model and singer. A former Elite fashion model (height 180 cm / 5'11\" - weight 47 kg / 103 lbs), she worked since age 17 for the German fashion magazine Burda Moden. She was published seminaked in two issues of Playboy magazine (in April 1978 for French edition and in April 1979 for Spanish edition).  In addition, she had five Top 10 hits on the US Hot Dance Music/Club Play chart in the late 1970s and early 1980s.\n\nBiography and career\nAt the age of 20, Madleen was discovered by J.C. Friederich, owner of Boona Music productions.  She became a popular singer working in 1978 with her album Rough Diamond, which became popular not only in the U.S., but across the globe. After she released Cheri in 1979, her singing career took off. Madleen's debut album Rough Diamond (1978) was originally released in France by CBS and soon after by Warner Bros. in North America. It became a hit on the Billboard Dance Chart. For this album, she recorded a disco version of C'est si bon. Paris-based production team Michaele, Lana & Paul Sébastian produced the album. They have also worked with Theo Vaness and on \"Argentina Forever\" by Pacific Blue.\nCheri (1979), was Kane's 2nd CBS Disques S.A. / Warner Bros. release, which featured \"Forbidden Love\", a dramatic \"pop-opus\" arranged by Thor Baldursson. The A-side suite of \"Forbidden Love\", the title track, its breakdown \"Fire In My Heart\" and \"Secret Love Affair\" gave her another club hit, which ran for over 15 minutes. Jim Burgess remixed it for a single, which was edited to just over eight minutes. The ballad \"You and I\", has become a wedding day favourite in Canada.Unlike the North American albums, the French releases of Rough Diamond and Cheri had gatefold sleeves. In addition, the American mix of \"Forbidden Love\" is different from that released in other countries. The track \"I Want You, Need You, Love You\" was omitted from the North American release.\nAt the beginning of the 1980s, Madleen moved to Chalet Records, part of Prelude Records, and released her third album, Sounds Of Love (1980). It featured \"Cherchez Pas\", which was more \"electronic\" as opposed to her usual symphonic disco songs, and peak #18 in Sweden. Madleen later worked with producers Giorgio Moroder and Pete Bellotte. Nevertheless, Giorgio Moroder appeared with his mixes in 1981 with \"Don't wanna Lose You\" and helped her album sales in Clubs but no longer in radios. \"You Can\" (1981), the Flashdance-esque lead single from those sessions spent three weeks on top of the Billboard Dance charts. It also got to number 77 on the Billboard Hot 100 in February 1982 and was Kane's only entry on that chart. The album Don't Wanna Lose You followed. This was again released on Chalet Records, which was owned by her then-husband Jean-Claude Friederich and distributed by dance promoter Tom Hayden and his TSR Record Company, which was to be Madleen's next record label. Other big hits: \"Playing For Time,\" \"You Can,\" \"I'm No Angel,\" \"Fire in My Heart\".\nLondon's Ian Anthony Stephens and Megatone recording artist Paul Parker teamed up to provide Madleen with \"I'm No Angel\", a Billboard Dance Hit from her 1985 album, Cover Girl. Madleen stopped her career as family life became her priority and she raised three children.\nA collection of her hits, 12 Inches And More (1994) was her final release. 12 Inches And More does not feature any of the extended mixes from her early career. Album versions are used in place of the remixes, possibly due to licensing issues. In January 2010, Madleen's first two albums were reissued on the MP3 via Amazon.com.\nIn 2011, Gold Legion reissued her album \"Rough Diamond\" on cd.\nIn June 2016, Madleen performed in Miami at the Cafe Iguana in Miami, Florida. In 2018 she published her memoir \"Rough Diamond\" through Mindstir Media.\n\nDiscography\nAlbums\nRough Diamond (1978) Warner Bros.\nCheri (1979) Warner Bros.\nSounds of Love (1980) Chalet\nDon't Wanna Lose You (1981) Chalet\nCover Girl (1985) TSR\n12 Inches and More (1994) TSR\n\nSingles\n\"Rough Diamond\" (1978)\n\"Fever\" (1978)\n\"Touch My Heart\" (1979)\n\"Forbidden Love\" (1979)\n\"You and I\" (1979)\n\"Secret Love Affair\" (1979)\n\"Cheri\" (1979)\n\"Cherchez Pas\" (1980)\n\"Boogie Talk\" (1980)\n\"You Can\" (1981)\n\"Fire In My Heart\" (1981)\n\"Playing For Time\" (1982)\n\"On Fire\" (1985)\n\"Ecstasy\" (1985)\n\"I'm No Angel\" (1985)\n\nCharts\nSee also\nList of artists who reached number one on the US Dance chart\nPassage 5:\nKristian Leontiou\nKristian Leontiou (born February 1982) is an English singer. Formerly a solo artist, he is the lead singer of indie rock band One eskimO.\n\nEarly life\nKristian Leontiou was born in London, England and is of Greek Cypriot descent. He went to Hatch End High School in Harrow and worked several jobs in and around London whilst concentrating on music when he had any free time.  In 2003 he signed a major record deal with Polydor. At the time, Leontiou was dubbed \"the new Dido\" by some media outlets. His debut single \"Story of My Life\" was released in June 2004 and reached #9 in the UK Singles Chart. His second single \"Shining\" peaked at #13 whilst the album Some Day Soon was certified gold selling in excess of 150,000 copies.\nLeontiou toured the album in November 2004 taking him to the US to work with L.A Reid, Chairman of the Island Def Jam music group.  Unhappy with the direction his career was going, on a flight back from the US in 2004 he decided to take his music in a new direction.  Splitting from his label in late 2005, he went on to collaborate with Faithless on the song \"Hope & Glory\" for their album ‘'To All New Arrivals'’.  It was this release that saw him unleash the One eskimO moniker.  It was through working with Rollo Armstrong on the Faithless album, that Rollo got to hear an early demo of \"Astronauts\" from the One eskimO project. Being more than impressed by what he heard, Rollo opened both his arms and studio doors to Leontiou and they began to co-produce the ‘'All Balloons’' album.\nIt was at this time that he paired up with good friend Adam Falkner, a drummer/musician, to introduce a live acoustic sound to the album. They recorded the album with engineer Phill Brown (engineer for Bob Marley and Robert Plant) at Ark studios in St John's Wood where they recorded live then headed back to Rollo's studio to add the cinematic electro touches that are prominent on the album.\nShortly after its completion, One eskimO's \"Hometime\" was used on a Toyota Prius advert in the USA. The funds from the advert were then used to develop the visual aspect of One eskimO. He teamed up with friend Nathan Erasmus (Gravy Media Productions) along with animation team Smuggling Peanuts (Matt Latchford and Lucy Sullivan) who together began to develop the One eskimO world, the first animation produced was for the track ‘Hometime’ which went on to win a British animation award in 2008.\nIn 2008 Leontiou started a new management venture with ATC Music. By mid-2008 Time Warner came on board to develop all 10 One eskimO animations which were produced the highly regarded Passion Pictures in London.  Now with all animation complete and a debut album, One eskimO prepare to unveil themselves fully to the world in summer 2009.\nLeontiou released a cover version of Tracy Chapman's \"Fast Car\", which was originally released as a single in 2005. Leontiou's version was unable to chart, however, due to there being no simultaneous physical release alongside the download single, a UK chart rule that was in place at the time. On 24 April 2011, the song entered the singles chart at number 88 due to Britain's Got Talent contestant Michael Collings covering the track on the show on 16 April 2011.\n\nDiscography\nAlbums\nSingles\nNotes\nA  - Originally released as a single in April 2005, Leontiou's version of \"Fast Car\" did not chart until 2011 in the UK.\n\nAlso featured on\nNow That's What I Call Music! 58 (Story of My Life)\nWin a Date with Tad Hamilton! OST, Love Love Songs - The Ultimate Love Collection (Shining)\nSummerland OST (The Crying)\nPassage 6:\nBilly Milano\nBilly Milano (born June 3, 1964) is an American heavy metal and hardcore punk musician. He is the singer and occasionally guitarist and bassist of crossover thrash band M.O.D., and was the singer of its predecessor, Stormtroopers of Death. Prior to these bands, Milano played in early New York hardcore band the Psychos, which also launched the career of future Agnostic Front vocalist Roger Miret. Milano was also the singer of United Forces, which included his Stormtroopers of Death bandmate Dan Lilker. Milano managed a number of bands, including Agnostic Front, for whom he also co-produced the 1997 Epitaph Records release Something's Gotta Give and roadie for Anthrax.\n\nDiscography\nStormtroopers of Death albums\nStormtroopers of Death videos\nMethod of Destruction (M.O.D.)\nMastery\nPassage 7:\nO Valencia!\n\"O Valencia!\" is the fifth single by the indie rock band The Decemberists, and the first released from their fourth studio album, The Crane Wife.\nThe music was written by The Decemberists and the lyrics by Colin Meloy. It tells a story of two star-crossed lovers. The singer falls in love with a person who belongs to an opposing gang. At the end of the song, the singer's lover jumps in to defend the singer, who is confronting his lover's brother (the singer's \"sworn enemy\") and is killed by the bullet intended for the singer.\n\nTrack listing\nThe 7\" single sold in the UK was mispressed, with \"Culling of the Fold\" as the B-side despite the artwork and record label listing \"After the Bombs\" as the B-side.\n\nMusic videos\nFor the \"O Valencia!\" music video, The Decemberists filmed themselves in front of a green screen and asked fans to complete it by digitally adding in background images or footage. Stephen Colbert of The Colbert Report, having recently asked fans to do the same with a video of him with a light saber in front of a green screen, brought up The Decemberists on his segment \"Look Who's Riding on My Coattails Now\" and accused the band of stealing the idea. The Decemberists' response was to challenge Stephen Colbert to a guitar solo showdown on December 20, 2006, on The Colbert Report.On January 19, 2007, The Decemberists premiered an alternate music video of \"O Valencia!\", directed by Aaron Stewart-Ahn, on MTV2. The video follows a character named Patrick, played by Meloy, as he and his love Francesca (Lisa Molinaro), daughter of \"the Boss\", plan an escape to an unknown location. At a cafe, a man in a suit, portrayed by the band member Chris Funk, tells him to hide in the \"Valencia\" hotel (the Super Value Inn on North Interstate Avenue in Portland, Oregon) while he gets them the necessary documentation to escape. Above the name of the hotel, there is a neon sign that reads \"Office\". The letters have all burnt out except for the \"O\", creating the title of the song. The video then introduces other characters - various assassination teams - who sit in different rooms of the hotel waiting for the chance to catch the two lovers. Most are portrayed by other members of the band (along with Meloy's wife, Carson Ellis). They kill off any potential witnesses to their plan. Patrick manages to take down one member from each team, before they gang up on him. The Boss arrives, along with the man from the cafe, who reveals that he snitched on Patrick and Francesca. They execute Francesca, while forcing Patrick to watch. After they leave, Patrick finds a note by Francesca, which reveals that she never fell in love with him, and only wanted protection. 2 months later, Patrick and the man, who has lost an eye from a previous assassination attempt, have a sit-down at the same cafe. The man reveals that he snitched on Patrick just to take over the town. Patrick reveals that he poisoned a drink the man was having, but before he could get away, the man stabs Patrick in the neck with a fork before dying, followed by Patrick.\nThe video is somewhat influenced by the distinct style and themes of director Wes Anderson, with bold fonts being used to introduce characters and groups on the bottom of the screen (much like in the film The Royal Tenenbaums). The band had previously (and more explicitly) drawn influence from Anderson's Rushmore in their video for \"Sixteen Military Wives\". The layout of the hotel is also similar to the one used in Bottle Rocket.\nKurt Nishimura was chosen as the winner by mtvU for his video that depicted a love affair between a woman and her television, with the TV containing the green-screened Decemberists video footage.\nPassage 8:\nYou Can\n\"You Can\" is a 1981 single by Madleen Kane and produced by Giorgio Moroder. The song was written by Yolanda Yvette Adams, Donald Ray Atkins and Marcus Ecby.  Along with the track, \"Fire in My Heart\", \"You Can\" was Kane's most successful single on the dance charts, spending three weeks at number one.  The single was her only Hot 100 chart entry, peaking at #77.\n\nCharts\nWeekly charts\nPassage 9:\nBernie Bonvoisin\nBernard Bonvoisin (French pronunciation: ​[bɛʁnaʁ bɔ̃vwazɛ̃]), known as Bernie Bonvoisin (French pronunciation: ​[bɛʁni bɔ̃vwazɛ̃], born 9 July 1956 in Nanterre, Hauts-de-Seine), is a French hard rock singer and film director. He is best known for having been the singer of Trust.\nHe was one of the best friends of Bon Scott the singer of AC/DC and together they recorded the song \"Ride On\" which was one of the last songs by Bon Scott.\n\nExternal links\nBernie Bonvoisin at IMDb\nPassage 10:\nAstrid North\nAstrid North (Astrid Karina North Radmann; 24 August 1973, West Berlin – 25 June 2019, Berlin) was a German soul singer and songwriter. She was the singer of the German band Cultured Pearls, with whom she released five Albums. As guest singer of the band Soulounge she published three albums.\n\nCareer\nNorth had her first experiences as a singer with her student band Colorful Dimension in Berlin. In March 1992 she met B. La (Bela Braukmann) and Tex Super (Peter Hinderthür) who then studied at the Hochschule für Musik und Theater Hamburg and who were looking for a singer for their band Cultured Pearls. The trio entered the German charts with four singles and four albums.\n\nIn 1994 North sang for the dance-pop band Big Light on their hit single Trouble Is. In 1996 she was a guest on the side project Little Red Riding Hood by Fury in the Slaughterhouse brothers Kai and Thorsten Wingenfelder which resulted in the release of the single Life's Too Short from the eponymous album.The song Sleepy Eyes, texted and sung by North, appears in the soundtrack of the movie Tor zum Himmel (2003) by director Veit Helmer. In 2003 she appeared at the festival Das Fest in Karlsruhe and sang alongside her own songs a cover version of the Aerosmith hit Walk This Way together with the German singer Sasha. North also toured with the American singer Gabriel Gordon.After the end of her band Cultured Pearls in 2003 North moved 2004 to New York City to write new songs, work with a number of different musicians and to experiment with her music.In 2005 she joined the charity project Home, which produced an album for the benefit of the orphans from the Beluga School for Life in Thailand which have been affected by the Indian Ocean earthquake in 2004 and the subsequent tsunami. Beside the orphans themselves also the following artists have been involved, guitarist Henning Rümenapp (Guano Apes), Kai Wingenfelder (Fury in the Slaughterhouse), Maya Saban and others. With Bobby Hebb Astrid North recorded a new version of his classic hit Sunny. It was the first time Hebb sung this song as duett and it appeared on his last album That's All I Wanna Know.\nNorth sang in 2006 My Ride, Spring Is Near and No One Can Tell on the album The Ride by Basic Jazz Lounge, a project by jazz trumpeter Joo Kraus. In addition, she worked as a workshop lecturer of the Popkurs at the Hochschule für Musik und Theater Hamburg.\nIn spring 2010 North performed as the opening act of the Fakebling-Tour of Miss Platnum. The magazine Der Spiegel described her as one of the \"leading ladies of the local soul scene\". On 20 July 2012 her solo debut album North was released.\nOn 16 September 2016 Astrid North released her second solo album, Precious Ruby, dedicated to her grandmother Precious Ruby North. North used crowdfunding to finance the album. The first single published from this album was the song Miss Lucy. In 2016 she also started her concert series North-Lichter in Berlin's Bar jeder Vernunft to which she invited singers such as Katharina Franck, Elke Brauweiler, Lizzy Scharnofske, Mia Diekow, Lisa Bassenge or Iris Romen.\n\nLife\nAstrid North was born in West Berlin, West Germany to Sondria North and Wolf-Dieter Radmann. She commuted between her birth city and her family in Houston, Texas until she was nine years old. In the USA she lived mainly with her grandparents and her time there significantly shaped her musical development.Besides her music career Astrid North worked also as lecturer in Hamburg at the Hochschule für Musik und Theater and as yoga teacher. North was the mother of two children, her daughter was born in 2001 and her son in 2006. Her sister Ondria North works as make-up artist and hair stylist in the German film industry.\nShe died in June 2019 at the age of 45 years from pancreatic cancer.\n\nDiscography\nwith Cultured PearlsAlbums\n\n1996: Sing Dela Sing (German chart position 92, 3 weeks)\n1997: Space Age Honeymoon (German chart position 54, 6 weeks)\n1999: Liquefied Days (German chart position 19, 9 weeks)\n2002: Life on a Tuesday (German chart position 74, 1 week)Singles\n\n1996: Tic Toc (1996) (German chart position 65, 10 weeks)\n1997: Sugar Sugar Honey (German chart position 72, 9 weeks)\n1998: Silverball (German chart position 99, 2 weeks)\n1999: Kissing the Sheets (German chart position 87, 9 weeks)with Soulounge\n\n2003: The Essence of the Live Event – Volume One\n2004: Home\n2006: Say It AllSolo\n\n2005: Sunny (Single, Bobby Hebb feat. Astrid North)\n2012: North (Album, 20. Juli 2012)\n2013: North Live (Album, live recordings from different venues in Germany)\n2016: Sunny (Compilation, Bobby Hebb feat. Astrid North)\n2016: Precious Ruby (Album, 16. September 2016)as guest singer\n\n1994: Trouble Is – Big Light (Single)\n1996: Life's Too Short – Little Red Riding Hood (Single)\n2006: Basic Jazz Lounge: The Ride – Joo Kraus (Album)", "answers": ["Sweden"], "length": 4845, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "a3c88e70534d79b432d32c0b53d0d181c957b353f658dc95"}
{"input": "Which film has the director who died earlier, Melody Of The World or Ladies Love Danger?", "context": "Passage 1:\nWhite: Melody of Death\nWhite: Melody of Death (Korean: 화이트: 저주의 멜로디; RR: Hwaiteu: Jeojooui Mellodi, lit. White: The Melody of the Curse) is a 2011 South Korean horror film by Kim Gok and Kim Sun.\nThe film was pre-sold in Malaysia and Singapore with the teaser trailer and poster released at the Hong Kong Film Mart. The movie was a commercial success grossing US$ 5,3 Million and ending up being the highest-grossing horror movie and among Top 30 highest-grossing movies in South Korea in 2011.\n\nPlot\nThe girl group Pink Dolls, which consists of A-rang, Je-ni, Shin-ji, and Eun-ju, make their debut on stage but fail to achieve popularity. They and the record company moved to a renovated studio that was burnt down in a fire 15 years ago. Eun-ju's sponsor (someone who funds an idol or group on the condition they receive sexual favors) was credited for making the move, and renovations happen. Eun-ju is bullied by the other three members for her involvement with the sponsor and considers quitting. Her vocal trainer and best friend, Soon-ye encouraged her to remain in the group as she believes they will find success and gain attention with their new song. While cleaning up in the dance rehearsal room, Eun-ju finds a VHS tape titled \"WHITE\" and plays it in her dorm room. The footage is of an old, unreleased music video. When the group's manager finds her watching the tape, she demands to permitted its song to be remade as their next single.\nWhen Pink Dolls receives overnight popularity with their debut of the song \"White,\" which has become a viral hit, the manager seeks to re-record the song but with the main vocalist overtaking the song. The tension begins to rise as Je-ni, A-rang, and Shin-ji become hostile and jealousy against each other as they fight over the spot. During this time, a ghost attacks the three members on different occasions; Je-ni by strangling her with microphone cords, A-rang by causing her to fall off a music-video set, and Shin-ji by crushing her with camera equipment. Fearing the song is cursed and that she will be the next victim, Eun-ju examines hidden images within the video with Soon-ye and an editor and from there believe that a trainee named Jang Ye-bin, who died before the studio caught fire, wrote the song. Eun-ju meets up with her sponsor and asks about the circumstances surrounding Ye-bin and replies that she died by suicide. After returning to the rehearsal room in a fit of depression, Eun-Ju finds a suicide note beside power sockets that may have started the fire and she smashes the sockets with a hammer until she fell asleep in the morning.\nConfident that the curse is broken, Eun-ju wants to get more attention by reinventing \"White\" with a new image, including dressing and dying her hair in white, and using the stage name \"White\"; but she takes credit for the song to her solo performance, and alienates those around her. While Soon-ye was destroying the evidence, she re-watched the video and noticed new details they had never seen. While doing so, she and the editor learned that Je-ni, A-rang, and Shin-ji, who had been hosts for a music television show, died from drinking bleach on air. Soon-ye calls Eun-ju, who is on her way to a venue to perform \"White,\" and warns her that the curse is not over, but Eun-ju ignores her. Soon-ye soon learns that the writer of the song was not Ye-bin, but a back-up dancer who was bullied by Ye-bin by disfiguring her face with acid and caused the back-up dancer to commit suicide by drinking bleach. Her ghost killed Ye-bin, who caused the fire when attempting to burn the suicide note.\nSoon-ye rushes to the venue to rescue Eun-ju but is unable to enter the stage with all the doors locked. During Eun-ju's performance, the stage goes blackout, and the electricity begins to malfunctions. Eun-ju's sponsor and manager try to get her off the stage, but they are both killed by stage equipment, and the ghost attempts to attack her. Afterward, the doors all open, and the panicking crowd starts to rush out of the building, Soon-ye enters and she and Eun-ju attempt to reunite, but Eun-ju trips in the crowd and gets trampled to death. The electricity eventually sets the venue on fire. After the incident, Soon-ye destroyed all of the remaining evidence of the song in the studio's karaoke room. However, the karaoke machine announces that the next song playing is \"White,\" alluding to the possibility that the curse has not been broken.\n\nCast\nHam Eun-jeong as Eun-ju, a leader of the Pink Dolls, who is a former back-up dancer\nHwang Woo-seul-hye as Soon-ye, a vocal trainer, and Eun-ju's best friend\nMay Doni Kim as Shin-ji, a rapper/dancer who is excellent for dancing performance.\nChoi Ah-ra as A-rang, a visual/singer who is addicted to plastic surgery\nJin Se-yeon as Je-ni, a lead singer who is insecure with hitting high notes\nAfter School as Pure\nByun Jung-soo as Talent Agent\nKim Young-min as Lee Tae-Yong\nKim Ki-bang as Manager\nYoo Mo-ri as Jang Ye-bin\nKim Soo-hyun as White\nLee Jun-ho as Music Fever host\n\nSoundtrack\nThe soundtrack contains 3 versions of the song \"White,\" the original (the one featured on the VHS tape), another sung by Pink Dolls (Ham Eun-jeong, May Doni Kim, Choi Ah-ra and Jin Se-yeon), and a solo version with just Eun-jeong.\n\nReception\nBox office\nThe film grossed US$1,265,702 its opening weekend landing at the fifth position of the box office chart. In total the film grossed US$5,299,831 by the end of its theatrical run. The film received a total of 791,133 admissions nationwide.\n\nAccolades\nListicles\nRelease\nWhite was released in Japan as a DVD on March 02, 2012 by NBC Universal. A re-issue ws released in the same country on July 21, 2017.\nPassage 2:\nElliot Silverstein\nElliot Silverstein (born August 3, 1927) is a retired American film and television director. He directed the Academy Award-winning western comedy Cat Ballou (1965), and other films including The Happening (1967), A Man Called Horse (1970), Nightmare Honeymoon (1974), and The Car (1977). His television work includes four episodes of The Twilight Zone (1961–1964).\n\nCareer\nElliot Silverstein was the director of six feature films in the mid-twentieth century. The most famous of these by far is Cat Ballou, a comedy-western starring Jane Fonda and Lee Marvin.\nThe other Silverstein films, in chronological order, are The Happening, A Man Called Horse,  Nightmare Honeymoon, The Car, and Flashfire.\nOther work included directing for the television shows The Twilight Zone, The Nurses, Picket Fences, and Tales from the Crypt.\nWhile Silverstein was not a prolific director, his films were often decorated. Cat Ballou, for instance, earned one Oscar and was nominated for four more. His high quality work was rewarded in 1990 with a Lifetime Achievement Award by the Directors Guild of America.\n\nAwards\nIn 1965, at the 15th Berlin International Film Festival, he won the Youth Film Award – Honorable Mention, in the category of Best Feature Film Suitable for Young People for Cat Ballou.\nHe was also nominated for the Golden Berlin Bear.In 1966, he was nominated for the DGA Award in the category for Outstanding Directorial Achievement in Motion Pictures (Cat Ballou).\nIn 1971, he won the Bronze Wrangler award at the Western Heritage Awards in the category of Theatrical Motion Picture for A Man Called Horse, along with producer Sandy Howard, writer Jack DeWitt, and actors Judith Anderson, Jean Gascon, Corinna Tsopei and Richard Harris.In 1985, he won the Robert B. Aldrich Achievement Award from the Directors Guild of America.\nIn 1990, he was awarded the DGA Honorary Life Member Award.\n\nPersonal life\nSilverstein has been married three times, each ending in divorce. His first marriage was to Evelyn Ward in 1962; the couple divorced in 1968. His second marriage was to Alana King. During his first marriage, he was the step-father of David Cassidy.\nHe currently lives in North Hollywood, Los Angeles. Actively retired, Silverstein has taught film at USC and continues to work on screen plays and other projects.\n\nFilmography\nTales from the Crypt (TV Series) (1991–94)\nPicket Fences (TV Series) (1993)\nRich Men, Single Women (TV Movie) (1990)\nFight for Life (TV Movie) (1987)\nNight of Courage (TV Movie) (1987)\nBetrayed by Innocence (TV Movie) (1986)\nThe Firm (TV Series) (1982–1983)\nThe Car (1977)\nNightmare Honeymoon (1974)\nA Man Called Horse (1970)\nThe Happening (1967)\nCat Ballou (1965)\nKraft Suspense Theatre (TV Series) (1963–64)\nThe Defenders (TV Series) (1962–64)\nArrest and Trial (TV Series) (1964)\nThe Doctors and the Nurses (TV Series) (1962–64)\nTwilight Zone (TV Series) (1961–64)\nBreaking Point (TV Series) (1963)\nDr. Kildare (TV Series) (1961–63)\nThe Dick Powell Theatre (TV Series) (1962)\nBelle Sommers (TV Movie) (1962)\nNaked City (TV Series) (1961–62)\nHave Gun - Will Travel (TV Series) (1961)\nRoute 66 (TV Series) (1960–61)\nCheckmate (TV Series) (1961)\nThe Westerner (TV Series) (1960)\nAssignment: Underwater (TV Series) (1960)\nBlack Saddle (TV Series) (1960)\nSuspicion (TV Series) (1958)\nOmnibus (TV Series) (1954–56)\nPassage 3:\nBen Palmer\nBen Palmer (born 1976) is a British film and television director.\nHis television credits include the Channel 4 sketch show Bo' Selecta! (2002–2006), the second and third series of the E4 sitcom The Inbetweeners (2009–2010) and the Sky Atlantic comedy-drama Breeders (2020). Palmer has also directed films such as the Inbetweeners spin-off, The Inbetweeners Movie (2011) and the romantic comedy Man Up (2015).\n\nBiography\nPalmer was born and raised in Penny Bridge, Barrow-in-Furness. He attended Chetwynde School.His first directing job was the Channel 4 sketch show Bo' Selecta!, which he co-developed with its main star, Leigh Francis. Palmer directed the second and third series of the E4 sitcom The Inbetweeners in 2009 and 2010, respectively.\n\nFilmography\nBo' Selecta! (2002–06)\nComedy Lab (2004–2010)\nBo! in the USA (2006)\nThe Inbetweeners (2009–2010)\nThe Inbetweeners Movie (2011)\nComedy Showcase (2012)\nMilton Jones's House of Rooms (2012)\nThem from That Thing (2012)\nBad Sugar (2012)\nChickens (2013)\nLondon Irish (2013)\nMan Up (2015)\nSunTrap (2015)\nBBC Comedy Feeds (2016)\nNigel Farage Gets His Life Back (2016)\nBack (2017)\nComedy Playhouse (2017)\nUrban Myths (2017–19)\nClick & Collect (2018)\nSemi-Detached (2019)\nBreeders (2020)\nPassage 4:\nWalter Ruttmann\nWalter Ruttmann (28 December 1887 – 15 July 1941) was a German cinematographer and film director, an important German  abstract experimental film maker, along with Hans Richter, Viking Eggeling and Oskar Fischinger. He is best known for directing the semi-documentary 'city symphony' silent film, with orchestral score by Edmund Meisel, in 1927, Berlin: Symphony of a Metropolis. His audio montage Wochenende (Weekend) (1930) is considered a major contribution in the development of audio plays.\n\nBiography\nRuttmann was born in Frankfurt am Main, the son of a wealthy mercantilist. He graduated high school in 1905 and began architectural studies in Zürich in 1907. In 1909 Ruttmann began painting in Munich, where he befriended Paul Klee and Lyonel Feininger, and he would later paint in Marburg.Ruttmann was conscripted into the army in 1913, first serving in Darmstadt, and shortly after the outbreak of World War I he was sent to the Eastern Front, where he served as an artillery lieutenant and a gas defense officer. After spending 1917 in a hospital for post traumatic stress disorder, he began making films. Ruttmann had the financial means to work independently of the major German studios of the time. He founded Ruttmann-Film S.R.O. in Munich and patented an animation table, in June 1920.\nHis first productions were the first fully animated German cartoons and abstract animated films. Lichtspiel: Opus I, produced between 1919 and 1921, premiered on 27 April 1921 at the Berlin Marmorhaus, and released for German theatrical distribution in 1922, is the \"oldest fully abstract motion picture known to survive, using only animated geometric forms, arranged and shown without reference to any representational imagery\".\nOpus I and Opus II, were experiments with new forms of film expression, and the influence of these early abstract films can be seen in some of the early work of Oskar Fischinger. Ruttmann and his colleagues of the avant garde movement enriched the language of film as a medium with new formal techniques.In 1926 he worked with Julius Pinschewer on Der Aufsteig, an experimental film advertising the GeSoLei trade fair in Düsseldorf.In 1926, Ruttmann licensed a Wax Slicing machine from Oskar Fischinger to create special effects for  The Adventures of Prince Achmed, an  animated fairy tale film, for Lotte Reiniger, making the moving backgrounds and magic scenes.Ruttmann was a prominent exponent of both avant-garde art and music. His early abstractions played at the 1929 Baden-Baden Festival to international acclaim despite their being almost eight years old. Together with Erwin Piscator, he worked on the film Melody of the World (1929), though he is best remembered for Berlin: Die Sinfonie der Großstadt (Berlin: Symphony of a Metropolis, 1927).\nWeekend (Wochenende), commissioned in 1928 by Berlin Radio Hour, and presented on 13 June 1930, is a pioneering work of musique concrète, a montage of sound clips, recorded using film optical sound track from the Tri-Ergon process. Ruttmann recorded the streets sounds of Berlin with a camera, but without images, this was before magnetic tape. Hans Richter called it “a symphony of sound, speech-fragments, and silence woven into a poem.”A pacifist, he traveled to Moscow in 1928 and 1929. During the Nazi period he was replaced by Leni Riefenstahl as director of the documentary which eventually became Triumph of the Will (1935), supposedly because Ruttmann's editing style was considered too \"Marxist\" and Soviet influenced. He died in Berlin 15 July 1941 due to an embolism after leg amputation.\n\nCulture and Media\nSegments from Ruttmann's experimental films Lichtspiel: Opus II (1923) and Lichtspiel: Opus IV (1925) are used in the credits of the German neo-noir television series Babylon Berlin. Soundtracks to sped-up versions of Lichtspiel: Opus I and Opus IV have been proposed in 2023.\n\nSelect filmography\nLichtspiel: Opus I (1920)\nDer Sieger (1922)\nDas Wunder (1922)\nLichtspiel: Opus II (1922)\nLichtspiel: Opus III (1924, with Lore Leudesdorff)\nLichtspiel: Opus IV (1925, with Lore Leudesdorff)\nDas wiedergefundene Paradies (1925)\nDer Aufstieg (1926)\nSpiel der Wellen (1926)\nDort wo der Rhein... (1927)\nBerlin: Die Sinfonie der Großstadt (1927)\nMelody of the World (Melodie der Welt) (1929)\nWochenende (1930) [an experimental film with sound only, no image]\nFeind im Blut (1931)\nIn der Nacht (1931)\nSteel (1933)\nBlut und Boden - Grundlagen zum neuen Reich\nAltgermanische Bauernkultur (1934)\nMetall des Himmels (1935)\nSchiff in Not (1936)\nMannesmann (1937)\nHenkel, ein deutsches Werk in seiner Arbeit (1938)\nWaffenkammern Deutschlands (1940)\nDeutsche Panzer (1940)\nKrebs (1941)\n\nFurther reading\nCowan, Michael. Walter Ruttmann and the Cinema of Multiplicity: Avant-garde-Advertising-Modernity. Amsterdam, NL: Amsterdam University Press, 2014. ISBN 9789089645852\nDombrug, Adrianus van. Walter Ruttmann in het beginsel. Purmerend, NL: J. Muusses, 1956.\nGoergen, Jeanpaul. Walter Ruttmann: Eine Dokumentation. Berlin: Freunde der deutschen Kinemathek, 1989.  ISBN 9783927876002\nRogers, Holly and Jeremy Barham The Music and Sound of Experimental Film. Oxford: Oxford University Press, 2017.  ISBN 9780190469900\nQuaresima, Leonard, editor. Walter Ruttmann: Cinema, pittura, ars acustica. Calliano (Trento), Italy: Manfrini, 1994. ISBN 9788870245035\nPassage 5:\nMelody of Death\nMelody of Death is a 1922 British silent crime film directed by Floyd Martin Thornton and starring Philip Anthony, Enid R. Reed and Dick Sutherd. It is an adaptation of the 1915 novel The Melody of Death by Edgar Wallace.\n\nCast\nPhilip Anthony as Gilbert Standerton\nEnid R. Reed as Enid Cathcart\nDick Sutherd as George Wallis\nH. Agar Lyons as Sir John Standerton\nFrank Petley\nHetta Bartlett as Mrs Cathcart\nBob Vallis\nPassage 6:\nH. Bruce Humberstone\nH. Bruce \"Lucky\" Humberstone (November 18, 1901 – October 11, 1984) was an American film director. He was previously a movie actor (as a child), a script clerk, and an assistant director, working with directors such as King Vidor, Edmund Goulding, and Allan Dwan.\n\nEarly years\nHumberstone was born in Buffalo, New York, and attended Miami Military Academy in Miami, Florida.\n\nFilm\nOne of 28 founders of the Directors Guild of America, Humberstone worked on several silent movie films for 20th Century Fox. Humberstone did not specialize; he worked on comedies, dramas, and melodramas. Humberstone is best known today for the seminal film noir I Wake Up Screaming (1941) and his work on some of the Charlie Chan films. In the 1950s, Humberstone worked mostly on TV. He retired in 1966.\n\nRecognition\nHumberstone has a star on the Hollywood Walk of Fame.\n\nDeath\nHumberstone died of pneumonia in Woodland Hills, California, on October 11, 1984, aged 82, and was buried at the Hollywood Forever Cemetery in Hollywood, California.\n\nPartial filmography as director\nPassage 7:\nLadies Love Danger\nLadies Love Danger is a 1935 American comedy film directed by H. Bruce Humberstone and written by Samson Raphaelson, Robert Ellis and Helen Logan. The film stars Mona Barrie, Gilbert Roland, Donald Cook, Adrienne Ames, Hardie Albright and Herbert Mundin. The film was released on May 3, 1935, by Fox Film Corporation.\n\nPlot\nCast\nPassage 8:\nAbhishek Saxena\nAbhishek Saxena is an Indian Bollywood and Punjabi film director who directed the movie Phullu. The Phullu movie was released in theaters on 16 June 2017, in which film Sharib Hashmi is the lead role. Apart from these, he has also directed Patiala Dreamz, this is a Punjabi film. This film was screened in cinemas in 2014.\n\nLife and background\nAbhishek Saxena was born on 19 September 1988 in the capital of India, Delhi, whose father's name is Mukesh Kumar Saxena. Abhishek Saxena married Ambica Sharma Saxena on 18 December 2014. His mother's name is Gurpreet Kaur Saxena.\nSaxena started his career with a Punjabi film Patiala Dreamz, after which he has also directed a Hindi film Phullu, which has appeared in Indian cinemas on 16 June 2017.\n\nCareer\nAbhishek Saxena made his film debut in 2011 as an assistant director on Doordarshan with Ashok Gaikwad. He made his first directed film Patiala Dreamz, this is a Punjabi movie.After this, he has also directed a Hindi film Phullu in 2017, which has been screened in cinemas on 16 June 2017. Saxena is now making his upcoming movie \"India Gate\".\nIn 2018 Abhishek Saxena has come up with topic of body-shaming in his upcoming movie Saroj ka Rishta.\n Where Sanah Kapoor will play the role of Saroj and actors Randeep Rai and Gaurav Pandey will play the two men in Saroj's life.Yeh Un Dinon ki Baat Hai lead Randeep Rai will make his Bollywood debut. Talking about the film, director Abhishek Saxena told Mumbai Mirror, \"As a fat person, I have noticed that body-shaming doesn’t happen only with those who are on the heavier side, but also with thin people. The idea germinated from there.\"\nCareer as an Assistant DirectorApart from this, he has played the role of assistant director in many films and serials in the beginning of his career, in which he has a television serial in 2011, Doordarshan, as well as in 2011, he also assisted in a serial of Star Plus.\nIn addition to these serials, he played the role of assistant director in the movie \"Girgit\" which was made in Telugu language.\n\nFilmography\nAs Director\nPassage 9:\nBrian Kennedy (gallery director)\nBrian Patrick Kennedy (born 5 November 1961) is an Irish-born art museum director who has worked in Ireland and Australia, and now lives and works in the United States.  He was the director of the Peabody Essex Museum in Salem for 17 months, resigning December 31, 2020. He was the director of the Toledo Museum of Art in Ohio from 2010 to 2019. He was the director of the Hood Museum of Art from 2005 to 2010, and the National Gallery of Australia (Canberra) from 1997 to 2004.\n\nCareer\nBrian Kennedy currently lives and works in the United States after leaving Australia in 2005 to direct the Hood Museum of Art at Dartmouth College. In October 2010 he became the ninth Director of the Toledo Museum of Art. On 1 July 2019, he succeeded Dan Monroe as the executive director and CEO of the Peabody Essex Museum.\n\nEarly life and career in Ireland\nKennedy was born in Dublin and attended Clonkeen College. He received B.A. (1982), M.A. (1985) and PhD (1989) degrees from University College-Dublin, where he studied both art history and history.\nHe worked in the Irish Department of Education (1982), the European Commission, Brussels (1983), and in Ireland at the Chester Beatty Library (1983–85), Government Publications Office (1985–86), and Department of Finance (1986–89). He married Mary Fiona Carlin in 1988.He was Assistant Director at the National Gallery of Ireland in Dublin from 1989 to 1997. He was Chair of the Irish Association of Art Historians from 1996 to 1997, and of the Council of Australian Art Museum Directors from 2001 to 2003. In September 1997 he became Director of the National Gallery of Australia.\n\nNational Gallery of Australia (NGA)\nKennedy expanded the traveling exhibitions and loans program throughout Australia, arranged for several major shows of Australian art abroad, increased the number of exhibitions at the museum itself and oversaw the development of an extensive multi-media site.  Although he oversaw several years of the museum's highest ever annual visitation, he discontinued the emphasis of his predecessor, Betty Churcher, on showing \"blockbuster\" exhibitions.\nDuring his directorship, the NGA gained government support for improving the building and significant private donations and corporate sponsorship. However, the initial design for the building proved controversial generating a public dispute with the original architect on moral rights grounds.  As a result, the project was not delivered during Dr Kennedy's tenure, with a significantly altered design completed some years later.  Private funding supported two acquisitions of British art, including David Hockney's A Bigger Grand Canyon in 1999, and Lucian Freud's After Cézanne in 2001. Kennedy built on the established collections at the museum by acquiring the Holmgren-Spertus collection of Indonesian textiles; the Kenneth Tyler collection of editioned prints, screens, multiples and unique proofs; and the Australian Print Workshop Archive. He was also notable for campaigning for the construction of a new \"front\" entrance to the Gallery, facing King Edward Terrace, which was completed in 2010 (see reference to the building project above).\nKennedy's cancellation of the \"Sensation exhibition\" (scheduled at the NGA from 2 June 2000 to 13 August 2000) was controversial, and seen by some as censorship. He claimed that the decision was due to the exhibition being \"too close to the market\" implying that a national cultural institution cannot exhibit the private collection of a speculative art investor. However, there were other exhibitions at the NGA during his tenure, which could have raised similar concerns. The exhibition featured the privately owned Young British Artists works belonging to Charles Saatchi and attracted large attendances in London and Brooklyn. Its most controversial work was Chris Ofili's The Holy Virgin Mary, a painting which used elephant dung and was accused of being blasphemous. The then-mayor of New York, Rudolph Giuliani, campaigned against the exhibition, claiming it was \"Catholic-bashing\" and an \"aggressive, vicious, disgusting attack on religion.\" In November 1999, Kennedy cancelled the exhibition and stated that the events in New York had \"obscured discussion of the artistic merit of the works of art\". He has said that it \"was the toughest decision of my professional life, so far.\"Kennedy was also repeatedly questioned on his management of a range of issues during the Australian Government's Senate Estimates process - particularly on the NGA's occupational health and safety record and concerns about the NGA's twenty-year-old air-conditioning system. The air-conditioning was finally renovated in 2003. Kennedy announced in 2002 that he would not seek extension of his contract beyond 2004, accepting a seven-year term as had his two predecessors.He became a joint Irish-Australian citizen in 2003.\n\nToledo Museum of Art\nThe Toledo Museum of Art is known for its exceptional collections of European and American paintings and sculpture, glass, antiquities, artist books, Japanese prints and netsuke. The museum offers free admission and is recognized for its historical leadership in the field of art education.  During his tenure, Kennedy has focused the museum's art education efforts on visual literacy, which he defines as \"learning to read, understand and write visual language.\"   Initiatives have included baby and toddler tours, specialized training for all staff, docents, volunteers and the launch of a website, www.vislit.org. In November 2014, the museum hosted the International Visual Literacy Association (IVLA) conference, the first Museum to do so. Kennedy has been a frequent speaker on the topic, including 2010 and 2013 TEDx talks on visual and sensory literacy.\nKennedy has expressed an interest in expanding the museum's collection of contemporary art and art by indigenous peoples. Works by Frank Stella, Sean Scully, Jaume Plensa, Ravinder Reddy and Mary Sibande have been acquired.  In addition, the museum has made major acquisitions of Old Master paintings by Frans Hals and Luca Giordano.During his tenure the Toledo Museum of Art has announced the return of several objects from its collection due to claims the objects were stolen and/or illegally exported prior being sold to the museum.  In 2011 a Meissen sweetmeat stand was returned to Germany followed by an Etruscan Kalpis or water jug to Italy (2013), an Indian sculpture of Ganesha (2014) and an astrological compendium to Germany in 2015.\n\nHood Museum of Art\nKennedy became Director of the Hood Museum of Art in July 2005. During his tenure, he implemented a series of large and small-scale exhibitions and oversaw the production of more than 20 publications to bring greater public attention to the museum's remarkable collections of the arts of America, Europe, Africa, Papua New Guinea and the Polar regions. At 70,000 objects, the Hood has one of the largest collections on any American college of university campus. The exhibition, Black Womanhood: Images, Icons, and Ideologies of the African Body, toured several US venues. Kennedy increased campus curricular use of works of art, with thousands of objects pulled from storage for classes annually. Numerous acquisitions were made with the museum's generous endowments, and he curated several exhibitions: including Wenda Gu: Forest of Stone Steles: Retranslation and Rewriting Tang Dynasty Poetry, Sean Scully: The Art of the Stripe, and Frank Stella: Irregular Polygons.\n\nPublications\nKennedy has written or edited a number of books on art, including:\n\nAlfred Chester Beatty and Ireland 1950-1968: A study in cultural politics, Glendale Press (1988), ISBN 978-0-907606-49-9\nDreams and responsibilities: The state and arts in independent Ireland, Arts Council of Ireland (1990), ISBN 978-0-906627-32-7\nJack B Yeats: Jack Butler Yeats, 1871-1957 (Lives of Irish Artists), Unipub (October 1991), ISBN 978-0-948524-24-0\nThe Anatomy Lesson: Art and Medicine (with Davis Coakley), National Gallery of Ireland (January 1992), ISBN 978-0-903162-65-4\nIreland: Art into History (with Raymond Gillespie), Roberts Rinehart Publishers (1994), ISBN 978-1-57098-005-3\nIrish Painting, Roberts Rinehart Publishers (November 1997),  ISBN 978-1-86059-059-7\nSean Scully: The Art of the Stripe, Hood Museum of Art (October 2008), ISBN 978-0-944722-34-3\nFrank Stella: Irregular Polygons, 1965-1966, Hood Museum of Art (October 2010), ISBN 978-0-944722-39-8\n\nHonors and achievements\nKennedy was awarded the Australian Centenary Medal in 2001 for service to Australian Society and its art. He is a trustee and treasurer of the Association of Art Museum Directors, a peer reviewer for the American Association of Museums and a member of the International Association of Art Critics. In 2013 he was appointed inaugural eminent professor at the University of Toledo and received an honorary doctorate from Lourdes University. Most recently, Kennedy received the 2014 Northwest Region, Ohio Art Education Association award for distinguished educator for art education.\n\n\n== Notes ==\nPassage 10:\nMelody of the World\nMelody of the World (German: Melodie der Welt) is a 1929 German film directed by Walter Ruttmann. It is also known as World Melody. The film is structured like a symphony and consists of documentary footage from all over the world, contrasted and juxtaposed to show a number of human activities as they take form in different cultures. There are also staged scenes with actors.\nThe film was produced by Tonbild-Syndikat AG as an assignment from Hapag. It has an original score by Wolfgang Zeller. It was advertised as Germany's first feature-length sound film.\n\nCast\nIvan Koval-Samborskij as sailor\nRenée Stobrawa as sailor's wife\nGrace Chiang as Japanese woman\nO. Idris as Malayan temple dancer\nWilhelm Cuno as general director of Hapag\n\nRelease\nThe world premiere took place on 27 July 1929 at the Deutsches Kammermusikfest in Baden-Baden. The film was released in regular German cinemas on 10 May 1930, distributed by Deutsches Lichtspiel-Syndikat AG.\n\nSee also\nList of early sound feature films (1926–1929)", "answers": ["Melody Of The World"], "length": 4784, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "c94e0220a7c5dadee3094df576df766b6d1a15b6c6e21011"}
{"input": "Where was the performer of song Égérie (Song) born?", "context": "Passage 1:\nAstrid North\nAstrid North (Astrid Karina North Radmann; 24 August 1973, West Berlin – 25 June 2019, Berlin) was a German soul singer and songwriter. She was the singer of the German band Cultured Pearls, with whom she released five Albums. As guest singer of the band Soulounge she published three albums.\n\nCareer\nNorth had her first experiences as a singer with her student band Colorful Dimension in Berlin. In March 1992 she met B. La (Bela Braukmann) and Tex Super (Peter Hinderthür) who then studied at the Hochschule für Musik und Theater Hamburg and who were looking for a singer for their band Cultured Pearls. The trio entered the German charts with four singles and four albums.\n\nIn 1994 North sang for the dance-pop band Big Light on their hit single Trouble Is. In 1996 she was a guest on the side project Little Red Riding Hood by Fury in the Slaughterhouse brothers Kai and Thorsten Wingenfelder which resulted in the release of the single Life's Too Short from the eponymous album.The song Sleepy Eyes, texted and sung by North, appears in the soundtrack of the movie Tor zum Himmel (2003) by director Veit Helmer. In 2003 she appeared at the festival Das Fest in Karlsruhe and sang alongside her own songs a cover version of the Aerosmith hit Walk This Way together with the German singer Sasha. North also toured with the American singer Gabriel Gordon.After the end of her band Cultured Pearls in 2003 North moved 2004 to New York City to write new songs, work with a number of different musicians and to experiment with her music.In 2005 she joined the charity project Home, which produced an album for the benefit of the orphans from the Beluga School for Life in Thailand which have been affected by the Indian Ocean earthquake in 2004 and the subsequent tsunami. Beside the orphans themselves also the following artists have been involved, guitarist Henning Rümenapp (Guano Apes), Kai Wingenfelder (Fury in the Slaughterhouse), Maya Saban and others. With Bobby Hebb Astrid North recorded a new version of his classic hit Sunny. It was the first time Hebb sung this song as duett and it appeared on his last album That's All I Wanna Know.\nNorth sang in 2006 My Ride, Spring Is Near and No One Can Tell on the album The Ride by Basic Jazz Lounge, a project by jazz trumpeter Joo Kraus. In addition, she worked as a workshop lecturer of the Popkurs at the Hochschule für Musik und Theater Hamburg.\nIn spring 2010 North performed as the opening act of the Fakebling-Tour of Miss Platnum. The magazine Der Spiegel described her as one of the \"leading ladies of the local soul scene\". On 20 July 2012 her solo debut album North was released.\nOn 16 September 2016 Astrid North released her second solo album, Precious Ruby, dedicated to her grandmother Precious Ruby North. North used crowdfunding to finance the album. The first single published from this album was the song Miss Lucy. In 2016 she also started her concert series North-Lichter in Berlin's Bar jeder Vernunft to which she invited singers such as Katharina Franck, Elke Brauweiler, Lizzy Scharnofske, Mia Diekow, Lisa Bassenge or Iris Romen.\n\nLife\nAstrid North was born in West Berlin, West Germany to Sondria North and Wolf-Dieter Radmann. She commuted between her birth city and her family in Houston, Texas until she was nine years old. In the USA she lived mainly with her grandparents and her time there significantly shaped her musical development.Besides her music career Astrid North worked also as lecturer in Hamburg at the Hochschule für Musik und Theater and as yoga teacher. North was the mother of two children, her daughter was born in 2001 and her son in 2006. Her sister Ondria North works as make-up artist and hair stylist in the German film industry.\nShe died in June 2019 at the age of 45 years from pancreatic cancer.\n\nDiscography\nwith Cultured PearlsAlbums\n\n1996: Sing Dela Sing (German chart position 92, 3 weeks)\n1997: Space Age Honeymoon (German chart position 54, 6 weeks)\n1999: Liquefied Days (German chart position 19, 9 weeks)\n2002: Life on a Tuesday (German chart position 74, 1 week)Singles\n\n1996: Tic Toc (1996) (German chart position 65, 10 weeks)\n1997: Sugar Sugar Honey (German chart position 72, 9 weeks)\n1998: Silverball (German chart position 99, 2 weeks)\n1999: Kissing the Sheets (German chart position 87, 9 weeks)with Soulounge\n\n2003: The Essence of the Live Event – Volume One\n2004: Home\n2006: Say It AllSolo\n\n2005: Sunny (Single, Bobby Hebb feat. Astrid North)\n2012: North (Album, 20. Juli 2012)\n2013: North Live (Album, live recordings from different venues in Germany)\n2016: Sunny (Compilation, Bobby Hebb feat. Astrid North)\n2016: Precious Ruby (Album, 16. September 2016)as guest singer\n\n1994: Trouble Is – Big Light (Single)\n1996: Life's Too Short – Little Red Riding Hood (Single)\n2006: Basic Jazz Lounge: The Ride – Joo Kraus (Album)\nPassage 2:\nKristian Leontiou\nKristian Leontiou (born February 1982) is an English singer. Formerly a solo artist, he is the lead singer of indie rock band One eskimO.\n\nEarly life\nKristian Leontiou was born in London, England and is of Greek Cypriot descent. He went to Hatch End High School in Harrow and worked several jobs in and around London whilst concentrating on music when he had any free time.  In 2003 he signed a major record deal with Polydor. At the time, Leontiou was dubbed \"the new Dido\" by some media outlets. His debut single \"Story of My Life\" was released in June 2004 and reached #9 in the UK Singles Chart. His second single \"Shining\" peaked at #13 whilst the album Some Day Soon was certified gold selling in excess of 150,000 copies.\nLeontiou toured the album in November 2004 taking him to the US to work with L.A Reid, Chairman of the Island Def Jam music group.  Unhappy with the direction his career was going, on a flight back from the US in 2004 he decided to take his music in a new direction.  Splitting from his label in late 2005, he went on to collaborate with Faithless on the song \"Hope & Glory\" for their album ‘'To All New Arrivals'’.  It was this release that saw him unleash the One eskimO moniker.  It was through working with Rollo Armstrong on the Faithless album, that Rollo got to hear an early demo of \"Astronauts\" from the One eskimO project. Being more than impressed by what he heard, Rollo opened both his arms and studio doors to Leontiou and they began to co-produce the ‘'All Balloons’' album.\nIt was at this time that he paired up with good friend Adam Falkner, a drummer/musician, to introduce a live acoustic sound to the album. They recorded the album with engineer Phill Brown (engineer for Bob Marley and Robert Plant) at Ark studios in St John's Wood where they recorded live then headed back to Rollo's studio to add the cinematic electro touches that are prominent on the album.\nShortly after its completion, One eskimO's \"Hometime\" was used on a Toyota Prius advert in the USA. The funds from the advert were then used to develop the visual aspect of One eskimO. He teamed up with friend Nathan Erasmus (Gravy Media Productions) along with animation team Smuggling Peanuts (Matt Latchford and Lucy Sullivan) who together began to develop the One eskimO world, the first animation produced was for the track ‘Hometime’ which went on to win a British animation award in 2008.\nIn 2008 Leontiou started a new management venture with ATC Music. By mid-2008 Time Warner came on board to develop all 10 One eskimO animations which were produced the highly regarded Passion Pictures in London.  Now with all animation complete and a debut album, One eskimO prepare to unveil themselves fully to the world in summer 2009.\nLeontiou released a cover version of Tracy Chapman's \"Fast Car\", which was originally released as a single in 2005. Leontiou's version was unable to chart, however, due to there being no simultaneous physical release alongside the download single, a UK chart rule that was in place at the time. On 24 April 2011, the song entered the singles chart at number 88 due to Britain's Got Talent contestant Michael Collings covering the track on the show on 16 April 2011.\n\nDiscography\nAlbums\nSingles\nNotes\nA  - Originally released as a single in April 2005, Leontiou's version of \"Fast Car\" did not chart until 2011 in the UK.\n\nAlso featured on\nNow That's What I Call Music! 58 (Story of My Life)\nWin a Date with Tad Hamilton! OST, Love Love Songs - The Ultimate Love Collection (Shining)\nSummerland OST (The Crying)\nPassage 3:\nNekfeu\nKen Samaras (Greek: Κεν Σαμαράς, Ken Samarás; born 3 April 1990), better known by his stage name Nekfeu (French pronunciation: ​[nɛk.fø]), is a French rapper, actor and record producer. He is also a member of the crew L'entourage and the bands $-Crew and 1995. He started his career as a member of $-Crew, with childhood friends Framal, Mekra, 2zer Washington and DJ Elite. He joined 1995 in 2007, participating in open mic duels around Paris.\n\nEarly life and career\nSamaras was born in La Trinité, a commune within the Nice metropolitan area, to a father of Greek descent and a mother of Scottish descent. At the age of 11, he and his family moved to the 15th arrondissement of Paris.After two extended plays with 1995 (La Source in 2011 and La Suite in 2012), as well as the studio album Paris Sud Minute in 2013, Nekfeu released his debut studio album Feu on 8 June 2015, for which he won Best Urban Music Album at the Victoires de la musique in February 2016, as well as Destins Liés with $-Crew in June 2016. In 2015, he also wrote and performed a song for the French version of the film Creed. In 2016, he released his second album entitled Cyborg.\nIn 2017, he made his acting debut in the film Tout nous sépare, opposite Catherine Deneuve.In 2019, he released his third album “les étoiles vagabondes”. The album premiered in cinemas, as a movie accompanied the album. The movie premiered 2h before the album was made available on streaming platforms. The album was extremely well received. It Included a song featuring mainstream Belgian rapper Damso (Tricheur), and many other songs entered the top 20 in the French charts.\n\nPersonal life\nNekfeu is a follower of Paris Saint-Germain, based in the city where he grew up, and his hometown club OGC Nice.\n\nDiscography\nAlbums, mixtapes and EPs\nAs part of 1995\n\n2011: La Source (EP)\n2012: La Suite (EP)\n2013: Paris Sud Minute (Album)With Alpha Wann (member of 1995 & L'Entourage)\n\n2011: En Sous-Marin (EP)As part of $-Crew\n\n2010: Même Signature (Mixtape)\n2012: Métamorphose (Mixtape)\n2013: Seine Zoo 仙豆 (Album)\n2016: Destins Liés (Album)\n2022: SZR2001 (Album)As part of L'Entourage\n\n2014: Jeunes Entrepreneurs (Album)As part of 5 Majeur\n\n2011: 5 Majeur (EP)\n2013: Variations (Album)Solo\n\nSingles\nFeatured in\nOther charted songs\nFilmography\nFilm\n2017 : Tout nous sépare, directed by Thierry Klifa\n2019 : L'Échappée, directed by Mathias Pardo\n\nTelevision\n2015 : Casting(s) (Television series, one episode : guest appearance as himself)\n2019 : My Hero Academia (Dubbing the character All for One in the French version)\nPassage 4:\nPanda (Astro song)\nAstro is the first album of long duration (after the EP Le disc of Astrou) of Chilean indie band Astro, released in 2011. The first single from the album was \"Ciervos\" and followed \"Colombo\", \"Panda\" and \"Manglares\".\nThis album was chosen by National Public Radio among the 50 discs of 2012.\n\nTrack listing\nAll tracks written by Andrés Nusser, except where noted.\n\nCiervos (Deer)\nCoco (Coconut)\nColombo\nDruida de las nubes (Druid of the clouds)\nPanda\nMiu-Miu\nManglares (Mangroves)\nMira, está nevando en las pirámides (Look, it's snowing in the pyramids)\nVolteretas (Tumbles)\nPepa\nNueces de Bangladesh (Nuts of Bangladesh)\nMiu-Miu reaparece (Miu-Miu reappears)\n\nPersonnel\nAstro\n\nAndrés Nusser – vocals, guitar\nOctavio Caviares – drums\nLego Moustache – keyboards, percussion\nZeta Moustache – keyboards, bassProduction\n\nAndrés Nusser – producer, recording and mixing\nChalo González – mixing and mastering\nCristóbal Carvajal – recording\nIgnacio Soto – recording\nPassage 5:\nBilly Milano\nBilly Milano (born June 3, 1964) is an American heavy metal and hardcore punk musician. He is the singer and occasionally guitarist and bassist of crossover thrash band M.O.D., and was the singer of its predecessor, Stormtroopers of Death. Prior to these bands, Milano played in early New York hardcore band the Psychos, which also launched the career of future Agnostic Front vocalist Roger Miret. Milano was also the singer of United Forces, which included his Stormtroopers of Death bandmate Dan Lilker. Milano managed a number of bands, including Agnostic Front, for whom he also co-produced the 1997 Epitaph Records release Something's Gotta Give and roadie for Anthrax.\n\nDiscography\nStormtroopers of Death albums\nStormtroopers of Death videos\nMethod of Destruction (M.O.D.)\nMastery\nPassage 6:\nCaspar Babypants\nCaspar Babypants is the stage name of children's music artist Chris Ballew, who is also the vocalist and bassist of The Presidents of the United States of America.\n\nHistory\nBallew's first brush with children's music came in 2002, when he recorded and donated an album of traditional children's songs to the nonprofit Program for Early Parent Support titled \"PEPS Sing A Long!\" Although that was a positive experience for him, he did not consider making music for families until he met his wife, collage artist Kate Endle. Her art inspired Ballew to consider making music that \"sounded like her art looked\" as he has said. Ballew began writing original songs and digging up nursery rhymes and folk songs in the public domain to interpret and make his own. The first album, Here I Am!, was recorded during the summer of 2008 and released in February 2009.\nBallew began to perform solo as Caspar Babypants in the Seattle area in January 2009. Fred Northup, a Seattle-based comedy improvisor, heard the album and offered to play as his live percussionist.  Northrup also suggested his frequent collaborator Ron Hippe as a keyboard player. \"Frederick Babyshirt\" and \"Ronald Babyshoes\" were the Caspar Babypants live band from May 2009 to April 2012. Both Northup and Hippe appear on some of his recordings but since April 2012 Caspar Babypants has exclusively performed solo. The reasons for the change were to include more improvisation in the show and to reduce the sound levels so that very young children and newborns could continue to attend without being overstimulated. \nBallew has made two albums of Beatles covers as Caspar Babypants. Baby Beatles! came out in September 2013 and Beatles Baby! came out in September 2015.\nBallew runs the Aurora Elephant Music record label, books shows, produces, records, and masters the albums himself. Distribution for the albums is handled by Burnside Distribution in Portland, Oregon.\nCaspar Babypants has released a total of 17 albums. The 17th album, BUG OUT!, was released on May 1, 2020. His album FLYING HIGH! was nominated for a Grammy Award for Best Children's Album. All 17 of the albums feature cover art by Ballew's wife, Kate Endle.\n\"FUN FAVORITES!\" and \"HAPPY HITS!\" are two vinyl-only collections of hit songs that Caspar Babypants has released in the last couple of years.\n\nDiscography\nAlbumsPEPS (2002)\nHere I Am! (Released 03/17/09) Special guests: Jen Wood, Fysah Thomas\nMore Please! (Released 12/15/09) Special guests: Fred Northup, Ron Hippe\nThis Is Fun! (Released 11/02/10)  Special guests: Fred Northup, Ron Hippe, Krist Novoselic, Charlie Hope\nSing Along! (Released 08/16/11) Special guests: Fred Northup, Ron Hippe, \"Weird Al\" Yankovic, Stone Gossard, Frances England, Rachel Loshak\nHot Dog! (Released 04/17/12) Special guests: Fred Northup, Ron Hippe, Rachel Flotard (Visqueen)\nI Found You! (Released 12/18/12) Special guests: Steve Turner (Mudhoney), Rachel Flotard (Visqueen), John Richards\nBaby Beatles! (Released 09/15/13)\nRise And Shine! (Released 09/16/14)\nNight Night! (Released 03/17/15)\nBeatles Baby! (Released 09/18/2015)\nAway We Go! (Released 08/12/2016)\nWinter Party! (Released 11/18/16)\nJump For Joy! (Released 08/18/17)\nSleep Tight! (Released 01/19/18)\nKeep It Real! (Released 08/17/18)\nBest Beatles! (Released 03/29/19)\nFlying High! (Released 08/16/19)\nBug Out! (released 05/1/20)\nHappy Heart! (Released 11/13/20)\nEasy Breezy! (Released 11/05/21)AppearancesMany Hands: Family Music for Haiti CD (released 2010) – Compilation of various artists\nSongs Stories And Friends: Let's Go Play – Charlie Hope (released 2011) – vocals on Alouette\nShake It Up, Shake It Off (released 2012) – Compilation of various artists\nKeep Hoping Machine Running – Songs Of Woody Guthrie (released 2012) – Compilation of various artists\nApple Apple – The Harmonica Pocket (released 2013) – vocals on Monkey Love\nSimpatico – Rennee and Friends (released 2015) – writer and vocals on I Am Not Afraid\nSundrops – The Harmonica Pocket (released 2015) – vocals on Digga Dog Kid\nPassage 7:\nO Valencia!\n\"O Valencia!\" is the fifth single by the indie rock band The Decemberists, and the first released from their fourth studio album, The Crane Wife.\nThe music was written by The Decemberists and the lyrics by Colin Meloy. It tells a story of two star-crossed lovers. The singer falls in love with a person who belongs to an opposing gang. At the end of the song, the singer's lover jumps in to defend the singer, who is confronting his lover's brother (the singer's \"sworn enemy\") and is killed by the bullet intended for the singer.\n\nTrack listing\nThe 7\" single sold in the UK was mispressed, with \"Culling of the Fold\" as the B-side despite the artwork and record label listing \"After the Bombs\" as the B-side.\n\nMusic videos\nFor the \"O Valencia!\" music video, The Decemberists filmed themselves in front of a green screen and asked fans to complete it by digitally adding in background images or footage. Stephen Colbert of The Colbert Report, having recently asked fans to do the same with a video of him with a light saber in front of a green screen, brought up The Decemberists on his segment \"Look Who's Riding on My Coattails Now\" and accused the band of stealing the idea. The Decemberists' response was to challenge Stephen Colbert to a guitar solo showdown on December 20, 2006, on The Colbert Report.On January 19, 2007, The Decemberists premiered an alternate music video of \"O Valencia!\", directed by Aaron Stewart-Ahn, on MTV2. The video follows a character named Patrick, played by Meloy, as he and his love Francesca (Lisa Molinaro), daughter of \"the Boss\", plan an escape to an unknown location. At a cafe, a man in a suit, portrayed by the band member Chris Funk, tells him to hide in the \"Valencia\" hotel (the Super Value Inn on North Interstate Avenue in Portland, Oregon) while he gets them the necessary documentation to escape. Above the name of the hotel, there is a neon sign that reads \"Office\". The letters have all burnt out except for the \"O\", creating the title of the song. The video then introduces other characters - various assassination teams - who sit in different rooms of the hotel waiting for the chance to catch the two lovers. Most are portrayed by other members of the band (along with Meloy's wife, Carson Ellis). They kill off any potential witnesses to their plan. Patrick manages to take down one member from each team, before they gang up on him. The Boss arrives, along with the man from the cafe, who reveals that he snitched on Patrick and Francesca. They execute Francesca, while forcing Patrick to watch. After they leave, Patrick finds a note by Francesca, which reveals that she never fell in love with him, and only wanted protection. 2 months later, Patrick and the man, who has lost an eye from a previous assassination attempt, have a sit-down at the same cafe. The man reveals that he snitched on Patrick just to take over the town. Patrick reveals that he poisoned a drink the man was having, but before he could get away, the man stabs Patrick in the neck with a fork before dying, followed by Patrick.\nThe video is somewhat influenced by the distinct style and themes of director Wes Anderson, with bold fonts being used to introduce characters and groups on the bottom of the screen (much like in the film The Royal Tenenbaums). The band had previously (and more explicitly) drawn influence from Anderson's Rushmore in their video for \"Sixteen Military Wives\". The layout of the hotel is also similar to the one used in Bottle Rocket.\nKurt Nishimura was chosen as the winner by mtvU for his video that depicted a love affair between a woman and her television, with the TV containing the green-screened Decemberists video footage.\nPassage 8:\nBernie Bonvoisin\nBernard Bonvoisin (French pronunciation: ​[bɛʁnaʁ bɔ̃vwazɛ̃]), known as Bernie Bonvoisin (French pronunciation: ​[bɛʁni bɔ̃vwazɛ̃], born 9 July 1956 in Nanterre, Hauts-de-Seine), is a French hard rock singer and film director. He is best known for having been the singer of Trust.\nHe was one of the best friends of Bon Scott the singer of AC/DC and together they recorded the song \"Ride On\" which was one of the last songs by Bon Scott.\n\nExternal links\nBernie Bonvoisin at IMDb\nPassage 9:\nJim Bob\nJames Robert Morrison, known as Jim Bob, is a British musician and author. He was the singer of indie punk band Carter the Unstoppable Sex Machine.\n\nBiography\nJim Bob played in various bands during the late 1970s and early 1980s, including Jamie Wednesday, who were performing between 1984 and 1987. In 1987 Jamie Wednesday split up just before a gig at the London Astoria. Morrison and Les \"Fruitbat\" Carter filled in, playing along to a backing tape, and Carter USM was born. Jim Bob and Les Carter had known each other since the late 1970s, when their bands The Ballpoints (featuring Jim on vocals) and Dead Clergy (Les on bass and vocals) used to rehearse at the same studio behind Streatham station. When The Ballpoints' bassist quit at the end of 1980, Carter joined the band, who than went on to play several gigs under the name Peter Pan's Playground.\nHe was a member of Carter USM. The band split up in 1997. Since Carter USM, Jim Bob has released two albums and three singles with his disco-pop-punk group Jim's Super Stereoworld, seven solo albums as Jim Bob or James Robert Morrison, and played various live shows both with his band and solo.  In 2001, he joined his old Carter bandmate Fruitbat on stage once again, as part of the group Who's The Daddy Now?.\nIn 2005, Cherry Red released a DVD of a live solo acoustic performance, titled Live From London, featuring songs from his solo career as well as many Carter USM tracks. This was followed by a concept album, School, released in March 2006.\nA best-of album was released in the autumn of 2006, accompanied by a UK tour. This was originally intended as a download-only release, but a physical CD was produced. The album was accompanied by a free CD of Jim Bob and Jim's Super Stereoworld rarities. The cover design was re-worked by Jim Bob from an image by Jim Connolly.\nThe album A Humpty Dumpty Thing was released in November 2007 by Cherry Red Records. The album came with a Jim Bob-penned-mini novel, \"Word Count\". A single from the album, \"Battling The Bottle\", was released with Jim Bob's re-working of the children's song \"The Wheels on the Bus\" on the B-side.\nJim Bob's next solo record, Goffam, was a semi-concept album about a city in the grip of crime, deserted by its superheroes. He toured the UK in April and September 2009 promoting the album.\nIn December 2009 Jim Bob performed his 2004 song \"Angelstrike!\" as part of the shows The Return of 9 Lessons and Carols for Godless People for two nights at the Bloomsbury Theatre and at Hammersmith Apollo. This was broadcast on BBC4 television under the title 'Nerdstack'.\nHis debut novel Storage Stories was released  on the day of a UK general election, 6 May 2010, by Ten Forty Books. This was followed by three novels with major publishers: Driving Jarvis Ham, The Extra Ordinary Life of Frank Derrick, Age 81 and Frank Derrick's Holiday of a Lifetime.\nJim Bob's autobiography, Goodnight Jim Bob – On The Road With Carter The Unstoppable Sex Machine, was published by Cherry Red Books in 2004. The sequel, Jim Bob from Carter, was published by Cherry Red Books on 23 March 2019. The double novel A Godawful Small Affair and Harvey King Unboxes His Family, written under the name J.B. Morrison, was published by Cherry Red Books in March 2020.\nThe 26-second song \"2020 WTF!\" was released in March 2020 on Cherry Red Records. It was the first single from Jim Bob's August 2020 album, Pop Up Jim Bob. The second single, \"Jo's Got Papercuts\", followed in June, and a third, \"If it Ain't Broke\", was released in July.\nThe album Pop Up Jim Bob was released on Cherry Red Records on 14 August 2020. Entering the official UK album chart at number 26, it was Jim Bob's first top-30 LP since Carter The Unstoppable Sex Machine.\nJim Bob's 13th solo album, Who Do We Hate Today, was released on Cherry Red Records on 20 August 2021 and reached 34 in the UK album charts, his second top-40 solo LP.\n\nSolo discography\nAll releases credited as Jim Bob unless otherwise stated. See Carter USM and Jamie Wednesday for those bands' discographies.\n\nAlbums\nJim's Super Stereoworld, 2001 (Jim's Super Stereoworld)\nJR, 2001 (James Robert Morrison)\nBig Flash Car on a Saturday Night, 2002 (Jim's Super Stereoworld)\nGoodnight Jim Bob, 2003\nAngelstrike!, 2004\nSchool, 2006\nBest of Jim Bob, 2006\nA Humpty Dumpty Thing, November 2007\nGoffam, April 2009\nWhat I Think About When I Think About You, November 2013\nJim Bob Sings Again, November 2016\nPop Up Jim Bob, August 2020 \nWho Do We Hate Today, August 2021\nThe Essential Jim Bob, November 2022\nThanks for Reaching Out, June 2023\n\nSingles\nJim's Super Stereoworld – \"Bonkers in the Nut\", 1999\nJim's Super Stereoworld – \"Could U B The 1 I Waited 4\", 1999\nJim's Super Stereoworld – \"Bubblegum EP\", 2002\n\"Dumb and Dumber\", March 2005\n\"Battling The Bottle (Fighting The Flab, At War with the World)\", November 2007\n\"The Man Behind the Counter of the Science Fiction Superstore\", Marc 2009\n\"Our Heroes\", June 2009\n\"Dream Come True\", September 2013\n\"Breaking News\", October 2013\n\"2020 WTF!\", April 2020\n\nOther releases\nAcoustic Party 7A Free CD recorded by Morrison at home and given away to the first 10 people to visit the T-shirt stall and ask for Marc or Neil on the October 2003 tour\n\nStolen from Westlife25 readers of Morrison's book won a copy of the CD 'Stolen From Westlife' – 8 cover versions recorded by Morrison – after answering some questions posed by the author on page 95.\n\nBuskerA free CD recorded by Morrison and containing six acoustic covers, the CD is currently being issued only to members of Morrison's \"street team\". The CD includes a cover of a track originally written and recorded by his former Carter bandmate Les Carter.\n\nDVDs\nLive From London, 2005A DVD featuring a live acoustic performance of Jim Bob songs and Carter USM songs.  Bonus features include an interview with Morrison, Morrison reading excerpts from his autobiography and the video for the Jim's Super Stereoworld song \"Bubblegum\".\n\nNATIONAL TREASURE – Live at the Shepherd’s Bush Empire , July 2019\n\nBibliography\nNon-fiction\n\nGoodnight Jim Bob (2006) – On the Road With Carter The Unstoppable Sex Machine Jim Bob's autobiographic tale of his time on tour with Carter USM. Published by Cherry Red Books.\nJim Bob from Carter: In the Shadow of my Former Self (2019) Published by Cherry Red BooksFiction\n\nStorage Stories (2010) – Jim Bob's debut novel, which took six years to write. described as a darkly comic rollercoaster ride full of thrills, spills and warm sick on the back of the neck. Published by 1040 Books.\nDriving Jarvis Ham (2012) – Jim Bob's second novel, following the life of the awkward character of Jarvis Ham, from the perspective of his oldest friend. A brilliantly witty story of unconventional, unwavering, and regularly exasperating friendship. Published by The Friday Club/HarperCollins \nThe Extra Ordinary Life of Frank Derrick, Age 81 (June 2014) – Under the name J.B. Morrison. Published by Pan Macmillan.\nFrank Derrick's Holiday Of A Lifetime (2015) Published by Pan Macmillan\nA Godawful Small Affair b/w Harvey King Unboxes His Family (2020) – Under the name J.B. Morrison. Published by Cherry Red Books\nPassage 10:\nÉgérie (song)\n\"Égérie\" (French for 'Muse') is a song by French hip hop artist Nekfeu, produced by himself and DJ Elite. It was released on April 3, 2015 as the lead single from his debut studio album Feu. It entered the French Singles Chart at number 49 on 11 April 2015, where it has since peaked.\n\nMusic video\nThe music video for the song was released on YouTube as part of the single's release on 3 April 2015. It is 4 minutes and 3 seconds long.\nDirected by Dawid Krepski, the video begins with a woman opening the trunk of a fifth-generation Chevrolet Camaro and stabbing at something inside before closing the trunk and entering the car, with Nekfeu waiting inside. The two share a kiss before the woman drives off as Nekfeu raps the lyrics to the song. The video is filled with a variety of psychedelic images and ends with a shot of the car being driven away into the horizon on a lonely desert road.\n\nTrack listing\nDigital download\"Égérie\" – 3:29\n\nChart performance", "answers": ["La Trinité"], "length": 4966, "dataset": "2wikimqa", "language": "en", "all_classes": null, "_id": "0bcff697444354703dd1e8987a709b8ed2f44bf9d6b2d320"}
